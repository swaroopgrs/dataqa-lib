Directory structure:
└── src/
    └── dataqa/
        ├── __init__.py
        ├── api.py
        ├── exceptions.py
        ├── logging_config.py
        ├── agent/
        │   ├── __init__.py
        │   ├── agent.py
        │   ├── nodes.py
        │   ├── state.py
        │   └── workflow.py
        ├── cli/
        │   ├── __init__.py
        │   └── main.py
        ├── config/
        │   ├── __init__.py
        │   ├── loader.py
        │   └── models.py
        ├── models/
        │   ├── __init__.py
        │   ├── document.py
        │   ├── execution.py
        │   └── message.py
        ├── orchestration/
        │   ├── __init__.py
        │   ├── config.py
        │   ├── models.py
        │   ├── agents/
        │   │   ├── __init__.py
        │   │   ├── base.py
        │   │   ├── hierarchy.py
        │   │   ├── manager.py
        │   │   └── worker.py
        │   ├── approval/
        │   │   ├── __init__.py
        │   │   ├── manager.py
        │   │   ├── models.py
        │   │   └── workflow.py
        │   ├── domain/
        │   │   ├── __init__.py
        │   │   ├── context.py
        │   │   ├── knowledge.py
        │   │   └── rules.py
        │   ├── evaluation/
        │   │   ├── __init__.py
        │   │   ├── analytics.py
        │   │   ├── benchmark.py
        │   │   └── judge.py
        │   └── planning/
        │       ├── __init__.py
        │       ├── execution_state.py
        │       ├── models.py
        │       ├── planner.py
        │       └── replanning.py
        ├── primitives/
        │   ├── __init__.py
        │   ├── executor.py
        │   ├── faiss_knowledge.py
        │   ├── in_memory_executor.py
        │   ├── knowledge.py
        │   └── llm.py
        └── utils/
            ├── __init__.py
            └── retry.py

================================================
FILE: dataqa/__init__.py
================================================
"""
DataQA - A composable data agent framework.

This package provides tools for building intelligent data agents that can
interact with structured data through natural language interfaces.
"""

__version__ = "0.1.0"
__author__ = "DataQA Team"

# Import main API components
from .agent.agent import DataAgent
from .api import (
    DataQAClient,
    agent_session,
    create_agent,
    create_agent_async,
    quick_query,
    quick_query_async,
)
from .config import AgentConfig
from .models.document import Document
from .models.message import Message

__all__ = [
    # Core classes
    "DataAgent",
    "DataQAClient", 
    "AgentConfig",
    "Document",
    "Message",
    # Factory functions
    "create_agent",
    "create_agent_async",
    # Context managers
    "agent_session",
    # Convenience functions
    "quick_query",
    "quick_query_async",
]


================================================
FILE: dataqa/api.py
================================================
"""
High-level Python API for DataQA framework.

This module provides convenient factory functions, context managers, and async
support for programmatic agent creation and management. It serves as the main
entry point for developers using DataQA in their applications.
"""

import asyncio
import logging
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Any, AsyncGenerator, Dict, List, Optional, Union

from .agent.agent import DataAgent, create_agent_from_config
from .config.loader import load_agent_config
from .config.models import AgentConfig
from .models.document import Document
from .models.message import Message

logger = logging.getLogger(__name__)


class DataQAClient:
    """High-level client for DataQA framework.
    
    This class provides a convenient interface for creating and managing
    DataQA agents with automatic resource management and async support.
    
    Example:
        ```python
        # Synchronous usage
        client = DataQAClient()
        agent = client.create_agent("my-agent", config_path="config/agent.yaml")
        response = client.query(agent, "Show me sales data for last month")
        
        # Async usage
        async with DataQAClient() as client:
            agent = await client.create_agent_async("my-agent", config_path="config/agent.yaml")
            response = await client.query_async(agent, "Show me sales data for last month")
        ```
    """
    
    def __init__(self, default_config_dir: Optional[Path] = None):
        """Initialize the DataQA client.
        
        Args:
            default_config_dir: Default directory for configuration files
        """
        self.default_config_dir = default_config_dir or Path("config")
        self._agents: Dict[str, DataAgent] = {}
        self._event_loop: Optional[asyncio.AbstractEventLoop] = None
        
    def __enter__(self) -> 'DataQAClient':
        """Enter synchronous context manager."""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """Exit synchronous context manager."""
        self.shutdown()
    
    async def __aenter__(self) -> 'DataQAClient':
        """Enter async context manager."""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Exit async context manager."""
        await self.shutdown_async()
    
    def create_agent(
        self,
        name: str,
        config: Optional[Union[AgentConfig, Dict[str, Any], str, Path]] = None,
        config_path: Optional[Union[str, Path]] = None,
        **kwargs
    ) -> DataAgent:
        """Create a DataQA agent synchronously.
        
        Args:
            name: Agent name/identifier
            config: Agent configuration (AgentConfig, dict, or YAML string)
            config_path: Path to configuration file
            **kwargs: Additional configuration parameters
            
        Returns:
            Initialized DataAgent instance
            
        Raises:
            ValueError: If neither config nor config_path is provided
            FileNotFoundError: If config file is not found
        """
        # Run async version in sync context
        if self._event_loop is None:
            self._event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._event_loop)
        
        return self._event_loop.run_until_complete(
            self.create_agent_async(name, config, config_path, **kwargs)
        )
    
    async def create_agent_async(
        self,
        name: str,
        config: Optional[Union[AgentConfig, Dict[str, Any], str, Path]] = None,
        config_path: Optional[Union[str, Path]] = None,
        **kwargs
    ) -> DataAgent:
        """Create a DataQA agent asynchronously.
        
        Args:
            name: Agent name/identifier
            config: Agent configuration (AgentConfig, dict, or YAML string)
            config_path: Path to configuration file
            **kwargs: Additional configuration parameters
            
        Returns:
            Initialized DataAgent instance
            
        Raises:
            ValueError: If neither config nor config_path is provided
            FileNotFoundError: If config file is not found
        """
        logger.info(f"Creating agent: {name}")
        
        # Load configuration
        agent_config = await self._load_config(config, config_path, name, **kwargs)
        
        # Create agent
        agent = await create_agent_from_config(agent_config)
        
        # Store agent reference
        self._agents[name] = agent
        
        logger.info(f"Agent created successfully: {name}")
        return agent
    
    def get_agent(self, name: str) -> Optional[DataAgent]:
        """Get an existing agent by name.
        
        Args:
            name: Agent name
            
        Returns:
            DataAgent instance or None if not found
        """
        return self._agents.get(name)
    
    def list_agents(self) -> List[str]:
        """List all created agent names.
        
        Returns:
            List of agent names
        """
        return list(self._agents.keys())
    
    def query(
        self,
        agent: Union[DataAgent, str],
        query: str,
        conversation_id: Optional[str] = None
    ) -> str:
        """Query an agent synchronously.
        
        Args:
            agent: DataAgent instance or agent name
            query: User query
            conversation_id: Optional conversation ID
            
        Returns:
            Agent response
            
        Raises:
            ValueError: If agent is not found
        """
        if self._event_loop is None:
            self._event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._event_loop)
        
        return self._event_loop.run_until_complete(
            self.query_async(agent, query, conversation_id)
        )
    
    async def query_async(
        self,
        agent: Union[DataAgent, str],
        query: str,
        conversation_id: Optional[str] = None
    ) -> str:
        """Query an agent asynchronously.
        
        Args:
            agent: DataAgent instance or agent name
            query: User query
            conversation_id: Optional conversation ID
            
        Returns:
            Agent response
            
        Raises:
            ValueError: If agent is not found
        """
        # Resolve agent
        if isinstance(agent, str):
            agent_instance = self._agents.get(agent)
            if agent_instance is None:
                raise ValueError(f"Agent not found: {agent}")
        else:
            agent_instance = agent
        
        # Process query
        return await agent_instance.query(query, conversation_id)
    
    def approve_operation(
        self,
        agent: Union[DataAgent, str],
        conversation_id: str,
        approved: bool = True,
        reason: Optional[str] = None
    ) -> str:
        """Approve or deny a pending operation synchronously.
        
        Args:
            agent: DataAgent instance or agent name
            conversation_id: Conversation ID
            approved: Whether to approve the operation
            reason: Optional reason for the decision
            
        Returns:
            Response after processing approval
        """
        if self._event_loop is None:
            self._event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._event_loop)
        
        return self._event_loop.run_until_complete(
            self.approve_operation_async(agent, conversation_id, approved, reason)
        )
    
    async def approve_operation_async(
        self,
        agent: Union[DataAgent, str],
        conversation_id: str,
        approved: bool = True,
        reason: Optional[str] = None
    ) -> str:
        """Approve or deny a pending operation asynchronously.
        
        Args:
            agent: DataAgent instance or agent name
            conversation_id: Conversation ID
            approved: Whether to approve the operation
            reason: Optional reason for the decision
            
        Returns:
            Response after processing approval
        """
        # Resolve agent
        if isinstance(agent, str):
            agent_instance = self._agents.get(agent)
            if agent_instance is None:
                raise ValueError(f"Agent not found: {agent}")
        else:
            agent_instance = agent
        
        # Process approval
        return await agent_instance.approve_operation(conversation_id, approved, reason)
    
    def ingest_knowledge(
        self,
        agent: Union[DataAgent, str],
        documents: List[Document]
    ) -> None:
        """Ingest documents into an agent's knowledge base synchronously.
        
        Args:
            agent: DataAgent instance or agent name
            documents: List of documents to ingest
        """
        if self._event_loop is None:
            self._event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._event_loop)
        
        self._event_loop.run_until_complete(
            self.ingest_knowledge_async(agent, documents)
        )
    
    async def ingest_knowledge_async(
        self,
        agent: Union[DataAgent, str],
        documents: List[Document]
    ) -> None:
        """Ingest documents into an agent's knowledge base asynchronously.
        
        Args:
            agent: DataAgent instance or agent name
            documents: List of documents to ingest
        """
        # Resolve agent
        if isinstance(agent, str):
            agent_instance = self._agents.get(agent)
            if agent_instance is None:
                raise ValueError(f"Agent not found: {agent}")
        else:
            agent_instance = agent
        
        # Ingest documents
        await agent_instance.ingest_knowledge(documents)
    
    def get_conversation_history(
        self,
        agent: Union[DataAgent, str],
        conversation_id: str
    ) -> List[Message]:
        """Get conversation history synchronously.
        
        Args:
            agent: DataAgent instance or agent name
            conversation_id: Conversation ID
            
        Returns:
            List of messages in the conversation
        """
        if self._event_loop is None:
            self._event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._event_loop)
        
        return self._event_loop.run_until_complete(
            self.get_conversation_history_async(agent, conversation_id)
        )
    
    async def get_conversation_history_async(
        self,
        agent: Union[DataAgent, str],
        conversation_id: str
    ) -> List[Message]:
        """Get conversation history asynchronously.
        
        Args:
            agent: DataAgent instance or agent name
            conversation_id: Conversation ID
            
        Returns:
            List of messages in the conversation
        """
        # Resolve agent
        if isinstance(agent, str):
            agent_instance = self._agents.get(agent)
            if agent_instance is None:
                raise ValueError(f"Agent not found: {agent}")
        else:
            agent_instance = agent
        
        # Get conversation history
        return await agent_instance.get_conversation_history(conversation_id)
    
    def health_check(self, agent: Union[DataAgent, str]) -> Dict[str, Any]:
        """Perform health check on an agent synchronously.
        
        Args:
            agent: DataAgent instance or agent name
            
        Returns:
            Health status information
        """
        if self._event_loop is None:
            self._event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._event_loop)
        
        return self._event_loop.run_until_complete(
            self.health_check_async(agent)
        )
    
    async def health_check_async(self, agent: Union[DataAgent, str]) -> Dict[str, Any]:
        """Perform health check on an agent asynchronously.
        
        Args:
            agent: DataAgent instance or agent name
            
        Returns:
            Health status information
        """
        # Resolve agent
        if isinstance(agent, str):
            agent_instance = self._agents.get(agent)
            if agent_instance is None:
                raise ValueError(f"Agent not found: {agent}")
        else:
            agent_instance = agent
        
        # Perform health check
        return await agent_instance.health_check()
    
    def shutdown(self) -> None:
        """Shutdown all agents and clean up resources synchronously."""
        if self._event_loop is None:
            self._event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._event_loop)
        
        self._event_loop.run_until_complete(self.shutdown_async())
        
        # Close event loop
        if self._event_loop and not self._event_loop.is_closed():
            self._event_loop.close()
            self._event_loop = None
    
    async def shutdown_async(self) -> None:
        """Shutdown all agents and clean up resources asynchronously."""
        logger.info("Shutting down DataQA client...")
        
        # Shutdown all agents
        for name, agent in self._agents.items():
            try:
                await agent.shutdown()
                logger.info(f"Agent shutdown complete: {name}")
            except Exception as e:
                logger.error(f"Error shutting down agent {name}: {e}")
        
        # Clear agent references
        self._agents.clear()
        
        logger.info("DataQA client shutdown complete")
    
    async def _load_config(
        self,
        config: Optional[Union[AgentConfig, Dict[str, Any], str, Path]],
        config_path: Optional[Union[str, Path]],
        name: str,
        **kwargs
    ) -> AgentConfig:
        """Load and validate agent configuration.
        
        Args:
            config: Configuration object, dict, or YAML string
            config_path: Path to configuration file
            name: Agent name
            **kwargs: Additional configuration parameters
            
        Returns:
            Validated AgentConfig instance
            
        Raises:
            ValueError: If neither config nor config_path is provided
            FileNotFoundError: If config file is not found
        """
        if config is not None:
            # Handle different config types
            if isinstance(config, AgentConfig):
                agent_config = config
            elif isinstance(config, dict):
                agent_config = AgentConfig(**config)
            elif isinstance(config, (str, Path)):
                # Treat as YAML content or file path
                if Path(config).exists():
                    agent_config = load_agent_config(Path(config))
                else:
                    # Treat as YAML string
                    import yaml
                    config_dict = yaml.safe_load(config)
                    agent_config = AgentConfig(**config_dict)
            else:
                raise ValueError(f"Invalid config type: {type(config)}")
        elif config_path is not None:
            # Load from file
            config_file = Path(config_path)
            if not config_file.is_absolute():
                config_file = self.default_config_dir / config_file
            
            if not config_file.exists():
                raise FileNotFoundError(f"Configuration file not found: {config_file}")
            
            agent_config = load_agent_config(config_file)
        else:
            raise ValueError("Either config or config_path must be provided")
        
        # Override with kwargs
        if kwargs:
            config_dict = agent_config.model_dump()
            config_dict.update(kwargs)
            agent_config = AgentConfig(**config_dict)
        
        # Ensure name is set
        if not agent_config.name:
            agent_config.name = name
        
        return agent_config


# Convenience factory functions

def create_agent(
    name: str,
    config: Optional[Union[AgentConfig, Dict[str, Any], str, Path]] = None,
    config_path: Optional[Union[str, Path]] = None,
    **kwargs
) -> DataAgent:
    """Create a DataQA agent with default client.
    
    This is a convenience function for simple use cases where you don't need
    to manage multiple agents or use async operations.
    
    Args:
        name: Agent name/identifier
        config: Agent configuration
        config_path: Path to configuration file
        **kwargs: Additional configuration parameters
        
    Returns:
        Initialized DataAgent instance
        
    Example:
        ```python
        agent = create_agent("my-agent", config_path="config/agent.yaml")
        response = agent.query("Show me sales data")
        ```
    """
    client = DataQAClient()
    return client.create_agent(name, config, config_path, **kwargs)


async def create_agent_async(
    name: str,
    config: Optional[Union[AgentConfig, Dict[str, Any], str, Path]] = None,
    config_path: Optional[Union[str, Path]] = None,
    **kwargs
) -> DataAgent:
    """Create a DataQA agent asynchronously with default client.
    
    Args:
        name: Agent name/identifier
        config: Agent configuration
        config_path: Path to configuration file
        **kwargs: Additional configuration parameters
        
    Returns:
        Initialized DataAgent instance
        
    Example:
        ```python
        agent = await create_agent_async("my-agent", config_path="config/agent.yaml")
        response = await agent.query("Show me sales data")
        ```
    """
    client = DataQAClient()
    return await client.create_agent_async(name, config, config_path, **kwargs)


@asynccontextmanager
async def agent_session(
    name: str,
    config: Optional[Union[AgentConfig, Dict[str, Any], str, Path]] = None,
    config_path: Optional[Union[str, Path]] = None,
    **kwargs
) -> AsyncGenerator[DataAgent, None]:
    """Create an agent within an async context manager for automatic cleanup.
    
    Args:
        name: Agent name/identifier
        config: Agent configuration
        config_path: Path to configuration file
        **kwargs: Additional configuration parameters
        
    Yields:
        Initialized DataAgent instance
        
    Example:
        ```python
        async with agent_session("my-agent", config_path="config/agent.yaml") as agent:
            response = await agent.query("Show me sales data")
            # Agent is automatically cleaned up when exiting the context
        ```
    """
    async with DataQAClient() as client:
        agent = await client.create_agent_async(name, config, config_path, **kwargs)
        try:
            yield agent
        finally:
            await agent.shutdown()


# Convenience functions for common operations

def quick_query(
    query: str,
    config_path: Optional[Union[str, Path]] = None,
    agent_name: str = "quick-agent",
    **config_kwargs
) -> str:
    """Perform a quick query with minimal setup.
    
    This function creates a temporary agent, processes the query, and cleans up.
    Useful for one-off queries or testing.
    
    Args:
        query: User query to process
        config_path: Optional path to configuration file
        agent_name: Name for the temporary agent
        **config_kwargs: Additional configuration parameters
        
    Returns:
        Query response
        
    Example:
        ```python
        response = quick_query(
            "Show me sales data for last month",
            config_path="config/agent.yaml"
        )
        ```
    """
    with DataQAClient() as client:
        # Create config from kwargs if provided, otherwise use config_path
        config = config_kwargs if config_kwargs else None
        agent = client.create_agent(agent_name, config=config, config_path=config_path)
        return client.query(agent, query)


async def quick_query_async(
    query: str,
    config_path: Optional[Union[str, Path]] = None,
    agent_name: str = "quick-agent",
    **config_kwargs
) -> str:
    """Perform a quick query asynchronously with minimal setup.
    
    Args:
        query: User query to process
        config_path: Optional path to configuration file
        agent_name: Name for the temporary agent
        **config_kwargs: Additional configuration parameters
        
    Returns:
        Query response
        
    Example:
        ```python
        response = await quick_query_async(
            "Show me sales data for last month",
            config_path="config/agent.yaml"
        )
        ```
    """
    # Create config from kwargs if provided, otherwise use config_path
    config = config_kwargs if config_kwargs else None
    async with agent_session(agent_name, config=config, config_path=config_path) as agent:
        return await agent.query(query)


# Export main classes and functions
__all__ = [
    "DataQAClient",
    "create_agent",
    "create_agent_async", 
    "agent_session",
    "quick_query",
    "quick_query_async"
]


================================================
FILE: dataqa/exceptions.py
================================================
"""Custom exception classes for DataQA components."""

import logging
from typing import Any, Dict, Optional


class DataQAError(Exception):
    """Base exception class for all DataQA errors.
    
    Provides structured error handling with user-friendly messages,
    technical details, and error recovery suggestions.
    """
    
    def __init__(
        self,
        message: str,
        *,
        user_message: Optional[str] = None,
        technical_details: Optional[Dict[str, Any]] = None,
        recovery_suggestions: Optional[list[str]] = None,
        error_code: Optional[str] = None,
        original_error: Optional[Exception] = None
    ):
        """Initialize DataQA error with structured information.
        
        Args:
            message: Technical error message for logging
            user_message: User-friendly error message
            technical_details: Additional technical context
            recovery_suggestions: List of suggested recovery actions
            error_code: Unique error code for categorization
            original_error: Original exception that caused this error
        """
        super().__init__(message)
        self.user_message = user_message or self._generate_user_message(message)
        self.technical_details = technical_details or {}
        self.recovery_suggestions = recovery_suggestions or []
        self.error_code = error_code
        self.original_error = original_error
        
        # Log the error with structured information
        self._log_error()
    
    def _generate_user_message(self, technical_message: str) -> str:
        """Generate a user-friendly message from technical details."""
        return f"An error occurred: {technical_message}"
    
    def _log_error(self) -> None:
        """Log the error with structured information."""
        logger = logging.getLogger(self.__class__.__module__)
        
        log_data = {
            "error_type": self.__class__.__name__,
            "error_code": self.error_code,
            "technical_message": str(self),
            "user_message": self.user_message,
            "technical_details": self.technical_details,
            "recovery_suggestions": self.recovery_suggestions
        }
        
        if self.original_error:
            log_data["original_error"] = {
                "type": type(self.original_error).__name__,
                "message": str(self.original_error)
            }
        
        logger.error(
            f"DataQA Error: {self.__class__.__name__}",
            extra={"error_data": log_data},
            exc_info=self.original_error
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert error to dictionary for serialization."""
        return {
            "error_type": self.__class__.__name__,
            "error_code": self.error_code,
            "message": str(self),
            "user_message": self.user_message,
            "technical_details": self.technical_details,
            "recovery_suggestions": self.recovery_suggestions,
            "original_error": str(self.original_error) if self.original_error else None
        }


class KnowledgeError(DataQAError):
    """Exception raised by knowledge primitive operations."""
    
    def _generate_user_message(self, technical_message: str) -> str:
        """Generate user-friendly message for knowledge errors."""
        if "connection" in technical_message.lower():
            return "Unable to connect to the knowledge base. Please check your configuration."
        elif "search" in technical_message.lower():
            return "Failed to search the knowledge base. The search service may be temporarily unavailable."
        elif "ingest" in technical_message.lower():
            return "Failed to add documents to the knowledge base. Please check the document format and try again."
        else:
            return "An issue occurred with the knowledge base. Please try again or contact support."


class ExecutionError(DataQAError):
    """Exception raised by executor primitive operations."""
    
    def _generate_user_message(self, technical_message: str) -> str:
        """Generate user-friendly message for execution errors."""
        if "sql" in technical_message.lower():
            return "There was an error executing the SQL query. Please check your data and query syntax."
        elif "python" in technical_message.lower():
            return "There was an error executing the Python code. The operation may not be supported."
        elif "connection" in technical_message.lower():
            return "Unable to connect to the database. Please check your connection settings."
        elif "permission" in technical_message.lower():
            return "You don't have permission to perform this operation. Please contact your administrator."
        else:
            return "An error occurred while executing your request. Please try a different approach."


class LLMError(DataQAError):
    """Exception raised by LLM interface operations."""
    
    def _generate_user_message(self, technical_message: str) -> str:
        """Generate user-friendly message for LLM errors."""
        if "rate limit" in technical_message.lower():
            return "The AI service is currently busy. Please wait a moment and try again."
        elif "authentication" in technical_message.lower():
            return "There's an issue with the AI service authentication. Please contact support."
        elif "timeout" in technical_message.lower():
            return "The AI service took too long to respond. Please try again with a simpler request."
        elif "connection" in technical_message.lower():
            return "Unable to connect to the AI service. Please check your internet connection."
        elif "quota" in technical_message.lower():
            return "You've reached your usage limit for the AI service. Please try again later."
        else:
            return "The AI service encountered an issue. Please try rephrasing your request."


class ConfigurationError(DataQAError):
    """Exception raised for configuration-related errors."""
    
    def _generate_user_message(self, technical_message: str) -> str:
        """Generate user-friendly message for configuration errors."""
        if "file not found" in technical_message.lower():
            return "Configuration file not found. Please check the file path and ensure it exists."
        elif "invalid yaml" in technical_message.lower():
            return "Configuration file format is invalid. Please check the YAML syntax."
        elif "missing" in technical_message.lower():
            return "Required configuration is missing. Please check your configuration file."
        elif "environment" in technical_message.lower():
            return "Required environment variables are not set. Please check your environment configuration."
        else:
            return "There's an issue with your configuration. Please review your settings."


class ValidationError(DataQAError):
    """Exception raised for data validation errors."""
    
    def _generate_user_message(self, technical_message: str) -> str:
        """Generate user-friendly message for validation errors."""
        if "required" in technical_message.lower():
            return "Required information is missing. Please provide all necessary details."
        elif "format" in technical_message.lower():
            return "The data format is incorrect. Please check your input and try again."
        elif "type" in technical_message.lower():
            return "The data type is not supported. Please use a different format."
        else:
            return "The provided data is not valid. Please check your input."


class WorkflowError(DataQAError):
    """Exception raised during workflow execution."""
    
    def _generate_user_message(self, technical_message: str) -> str:
        """Generate user-friendly message for workflow errors."""
        if "state" in technical_message.lower():
            return "There was an issue with the conversation state. Please start a new conversation."
        elif "step" in technical_message.lower():
            return "A workflow step failed. Please try your request again."
        elif "timeout" in technical_message.lower():
            return "The operation took too long to complete. Please try a simpler request."
        else:
            return "There was an issue processing your request. Please try again."


class SecurityError(DataQAError):
    """Exception raised for security-related issues."""
    
    def _generate_user_message(self, technical_message: str) -> str:
        """Generate user-friendly message for security errors."""
        if "unauthorized" in technical_message.lower():
            return "You are not authorized to perform this operation."
        elif "unsafe" in technical_message.lower():
            return "This operation is not allowed for security reasons."
        elif "blocked" in technical_message.lower():
            return "This request was blocked by security policies."
        else:
            return "This operation cannot be completed due to security restrictions."


class RetryableError(DataQAError):
    """Exception for errors that can be retried."""
    
    def __init__(self, *args, retry_after: Optional[float] = None, max_retries: int = 3, **kwargs):
        """Initialize retryable error.
        
        Args:
            retry_after: Seconds to wait before retry
            max_retries: Maximum number of retry attempts
        """
        super().__init__(*args, **kwargs)
        self.retry_after = retry_after
        self.max_retries = max_retries


# Error recovery utilities
class ErrorRecovery:
    """Utilities for error recovery and graceful degradation."""
    
    @staticmethod
    def suggest_recovery_actions(error: DataQAError) -> list[str]:
        """Suggest recovery actions based on error type and context."""
        suggestions = []
        
        if isinstance(error, LLMError):
            suggestions.extend([
                "Try rephrasing your question",
                "Break complex requests into smaller parts",
                "Wait a moment and try again"
            ])
        elif isinstance(error, ExecutionError):
            suggestions.extend([
                "Check your data for any issues",
                "Try a simpler query",
                "Verify your database connection"
            ])
        elif isinstance(error, KnowledgeError):
            suggestions.extend([
                "Check if the knowledge base is properly configured",
                "Try searching with different keywords",
                "Verify the knowledge base contains relevant information"
            ])
        elif isinstance(error, ConfigurationError):
            suggestions.extend([
                "Check your configuration file syntax",
                "Verify all required environment variables are set",
                "Review the configuration documentation"
            ])
        
        return suggestions
    
    @staticmethod
    def should_retry(error: Exception, attempt: int = 1) -> bool:
        """Determine if an error should be retried."""
        if isinstance(error, RetryableError):
            return attempt <= error.max_retries
        
        # Retry on specific error types
        if isinstance(error, (LLMError, KnowledgeError)):
            if any(keyword in str(error).lower() for keyword in ["timeout", "connection", "rate limit"]):
                return attempt <= 3
        
        return False
    
    @staticmethod
    def get_retry_delay(error: Exception, attempt: int = 1) -> float:
        """Get delay before retry attempt."""
        if isinstance(error, RetryableError) and error.retry_after:
            return error.retry_after
        
        # Exponential backoff with jitter
        import random
        base_delay = min(2 ** attempt, 60)  # Cap at 60 seconds
        jitter = random.uniform(0.1, 0.3) * base_delay
        return base_delay + jitter


================================================
FILE: dataqa/logging_config.py
================================================
"""Structured logging configuration for DataQA."""

import json
import logging
import logging.config
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

from rich.console import Console
from rich.logging import RichHandler


class StructuredFormatter(logging.Formatter):
    """Custom formatter for structured JSON logging."""
    
    def format(self, record: logging.LogRecord) -> str:
        """Format log record as structured JSON."""
        log_entry = {
            "timestamp": datetime.fromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        
        # Add extra fields if present
        if hasattr(record, 'error_data'):
            log_entry["error_data"] = record.error_data
        
        if hasattr(record, 'performance_data'):
            log_entry["performance_data"] = record.performance_data
        
        if hasattr(record, 'user_context'):
            log_entry["user_context"] = record.user_context
        
        # Add exception information if present
        if record.exc_info:
            log_entry["exception"] = {
                "type": record.exc_info[0].__name__ if record.exc_info[0] else None,
                "message": str(record.exc_info[1]) if record.exc_info[1] else None,
                "traceback": self.formatException(record.exc_info) if record.exc_info else None
            }
        
        return json.dumps(log_entry, default=str)


class DataQALoggerAdapter(logging.LoggerAdapter):
    """Logger adapter that adds DataQA-specific context."""
    
    def __init__(self, logger: logging.Logger, extra: Optional[Dict[str, Any]] = None):
        """Initialize adapter with context."""
        super().__init__(logger, extra or {})
    
    def process(self, msg: str, kwargs: Dict[str, Any]) -> tuple[str, Dict[str, Any]]:
        """Process log message and add context."""
        # Add default context
        if 'extra' not in kwargs:
            kwargs['extra'] = {}
        
        kwargs['extra'].update(self.extra)
        return msg, kwargs
    
    def log_performance(
        self,
        operation: str,
        duration: float,
        *,
        success: bool = True,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """Log performance metrics."""
        performance_data = {
            "operation": operation,
            "duration_seconds": duration,
            "success": success,
            "details": details or {}
        }
        
        level = logging.INFO if success else logging.WARNING
        self.log(
            level,
            f"Performance: {operation} completed in {duration:.3f}s",
            extra={"performance_data": performance_data}
        )
    
    def log_user_action(
        self,
        action: str,
        user_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """Log user actions for audit trail."""
        user_context = {
            "action": action,
            "user_id": user_id,
            "conversation_id": conversation_id,
            "details": details or {}
        }
        
        self.info(
            f"User action: {action}",
            extra={"user_context": user_context}
        )
    
    def log_security_event(
        self,
        event_type: str,
        severity: str,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """Log security-related events."""
        security_data = {
            "event_type": event_type,
            "severity": severity,
            "details": details or {}
        }
        
        level = getattr(logging, severity.upper(), logging.WARNING)
        self.log(
            level,
            f"Security event: {event_type}",
            extra={"security_data": security_data}
        )


def setup_logging(
    level: str = "INFO",
    log_file: Optional[Path] = None,
    structured: bool = False,
    console_output: bool = True,
    rich_console: bool = True
) -> None:
    """Set up comprehensive logging configuration.
    
    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Path to log file (optional)
        structured: Use structured JSON logging
        console_output: Enable console output
        rich_console: Use Rich console formatting
    """
    
    # Convert level string to logging constant
    log_level = getattr(logging, level.upper(), logging.INFO)
    
    # Create handlers
    handlers = []
    
    # Console handler
    if console_output:
        if rich_console:
            console = Console()
            console_handler = RichHandler(
                console=console,
                rich_tracebacks=True,
                show_path=False,
                show_time=True
            )
            console_handler.setFormatter(
                logging.Formatter("%(message)s", datefmt="[%X]")
            )
        else:
            console_handler = logging.StreamHandler(sys.stdout)
            if structured:
                console_handler.setFormatter(StructuredFormatter())
            else:
                console_handler.setFormatter(
                    logging.Formatter(
                        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
                    )
                )
        
        console_handler.setLevel(log_level)
        handlers.append(console_handler)
    
    # File handler
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file)
        
        if structured:
            file_handler.setFormatter(StructuredFormatter())
        else:
            file_handler.setFormatter(
                logging.Formatter(
                    "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"
                )
            )
        
        file_handler.setLevel(log_level)
        handlers.append(file_handler)
    
    # Configure root logger
    logging.basicConfig(
        level=log_level,
        handlers=handlers,
        force=True
    )
    
    # Reduce noise from external libraries
    _configure_external_loggers()
    
    # Log configuration
    root_logger = logging.getLogger()
    root_logger.info(f"Logging configured: level={level}, structured={structured}, file={log_file}")


def _configure_external_loggers() -> None:
    """Configure external library loggers to reduce noise."""
    external_loggers = {
        "httpx": logging.WARNING,
        "openai": logging.WARNING,
        "sentence_transformers": logging.WARNING,
        "transformers": logging.WARNING,
        "torch": logging.WARNING,
        "faiss": logging.WARNING,
        "duckdb": logging.WARNING,
        "matplotlib": logging.WARNING,
        "PIL": logging.WARNING,
        "urllib3": logging.WARNING,
        "requests": logging.WARNING
    }
    
    for logger_name, level in external_loggers.items():
        logging.getLogger(logger_name).setLevel(level)


def get_logger(name: str, **context) -> DataQALoggerAdapter:
    """Get a DataQA logger with optional context.
    
    Args:
        name: Logger name (usually __name__)
        **context: Additional context to include in all log messages
    
    Returns:
        Configured logger adapter
    """
    base_logger = logging.getLogger(name)
    return DataQALoggerAdapter(base_logger, context)


def log_function_call(func_name: str, args: tuple = (), kwargs: Optional[Dict] = None):
    """Decorator to log function calls with arguments."""
    def decorator(func):
        def wrapper(*args, **kwargs):
            logger = get_logger(func.__module__)
            
            # Log function entry
            logger.debug(
                f"Entering {func_name}",
                extra={
                    "function_call": {
                        "function": func_name,
                        "args_count": len(args),
                        "kwargs_keys": list(kwargs.keys()) if kwargs else []
                    }
                }
            )
            
            try:
                result = func(*args, **kwargs)
                logger.debug(f"Exiting {func_name} successfully")
                return result
            except Exception as e:
                logger.error(
                    f"Exception in {func_name}: {e}",
                    extra={
                        "function_error": {
                            "function": func_name,
                            "error_type": type(e).__name__,
                            "error_message": str(e)
                        }
                    },
                    exc_info=True
                )
                raise
        
        return wrapper
    return decorator


class LoggingContext:
    """Context manager for adding temporary logging context."""
    
    def __init__(self, logger: DataQALoggerAdapter, **context):
        """Initialize logging context.
        
        Args:
            logger: Logger to add context to
            **context: Context to add
        """
        self.logger = logger
        self.context = context
        self.original_extra = logger.extra.copy()
    
    def __enter__(self):
        """Enter context and add logging context."""
        self.logger.extra.update(self.context)
        return self.logger
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit context and restore original context."""
        self.logger.extra = self.original_extra


# Pre-configured loggers for common use cases
def get_agent_logger(agent_name: str, conversation_id: Optional[str] = None) -> DataQALoggerAdapter:
    """Get logger for agent operations."""
    context = {"component": "agent", "agent_name": agent_name}
    if conversation_id:
        context["conversation_id"] = conversation_id
    return get_logger("dataqa.agent", **context)


def get_primitive_logger(primitive_type: str, primitive_name: str) -> DataQALoggerAdapter:
    """Get logger for primitive operations."""
    return get_logger(
        f"dataqa.primitives.{primitive_type}",
        component="primitive",
        primitive_type=primitive_type,
        primitive_name=primitive_name
    )


def get_workflow_logger(workflow_name: str, step: Optional[str] = None) -> DataQALoggerAdapter:
    """Get logger for workflow operations."""
    context = {"component": "workflow", "workflow_name": workflow_name}
    if step:
        context["workflow_step"] = step
    return get_logger("dataqa.workflow", **context)


def get_api_logger(endpoint: str, request_id: Optional[str] = None) -> DataQALoggerAdapter:
    """Get logger for API operations."""
    context = {"component": "api", "endpoint": endpoint}
    if request_id:
        context["request_id"] = request_id
    return get_logger("dataqa.api", **context)


================================================
FILE: dataqa/agent/__init__.py
================================================
"""Main DataAgent orchestration classes."""

from .agent import DataAgent
from .state import SharedState
from .workflow import DataAgentWorkflow

__all__ = ["DataAgent", "SharedState", "DataAgentWorkflow"]


================================================
FILE: dataqa/agent/agent.py
================================================
"""Main DataAgent class for high-level agent orchestration."""

import logging
from typing import Any, Dict, List, Optional

from ..config.models import AgentConfig
from ..primitives.executor import ExecutorPrimitive
from ..primitives.faiss_knowledge import FAISSKnowledge
from ..primitives.in_memory_executor import InMemoryExecutor
from ..primitives.knowledge import KnowledgePrimitive
from ..primitives.llm import LLMInterface, create_llm_interface
from ..models.document import Document
from ..models.message import Message
from .state import SharedState
from .workflow import DataAgentWorkflow

logger = logging.getLogger(__name__)


class DataAgent:
    """Main DataAgent class that orchestrates all components.
    
    This class provides a high-level interface for creating and managing
    data agents. It handles component initialization, workflow orchestration,
    and conversation state management.
    """
    
    def __init__(
        self,
        config: AgentConfig,
        llm: Optional[LLMInterface] = None,
        knowledge: Optional[KnowledgePrimitive] = None,
        executor: Optional[ExecutorPrimitive] = None
    ):
        """Initialize the DataAgent with configuration and optional components.
        
        Args:
            config: Agent configuration
            llm: Optional LLM interface (will be created from config if not provided)
            knowledge: Optional knowledge primitive (will be created from config if not provided)
            executor: Optional executor primitive (will be created from config if not provided)
        """
        self.config = config
        
        # Initialize components
        self.llm = llm or self._create_llm()
        self.knowledge = knowledge or self._create_knowledge()
        self.executor = executor or self._create_executor()
        
        # Initialize workflow
        self.workflow = DataAgentWorkflow(
            llm=self.llm,
            knowledge=self.knowledge,
            executor=self.executor,
            config=config
        )
        
        # Conversation management
        self._conversations: Dict[str, SharedState] = {}
        
        logger.info(f"DataAgent initialized: {config.name}")
    
    def _create_llm(self) -> LLMInterface:
        """Create LLM interface from configuration.
        
        Returns:
            Configured LLM interface
        """
        return create_llm_interface(self.config.llm)
    
    def _create_knowledge(self) -> KnowledgePrimitive:
        """Create knowledge primitive from configuration.
        
        Returns:
            Configured knowledge primitive
        """
        # For now, only FAISS is implemented
        knowledge_config = self.config.knowledge
        return FAISSKnowledge(
            model_name=knowledge_config.embedding_model,
            index_path=knowledge_config.index_path,
            embedding_dim=getattr(knowledge_config, 'embedding_dim', None)
        )
    
    def _create_executor(self) -> ExecutorPrimitive:
        """Create executor primitive from configuration.
        
        Returns:
            Configured executor primitive
        """
        # For now, only InMemoryExecutor is implemented
        executor_config = self.config.executor
        config_dict = {
            "database_path": getattr(executor_config, 'database_url', ":memory:"),
            "max_execution_time": executor_config.max_execution_time,
            "max_memory_mb": executor_config.max_memory_mb
        }
        return InMemoryExecutor(config_dict)
    
    async def query(
        self,
        query: str,
        conversation_id: Optional[str] = None
    ) -> str:
        """Process a user query and return a response.
        
        Args:
            query: User query to process
            conversation_id: Optional conversation ID for state persistence
            
        Returns:
            Agent response as a string
        """
        logger.info(f"Processing query: {query[:100]}...")
        
        # Use default conversation ID if not provided
        if conversation_id is None:
            conversation_id = "default"
        
        # Get existing conversation state if available
        existing_state = self._conversations.get(conversation_id)
        
        # Process the query through the workflow
        final_state = await self.workflow.process_query(
            query=query,
            conversation_id=conversation_id,
            existing_state=existing_state
        )
        
        # Store conversation state
        self._conversations[conversation_id] = final_state
        
        # Return the formatted response
        if final_state.formatted_response:
            return final_state.formatted_response
        elif final_state.error_occurred:
            return f"Error: {final_state.error_message}"
        else:
            return "I'm sorry, I couldn't process your query. Please try again."
    
    async def approve_operation(
        self,
        conversation_id: str,
        approved: bool = True,
        reason: Optional[str] = None
    ) -> str:
        """Approve or deny a pending operation.
        
        Args:
            conversation_id: Conversation ID with pending approval
            approved: Whether to approve the operation
            reason: Optional reason for the decision
            
        Returns:
            Response after processing the approval
        """
        logger.info(f"Processing approval for conversation {conversation_id}: {approved}")
        
        # Continue workflow with approval decision
        final_state = await self.workflow.continue_with_approval(
            conversation_id=conversation_id,
            approved=approved,
            reason=reason
        )
        
        # Update conversation state
        self._conversations[conversation_id] = final_state
        
        # Return the response
        if final_state.formatted_response:
            return final_state.formatted_response
        elif final_state.error_occurred:
            return f"Error: {final_state.error_message}"
        else:
            return "Operation processed."
    
    async def get_conversation_history(self, conversation_id: str) -> List[Message]:
        """Get conversation history for a specific conversation.
        
        Args:
            conversation_id: Conversation ID to retrieve
            
        Returns:
            List of messages in the conversation
        """
        state = self._conversations.get(conversation_id)
        if state:
            return state.conversation_history
        return []
    
    async def get_conversation_status(self, conversation_id: str) -> Dict[str, Any]:
        """Get the current status of a conversation.
        
        Args:
            conversation_id: Conversation ID to check
            
        Returns:
            Dictionary containing conversation status information
        """
        state = self._conversations.get(conversation_id)
        if not state:
            return {"exists": False}
        
        return {
            "exists": True,
            "current_step": state.current_step,
            "workflow_complete": state.workflow_complete,
            "error_occurred": state.error_occurred,
            "error_message": state.error_message,
            "pending_approval": state.pending_approval is not None,
            "iteration_count": state.iteration_count,
            "message_count": len(state.conversation_history)
        }
    
    async def clear_conversation(self, conversation_id: str) -> bool:
        """Clear a conversation from memory.
        
        Args:
            conversation_id: Conversation ID to clear
            
        Returns:
            True if conversation was cleared, False if it didn't exist
        """
        if conversation_id in self._conversations:
            del self._conversations[conversation_id]
            logger.info(f"Cleared conversation: {conversation_id}")
            return True
        return False
    
    async def ingest_knowledge(self, documents: List[Document]) -> None:
        """Ingest documents into the knowledge base.
        
        Args:
            documents: List of documents to ingest
        """
        logger.info(f"Ingesting {len(documents)} documents into knowledge base...")
        await self.knowledge.ingest(documents)
        logger.info("Knowledge ingestion complete")
    
    async def search_knowledge(
        self,
        query: str,
        limit: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """Search the knowledge base.
        
        Args:
            query: Search query
            limit: Maximum number of results
            filters: Optional search filters
            
        Returns:
            List of relevant documents
        """
        return await self.knowledge.search(query, limit, filters)
    
    async def get_database_schema(self, table_name: Optional[str] = None) -> Dict[str, Any]:
        """Get database schema information.
        
        Args:
            table_name: Optional specific table name
            
        Returns:
            Schema information
        """
        return await self.executor.get_schema(table_name)
    
    async def list_database_tables(self) -> List[str]:
        """List all available database tables.
        
        Returns:
            List of table names
        """
        return await self.executor.list_tables()
    
    def get_agent_info(self) -> Dict[str, Any]:
        """Get information about the agent configuration.
        
        Returns:
            Dictionary containing agent information
        """
        return {
            "name": self.config.name,
            "description": self.config.description,
            "version": self.config.version,
            "llm_provider": self.config.llm.provider.value,
            "llm_model": self.config.llm.model,
            "knowledge_provider": self.config.knowledge.provider.value,
            "executor_provider": self.config.executor.provider.value,
            "workflow_info": self.workflow.get_workflow_info(),
            "active_conversations": len(self._conversations)
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Perform a health check on all components.
        
        Returns:
            Health status of all components
        """
        health = {
            "agent": "healthy",
            "llm": "unknown",
            "knowledge": "unknown",
            "executor": "unknown",
            "timestamp": None
        }
        
        try:
            # Check LLM
            llm_info = await self.llm.get_model_info()
            health["llm"] = "healthy" if llm_info else "unhealthy"
        except Exception as e:
            health["llm"] = f"unhealthy: {e}"
        
        try:
            # Check knowledge base
            kb_stats = await self.knowledge.get_stats()
            health["knowledge"] = "healthy" if kb_stats else "unhealthy"
        except Exception as e:
            health["knowledge"] = f"unhealthy: {e}"
        
        try:
            # Check executor
            tables = await self.executor.list_tables()
            health["executor"] = "healthy" if isinstance(tables, list) else "unhealthy"
        except Exception as e:
            health["executor"] = f"unhealthy: {e}"
        
        from datetime import datetime
        health["timestamp"] = datetime.now().isoformat()
        
        return health
    
    async def shutdown(self) -> None:
        """Shutdown the agent and clean up resources.
        
        This method should be called when the agent is no longer needed
        to ensure proper cleanup of resources.
        """
        logger.info(f"Shutting down DataAgent: {self.config.name}")
        
        # Clear conversations
        self._conversations.clear()
        
        # Note: In a full implementation, we might need to close
        # database connections, save state, etc.
        
        logger.info("DataAgent shutdown complete")


async def create_agent_from_config(config: AgentConfig) -> DataAgent:
    """Factory function to create a DataAgent from configuration.
    
    Args:
        config: Agent configuration
        
    Returns:
        Initialized DataAgent instance
    """
    agent = DataAgent(config)
    
    # Perform initial health check
    health = await agent.health_check()
    logger.info(f"Agent health check: {health}")
    
    return agent


================================================
FILE: dataqa/agent/nodes.py
================================================
"""Workflow nodes for LangGraph-based agent orchestration."""

from typing import Any, Dict

from ..exceptions import ExecutionError, KnowledgeError, LLMError, WorkflowError
from ..logging_config import get_workflow_logger
from ..models.execution import ExecutionResult
from ..primitives.executor import ExecutorPrimitive
from ..primitives.knowledge import KnowledgePrimitive
from ..primitives.llm import LLMInterface
from ..utils.retry import retry_async
from .state import SharedState


class WorkflowNodes:
    """Collection of workflow nodes for agent orchestration.
    
    This class contains all the workflow nodes that can be used in LangGraph
    workflows. Each node is a function that takes the shared state and
    returns an updated state.
    """
    
    def __init__(
        self,
        llm: LLMInterface,
        knowledge: KnowledgePrimitive,
        executor: ExecutorPrimitive,
        require_approval: bool = True
    ):
        """Initialize workflow nodes with required components.
        
        Args:
            llm: LLM interface for code generation and analysis
            knowledge: Knowledge primitive for context retrieval
            executor: Executor primitive for code execution
            require_approval: Whether to require human approval for execution
        """
        self.llm = llm
        self.knowledge = knowledge
        self.executor = executor
        self.require_approval = require_approval
        self.logger = get_workflow_logger("workflow_nodes")
    
    async def query_processor(self, state: SharedState) -> SharedState:
        """Process and analyze the user query.
        
        This node analyzes the user's query to understand intent, extract entities,
        and determine the appropriate approach for generating a response.
        
        Args:
            state: Current shared state
            
        Returns:
            Updated state with query analysis results
        """
        self.logger.info(f"Processing query: {state.current_query[:100]}...")
        
        try:
            # Analyze the query using the LLM
            analysis = await self.llm.analyze_query(
                query=state.current_query,
                conversation_history=state.get_recent_messages()
            )
            
            state.query_analysis = analysis
            state.current_step = "context_retriever"
            
            self.logger.info(f"Query analysis complete: {analysis.get('query_type', 'unknown')}")
            
            # Check if clarification is needed
            if analysis.get("requires_clarification", False):
                # Generate clarification questions
                ambiguities = analysis.get("ambiguities", ["Query intent unclear"])
                clarification = await self.llm.generate_clarification(
                    query=state.current_query,
                    ambiguities=ambiguities,
                    conversation_history=state.get_recent_messages()
                )
                
                state.formatted_response = clarification
                state.workflow_complete = True
                state.current_step = "complete"
                
                self.logger.info("Query requires clarification, workflow complete")
            
        except LLMError as e:
            self.logger.error(f"LLM error in query processing: {e}")
            raise WorkflowError(
                f"Failed to analyze query: {e}",
                user_message="Failed to understand your question. Please try rephrasing it.",
                error_code="QUERY_PROCESSING_FAILED",
                original_error=e
            )
        except Exception as e:
            self.logger.error(f"Unexpected error in query processing: {e}")
            raise WorkflowError(
                f"Unexpected error during query analysis: {e}",
                user_message="An unexpected error occurred while processing your question.",
                error_code="QUERY_PROCESSING_UNEXPECTED_ERROR",
                original_error=e
            )
        
        return state
    
    async def context_retriever(self, state: SharedState) -> SharedState:
        """Retrieve relevant context from the knowledge base.
        
        This node searches the knowledge base for documents relevant to the
        user's query and prepares context for code generation.
        
        Args:
            state: Current shared state
            
        Returns:
            Updated state with retrieved context
        """
        self.logger.info("Retrieving context from knowledge base...")
        
        try:
            # Search for relevant documents
            documents = await self.knowledge.search(
                query=state.current_query,
                limit=5
            )
            
            state.retrieved_context = documents
            
            # Create context summary for LLM
            if documents:
                context_parts = []
                for doc in documents:
                    context_parts.append(f"Source: {doc.source}")
                    context_parts.append(f"Content: {doc.content[:500]}...")
                    if doc.metadata:
                        context_parts.append(f"Metadata: {doc.metadata}")
                    context_parts.append("---")
                
                state.context_summary = "\n".join(context_parts)
                self.logger.info(f"Retrieved {len(documents)} relevant documents")
            else:
                state.context_summary = "No relevant context found in knowledge base."
                self.logger.info("No relevant context found")
            
            state.current_step = "code_generator"
            
        except KnowledgeError as e:
            self.logger.error(f"Knowledge error in context retrieval: {e}")
            # Continue without context rather than failing
            state.retrieved_context = []
            state.context_summary = "Context retrieval failed, proceeding without additional context."
            state.current_step = "code_generator"
            self.logger.warning("Continuing without context due to retrieval error")
        except Exception as e:
            self.logger.error(f"Unexpected error in context retrieval: {e}")
            raise WorkflowError(
                f"Unexpected error during context retrieval: {e}",
                user_message="An error occurred while retrieving relevant information.",
                error_code="CONTEXT_RETRIEVAL_ERROR",
                original_error=e
            )
        
        return state
    
    async def code_generator(self, state: SharedState) -> SharedState:
        """Generate code based on the query and context.
        
        This node uses the LLM to generate appropriate SQL or Python code
        to answer the user's query, incorporating retrieved context.
        
        Args:
            state: Current shared state
            
        Returns:
            Updated state with generated code
        """
        self.logger.info("Generating code for query...")
        
        try:
            # Determine code type based on query analysis
            query_type = state.query_analysis.get("query_type", "analysis") if state.query_analysis else "analysis"
            
            # Default to SQL for data retrieval, Python for analysis/visualization
            if query_type in ["data_retrieval"]:
                code_type = "sql"
            else:
                code_type = "python"
            
            # Generate code using LLM
            generated_code = await self.llm.generate_code(
                query=state.current_query,
                context=state.context_summary,
                code_type=code_type,
                conversation_history=state.get_recent_messages()
            )
            
            state.generated_code = generated_code
            state.code_type = code_type
            
            # Validate the generated code
            validation = await self.llm.validate_generated_code(
                code=generated_code,
                code_type=code_type,
                context=state.context_summary
            )
            
            state.code_validation = validation
            
            self.logger.info(f"Generated {code_type} code, validation: {validation.get('is_valid', False)}")
            
            # Check if code is valid
            if not validation.get("is_valid", False):
                error_msg = f"Generated code failed validation: {validation.get('issues', [])}"
                self.logger.error(error_msg)
                state.set_error(error_msg)
                return state
            
            # Check if approval is required
            risk_level = validation.get("risk_level", "low")
            if self.require_approval and risk_level in ["medium", "high"]:
                state.pending_approval = generated_code
                state.current_step = "approval_gate"
                self.logger.info(f"Code requires approval due to {risk_level} risk level")
            else:
                state.current_step = "executor"
                self.logger.info("Code approved for execution")
            
        except LLMError as e:
            self.logger.error(f"LLM error in code generation: {e}")
            state.set_error(f"Failed to generate code: {e}")
        except Exception as e:
            self.logger.error(f"Unexpected error in code generation: {e}")
            state.set_error(f"Unexpected error during code generation: {e}")
        
        return state
    
    async def approval_gate(self, state: SharedState) -> SharedState:
        """Handle human-in-the-loop approval for code execution.
        
        This node manages the approval process for potentially risky operations.
        It provides detailed information about the code to be executed and
        handles various approval scenarios with proper error handling.
        
        Args:
            state: Current shared state
            
        Returns:
            Updated state with approval status
        """
        self.logger.info("Processing approval gate...")
        
        try:
            # Validate approval gate state
            if not state.pending_approval:
                self.logger.error("Approval gate called without pending approval")
                state.set_error("No operation pending approval")
                return state
            
            if state.approval_granted:
                # Approval already granted, proceed to execution
                state.current_step = "executor"
                self.logger.info("Approval granted, proceeding to execution")
                
                # Log approval metrics
                self.logger.info("Approval granted")
                state.metadata["approval_granted"] = {
                    "code_type": state.code_type,
                    "risk_level": state.code_validation.get('risk_level', 'unknown') if state.code_validation else 'unknown'
                }
                return state
            
            # Generate comprehensive approval request
            approval_request = await self._generate_approval_request(state)
            
            if not approval_request:
                state.set_error("Failed to generate approval request")
                return state
            
            state.formatted_response = approval_request
            state.workflow_complete = True
            state.current_step = "awaiting_approval"
            
            # Log approval request metrics
            self.logger.info("Approval requested")
            state.metadata["approval_requested"] = {
                "code_type": state.code_type,
                "risk_level": state.code_validation.get('risk_level', 'unknown') if state.code_validation else 'unknown',
                "security_concerns": len(state.code_validation.get('security_concerns', [])) if state.code_validation else 0
            }
            
            self.logger.info("Approval request formatted, workflow paused")
            
        except Exception as e:
            self.logger.error(f"Unexpected error in approval gate: {e}")
            state.set_error(f"Approval gate error: {e}")
            state.metadata["approval_gate_error"] = str(e)
        
        return state
    
    async def _generate_approval_request(self, state: SharedState) -> str:
        """Generate a comprehensive approval request message.
        
        Args:
            state: Current shared state
            
        Returns:
            Formatted approval request string
        """
        try:
            # Extract validation information
            validation = state.code_validation or {}
            risk_level = validation.get('risk_level', 'unknown')
            issues = validation.get('issues', [])
            security_concerns = validation.get('security_concerns', [])
            suggestions = validation.get('suggestions', [])
            
            # Build approval request
            request_parts = [
                "🔒 **CODE APPROVAL REQUIRED**",
                "",
                f"**Query:** {state.current_query}",
                f"**Code Type:** {state.code_type.upper()}",
                f"**Risk Level:** {risk_level.upper()}",
                "",
                "**Code to Execute:**",
                f"```{state.code_type}",
                state.pending_approval,
                "```",
                ""
            ]
            
            # Add validation details if available
            if issues:
                request_parts.extend([
                    "**⚠️ Validation Issues:**",
                    *[f"• {issue}" for issue in issues],
                    ""
                ])
            
            if security_concerns:
                request_parts.extend([
                    "**🛡️ Security Concerns:**",
                    *[f"• {concern}" for concern in security_concerns],
                    ""
                ])
            
            if suggestions:
                request_parts.extend([
                    "**💡 Suggestions:**",
                    *[f"• {suggestion}" for suggestion in suggestions],
                    ""
                ])
            
            # Add risk-specific warnings
            if risk_level == "high":
                request_parts.extend([
                    "**⚠️ HIGH RISK OPERATION**",
                    "This operation has been flagged as high risk. Please review carefully.",
                    ""
                ])
            elif risk_level == "medium":
                request_parts.extend([
                    "**⚠️ MEDIUM RISK OPERATION**",
                    "This operation requires review before execution.",
                    ""
                ])
            
            request_parts.extend([
                "**Please review and approve this code for execution.**",
                "",
                "Reply with 'approve' to execute or 'deny' to cancel."
            ])
            
            return "\n".join(request_parts)
            
        except Exception as e:
            self.logger.error(f"Error generating approval request: {e}")
            # Fallback to simple request
            return f"""
Code Approval Required

Code Type: {state.code_type}
Risk Level: {state.code_validation.get('risk_level', 'unknown') if state.code_validation else 'unknown'}

Code:
```{state.code_type}
{state.pending_approval}
```

Please review and approve this code for execution.
"""
    
    async def execute_code(self, state: SharedState) -> SharedState:
        """Execute the generated code safely.
        
        This node executes the validated code in a secure environment
        and captures the results for response formatting. Includes enhanced
        error handling and recovery mechanisms.
        
        Args:
            state: Current shared state
            
        Returns:
            Updated state with execution results
        """
        self.logger.info(f"Executing {state.code_type} code...")
        
        try:
            if not state.generated_code or not state.code_type:
                state.set_error("No code available for execution")
                return state
            
            # Validate state is ready for execution
            if not state.is_ready_for_execution():
                state.set_error("State not ready for execution - missing validation")
                return state
            
            # Execute the code based on type with timeout and resource limits
            self.logger.info("Starting code execution")
            
            if state.code_type == "sql":
                result = await self.executor.execute_sql(state.generated_code)
            elif state.code_type == "python":
                result = await self.executor.execute_python(state.generated_code)
            else:
                state.set_error(f"Unsupported code type: {state.code_type}")
                return state
            
            state.execution_results = result
            
            if result.success:
                self.logger.info(f"Code execution successful, execution time: {result.execution_time:.2f}s")
                state.current_step = "response_formatter"
                
                # Log execution metrics for monitoring
                state.metadata["execution_metrics"] = {
                    "execution_time": result.execution_time,
                    "code_type": state.code_type,
                    "output_type": result.output_type,
                    "success": True
                }
            else:
                self.logger.error(f"Code execution failed: {result.error}")
                
                # Attempt error recovery based on error type
                recovery_attempted = await self._attempt_execution_recovery(state, result)
                
                if not recovery_attempted:
                    state.set_error(f"Code execution failed: {result.error}")
                    state.metadata["execution_error"] = {
                        "error": result.error,
                        "code_executed": result.code_executed,
                        "execution_time": result.execution_time
                    }
            
        except ExecutionError as e:
            self.logger.error(f"Execution error: {e}")
            state.set_error(f"Failed to execute code: {e}")
            state.metadata["execution_exception"] = str(e)
        except Exception as e:
            self.logger.error(f"Unexpected error in code execution: {e}")
            state.set_error(f"Unexpected error during code execution: {e}")
            state.metadata["unexpected_error"] = str(e)
        
        return state
    
    async def _attempt_execution_recovery(self, state: SharedState, failed_result: ExecutionResult) -> bool:
        """Attempt to recover from execution errors.
        
        Args:
            state: Current shared state
            failed_result: The failed execution result
            
        Returns:
            True if recovery was attempted, False otherwise
        """
        error_msg = failed_result.error or ""
        
        # Common SQL error recovery patterns
        if state.code_type == "sql":
            # Check for column errors first (more specific)
            if "column" in error_msg.lower() and ("not exist" in error_msg.lower() or "does not exist" in error_msg.lower()):
                self.logger.info("Attempting recovery for missing column error")
                state.metadata["recovery_suggestion"] = "Column not found - consider checking table schema"
                return False
            # Then check for table errors
            elif "table" in error_msg.lower() and ("not exist" in error_msg.lower() or "does not exist" in error_msg.lower()):
                self.logger.info("Attempting recovery for missing table error")
                state.metadata["recovery_suggestion"] = "Table not found - consider checking available tables"
                return False
        
        # Common Python error recovery patterns
        elif state.code_type == "python":
            if "no module named" in error_msg.lower():
                self.logger.info("Attempting recovery for missing module error")
                state.metadata["recovery_suggestion"] = "Missing Python module - check available libraries"
                return False
            
            if "name" in error_msg.lower() and "not defined" in error_msg.lower():
                self.logger.info("Attempting recovery for undefined variable error")
                state.metadata["recovery_suggestion"] = "Variable not defined - check variable names"
                return False
        
        return False
    
    async def response_formatter(self, state: SharedState) -> SharedState:
        """Format the execution results into a user-friendly response.
        
        This node takes the raw execution results and formats them into
        a natural language response for the user. Includes enhanced error
        handling and fallback formatting mechanisms.
        
        Args:
            state: Current shared state
            
        Returns:
            Updated state with formatted response
        """
        self.logger.info("Formatting response for user...")
        
        try:
            if not state.execution_results:
                state.set_error("No execution results available for formatting")
                return state
            
            # Prepare results for formatting with enhanced metadata
            results_dict = {
                "success": state.execution_results.success,
                "data": state.execution_results.data,
                "execution_time": state.execution_results.execution_time,
                "output_type": state.execution_results.output_type,
                "code_executed": state.execution_results.code_executed,
                "query": state.current_query,
                "code_type": state.code_type,
                "metadata": state.metadata
            }
            
            # Add recovery suggestions if available
            if "recovery_suggestion" in state.metadata:
                results_dict["recovery_suggestion"] = state.metadata["recovery_suggestion"]
            
            # Format response using LLM with retry logic
            formatted_response = await self._format_response_with_retry(
                results_dict, state
            )
            
            if not formatted_response:
                # Use fallback formatting if LLM formatting fails
                formatted_response = self._generate_fallback_response(state)
            
            state.formatted_response = formatted_response
            state.workflow_complete = True
            state.current_step = "complete"
            
            # Log formatting metrics
            self.logger.info("Response formatting complete")
            state.metadata["response_formatted"] = {
                "response_length": len(formatted_response),
                "fallback_used": "fallback" in formatted_response.lower()
            }
            
            self.logger.info("Response formatting complete")
            
        except Exception as e:
            self.logger.error(f"Unexpected error in response formatting: {e}")
            
            # Generate emergency fallback response
            try:
                fallback_response = self._generate_fallback_response(state)
                state.formatted_response = fallback_response
                state.workflow_complete = True
                state.current_step = "complete"
                state.metadata["emergency_fallback"] = True
                self.logger.warning("Using emergency fallback response")
            except Exception as fallback_error:
                self.logger.error(f"Emergency fallback failed: {fallback_error}")
                state.set_error(f"Response formatting failed: {e}")
        
        return state
    
    async def _format_response_with_retry(self, results_dict: dict, state: SharedState, max_retries: int = 2) -> str:
        """Format response with retry logic for LLM failures.
        
        Args:
            results_dict: Results dictionary for formatting
            state: Current shared state
            max_retries: Maximum number of retry attempts
            
        Returns:
            Formatted response string or empty string if all attempts fail
        """
        for attempt in range(max_retries + 1):
            try:
                formatted_response = await self.llm.format_response(
                    results=results_dict,
                    query=state.current_query,
                    conversation_history=state.get_recent_messages()
                )
                return formatted_response
                
            except LLMError as e:
                self.logger.warning(f"LLM formatting attempt {attempt + 1} failed: {e}")
                if attempt == max_retries:
                    self.logger.error("All LLM formatting attempts failed")
                    return ""
                # Wait briefly before retry (in a real implementation)
                continue
            except Exception as e:
                self.logger.error(f"Unexpected error in formatting attempt {attempt + 1}: {e}")
                return ""
        
        return ""
    
    def _generate_fallback_response(self, state: SharedState) -> str:
        """Generate a fallback response when LLM formatting fails.
        
        Args:
            state: Current shared state
            
        Returns:
            Fallback formatted response
        """
        try:
            if not state.execution_results:
                return "Query processing completed, but no results are available."
            
            result = state.execution_results
            
            # Build fallback response based on execution results
            response_parts = [
                f"✅ **Query Executed Successfully**",
                "",
                f"**Query:** {state.current_query}",
                f"**Execution Time:** {result.execution_time:.2f} seconds",
                f"**Code Type:** {state.code_type.upper() if state.code_type else 'Unknown'}",
                ""
            ]
            
            # Add data summary
            if result.data:
                if isinstance(result.data, dict):
                    if "columns" in result.data and "data" in result.data:
                        # DataFrame-like structure
                        num_rows = len(result.data["data"]) if result.data["data"] else 0
                        num_cols = len(result.data["columns"]) if result.data["columns"] else 0
                        response_parts.extend([
                            f"**Results:** {num_rows} rows, {num_cols} columns",
                            ""
                        ])
                    else:
                        # Generic dictionary
                        response_parts.extend([
                            f"**Results:** {len(result.data)} data items",
                            ""
                        ])
                elif isinstance(result.data, list):
                    response_parts.extend([
                        f"**Results:** {len(result.data)} items",
                        ""
                    ])
                else:
                    response_parts.extend([
                        "**Results:** Data available",
                        ""
                    ])
            
            # Add output type information
            if result.output_type:
                response_parts.extend([
                    f"**Output Type:** {result.output_type.title()}",
                    ""
                ])
            
            # Add recovery suggestions if available
            if "recovery_suggestion" in state.metadata:
                response_parts.extend([
                    "**Note:** " + state.metadata["recovery_suggestion"],
                    ""
                ])
            
            # Add code executed (truncated)
            if result.code_executed:
                code_preview = result.code_executed[:200]
                if len(result.code_executed) > 200:
                    code_preview += "..."
                
                response_parts.extend([
                    "**Code Executed:**",
                    f"```{state.code_type or 'text'}",
                    code_preview,
                    "```"
                ])
            
            return "\n".join(response_parts)
            
        except Exception as e:
            self.logger.error(f"Fallback response generation failed: {e}")
            return f"Query completed. Execution time: {state.execution_results.execution_time:.2f}s" if state.execution_results else "Query completed."
    
    def grant_approval(self, state: SharedState) -> SharedState:
        """Grant approval for pending operations.
        
        This is a utility method that can be called externally to grant
        approval for operations waiting in the approval gate.
        
        Args:
            state: Current shared state
            
        Returns:
            Updated state with approval granted
        """
        if state.pending_approval:
            state.approval_granted = True
            state.current_step = "executor"
            self.logger.info("Approval granted for pending operation")
        else:
            self.logger.warning("No pending approval to grant")
        
        return state
    
    def deny_approval(self, state: SharedState, reason: str = "User denied approval") -> SharedState:
        """Deny approval for pending operations.
        
        Args:
            state: Current shared state
            reason: Reason for denial
            
        Returns:
            Updated state with approval denied
        """
        if state.pending_approval:
            state.set_error(f"Operation denied: {reason}")
            logger.info(f"Approval denied: {reason}")
        else:
            logger.warning("No pending approval to deny")
        
        return state


================================================
FILE: dataqa/agent/state.py
================================================
"""Shared state model for LangGraph-based agent orchestration."""

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field

from ..models.document import Document
from ..models.execution import ExecutionResult
from ..models.message import Message


class SharedState(BaseModel):
    """Shared state for conversation and workflow management.
    
    This model maintains the state throughout the agent's workflow execution,
    including conversation history, retrieved context, generated code, and
    execution results. It serves as the central state container for LangGraph
    workflow orchestration.
    """
    
    # Core conversation state
    conversation_history: List[Message] = Field(
        default_factory=list,
        description="Complete conversation history between user and agent"
    )
    current_query: str = Field(
        default="",
        description="The current user query being processed"
    )
    
    # Query analysis results
    query_analysis: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Analysis results from query processing (intent, entities, etc.)"
    )
    
    # Knowledge retrieval state
    retrieved_context: List[Document] = Field(
        default_factory=list,
        description="Documents retrieved from knowledge base for current query"
    )
    context_summary: Optional[str] = Field(
        default=None,
        description="Summary of retrieved context for LLM consumption"
    )
    
    # Code generation state
    generated_code: Optional[str] = Field(
        default=None,
        description="Code generated by the LLM for the current query"
    )
    code_type: Optional[str] = Field(
        default=None,
        description="Type of generated code (sql, python)"
    )
    code_validation: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Validation results for generated code"
    )
    
    # Execution state
    execution_results: Optional[ExecutionResult] = Field(
        default=None,
        description="Results from code execution"
    )
    
    # Approval workflow state
    pending_approval: Optional[str] = Field(
        default=None,
        description="Code or operation pending human approval"
    )
    approval_granted: bool = Field(
        default=False,
        description="Whether approval has been granted for pending operation"
    )
    
    # Response formatting state
    formatted_response: Optional[str] = Field(
        default=None,
        description="Final formatted response for the user"
    )
    
    # Workflow control state
    current_step: str = Field(
        default="query_processor",
        description="Current workflow step being executed"
    )
    workflow_complete: bool = Field(
        default=False,
        description="Whether the workflow has completed successfully"
    )
    error_occurred: bool = Field(
        default=False,
        description="Whether an error occurred during workflow execution"
    )
    error_message: Optional[str] = Field(
        default=None,
        description="Error message if an error occurred"
    )
    
    # Iteration control
    iteration_count: int = Field(
        default=0,
        description="Number of workflow iterations completed"
    )
    max_iterations: int = Field(
        default=10,
        description="Maximum allowed workflow iterations"
    )
    
    # Additional metadata
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional metadata for workflow execution"
    )
    
    def add_message(self, role: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> None:
        """Add a message to the conversation history.
        
        Args:
            role: Message role (user, assistant, system)
            content: Message content
            metadata: Optional message metadata
        """
        message = Message(
            role=role,  # type: ignore
            content=content,
            metadata=metadata
        )
        self.conversation_history.append(message)
    
    def get_recent_messages(self, limit: int = 5) -> List[Message]:
        """Get recent messages from conversation history.
        
        Args:
            limit: Maximum number of recent messages to return
            
        Returns:
            List of recent messages
        """
        return self.conversation_history[-limit:] if self.conversation_history else []
    
    def reset_for_new_query(self, query: str) -> None:
        """Reset state for processing a new query.
        
        Args:
            query: The new query to process
        """
        self.current_query = query
        self.query_analysis = None
        self.retrieved_context = []
        self.context_summary = None
        self.generated_code = None
        self.code_type = None
        self.code_validation = None
        self.execution_results = None
        self.pending_approval = None
        self.approval_granted = False
        self.formatted_response = None
        self.current_step = "query_processor"
        self.workflow_complete = False
        self.error_occurred = False
        self.error_message = None
        self.iteration_count = 0
    
    def set_error(self, error_message: str) -> None:
        """Set error state for the workflow.
        
        Args:
            error_message: Description of the error that occurred
        """
        self.error_occurred = True
        self.error_message = error_message
        self.workflow_complete = True
    
    def increment_iteration(self) -> bool:
        """Increment iteration count and check if max iterations reached.
        
        Returns:
            True if max iterations not reached, False otherwise
        """
        self.iteration_count += 1
        if self.iteration_count >= self.max_iterations:
            self.set_error(f"Maximum iterations ({self.max_iterations}) reached")
            return False
        return True
    
    def is_ready_for_execution(self) -> bool:
        """Check if state is ready for code execution.
        
        Returns:
            True if generated code is available and validated
        """
        return (
            self.generated_code is not None and
            self.code_type is not None and
            self.code_validation is not None and
            self.code_validation.get("is_valid", False)
        )
    
    def requires_approval(self) -> bool:
        """Check if current operation requires human approval.
        
        Returns:
            True if approval is required
        """
        return (
            self.pending_approval is not None and
            not self.approval_granted
        )


================================================
FILE: dataqa/agent/workflow.py
================================================
"""LangGraph workflow implementation for DataQA agent orchestration."""

from typing import Any, Dict, Optional

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from ..config.models import AgentConfig
from ..exceptions import WorkflowError
from ..logging_config import get_workflow_logger
from ..primitives.executor import ExecutorPrimitive
from ..primitives.knowledge import KnowledgePrimitive
from ..primitives.llm import LLMInterface
from ..utils.retry import retry_async
from .nodes import WorkflowNodes
from .state import SharedState


class DataAgentWorkflow:
    """LangGraph-based workflow for DataQA agent orchestration.
    
    This class implements the main agent workflow using LangGraph's state
    management and graph execution capabilities. It coordinates between
    query processing, context retrieval, code generation, and execution.
    """
    
    def __init__(
        self,
        llm: LLMInterface,
        knowledge: KnowledgePrimitive,
        executor: ExecutorPrimitive,
        config: AgentConfig
    ):
        """Initialize the workflow with required components.
        
        Args:
            llm: LLM interface for code generation and analysis
            knowledge: Knowledge primitive for context retrieval
            executor: Executor primitive for code execution
            config: Agent configuration
        """
        self.llm = llm
        self.knowledge = knowledge
        self.executor = executor
        self.config = config
        
        # Initialize workflow nodes
        self.nodes = WorkflowNodes(
            llm=llm,
            knowledge=knowledge,
            executor=executor,
            require_approval=config.workflow.require_approval
        )
        
        # Initialize memory for conversation persistence
        self.memory = MemorySaver()
        
        # Initialize logger
        self.logger = get_workflow_logger(config.name)
        
        # Build the workflow graph
        self.graph = self._build_graph()
        
        self.logger.info(f"DataAgent workflow initialized: {config.name}")
    
    def _build_graph(self) -> StateGraph:
        """Build the LangGraph workflow graph.
        
        Returns:
            Configured StateGraph for workflow execution
        """
        # Create the graph with SharedState
        workflow = StateGraph(SharedState)
        
        # Add nodes
        workflow.add_node("query_processor", self.nodes.query_processor)
        workflow.add_node("context_retriever", self.nodes.context_retriever)
        workflow.add_node("code_generator", self.nodes.code_generator)
        workflow.add_node("approval_gate", self.nodes.approval_gate)
        workflow.add_node("executor", self.nodes.execute_code)
        workflow.add_node("response_formatter", self.nodes.response_formatter)
        
        # Set entry point
        workflow.set_entry_point("query_processor")
        
        # Add conditional edges based on state
        workflow.add_conditional_edges(
            "query_processor",
            self._route_from_query_processor,
            {
                "context_retriever": "context_retriever",
                "complete": END
            }
        )
        
        workflow.add_edge("context_retriever", "code_generator")
        
        workflow.add_conditional_edges(
            "code_generator",
            self._route_from_code_generator,
            {
                "approval_gate": "approval_gate",
                "executor": "executor",
                "error": END
            }
        )
        
        workflow.add_conditional_edges(
            "approval_gate",
            self._route_from_approval_gate,
            {
                "executor": "executor",
                "awaiting_approval": END,
                "error": END
            }
        )
        
        workflow.add_conditional_edges(
            "executor",
            self._route_from_executor,
            {
                "response_formatter": "response_formatter",
                "error": END
            }
        )
        
        workflow.add_edge("response_formatter", END)
        
        return workflow.compile(checkpointer=self.memory)
    
    def _route_from_query_processor(self, state: SharedState) -> str:
        """Route from query processor based on state.
        
        Args:
            state: Current shared state
            
        Returns:
            Next node to execute
        """
        if state.workflow_complete:
            return "complete"
        elif state.error_occurred:
            return "complete"
        else:
            return "context_retriever"
    
    def _route_from_code_generator(self, state: SharedState) -> str:
        """Route from code generator based on state.
        
        Args:
            state: Current shared state
            
        Returns:
            Next node to execute
        """
        if state.error_occurred:
            return "error"
        elif state.pending_approval:
            return "approval_gate"
        else:
            return "executor"
    
    def _route_from_approval_gate(self, state: SharedState) -> str:
        """Route from approval gate based on state.
        
        Args:
            state: Current shared state
            
        Returns:
            Next node to execute
        """
        if state.error_occurred:
            return "error"
        elif state.workflow_complete:
            return "awaiting_approval"
        elif state.approval_granted:
            return "executor"
        else:
            return "awaiting_approval"
    
    def _route_from_executor(self, state: SharedState) -> str:
        """Route from executor based on state.
        
        Args:
            state: Current shared state
            
        Returns:
            Next node to execute
        """
        if state.error_occurred:
            return "error"
        else:
            return "response_formatter"
    
    async def process_query(
        self,
        query: str,
        conversation_id: Optional[str] = None,
        existing_state: Optional[SharedState] = None
    ) -> SharedState:
        """Process a user query through the workflow.
        
        Args:
            query: User query to process
            conversation_id: Optional conversation ID for state persistence
            existing_state: Optional existing state to continue from
            
        Returns:
            Final state after workflow execution
        """
        self.logger.info(f"Processing query: {query[:100]}...")
        
        # Initialize or update state
        if existing_state:
            state = existing_state
            state.reset_for_new_query(query)
        else:
            state = SharedState(
                current_query=query,
                max_iterations=self.config.workflow.max_iterations
            )
        
        # Add user message to conversation history
        state.add_message("user", query)
        
        # Configure execution
        config = {
            "configurable": {
                "thread_id": conversation_id or "default"
            }
        }
        
        try:
            # Execute the workflow
            result = await self.graph.ainvoke(
                state,
                config=config
            )
            
            # LangGraph returns a dictionary representation of the state
            # We need to reconstruct the SharedState object
            if isinstance(result, dict):
                final_state = SharedState.model_validate(result)
            else:
                final_state = result
            
            # Add assistant response to conversation history
            if final_state.formatted_response:
                final_state.add_message("assistant", final_state.formatted_response)
            
            self.logger.info(f"Query processing complete: {final_state.current_step}")
            return final_state
            
        except Exception as e:
            self.logger.error(f"Workflow execution failed: {e}")
            raise WorkflowError(
                f"Workflow execution failed: {e}",
                user_message="An error occurred while processing your request. Please try again.",
                error_code="WORKFLOW_EXECUTION_FAILED",
                original_error=e
            )
    
    async def continue_with_approval(
        self,
        conversation_id: str,
        approved: bool,
        reason: Optional[str] = None
    ) -> SharedState:
        """Continue workflow execution after approval decision.
        
        Args:
            conversation_id: Conversation ID to continue
            approved: Whether the operation was approved
            reason: Optional reason for approval/denial
            
        Returns:
            Updated state after approval processing
        """
        self.logger.info(f"Continuing workflow with approval: {approved}")
        
        # Get current state from memory
        config = {
            "configurable": {
                "thread_id": conversation_id
            }
        }
        
        try:
            # Get the current state
            current_state = await self.graph.aget_state(config)
            if not current_state or not current_state.values:
                raise ValueError("No conversation state found")
            
            state = current_state.values["state"]
            
            # Process approval decision
            if approved:
                self.nodes.grant_approval(state)
            else:
                self.nodes.deny_approval(state, reason or "User denied approval")
            
            # Continue execution if approved
            if approved and not state.error_occurred:
                final_state = await self.graph.ainvoke(
                    state,
                    config=config
                )
                
                # Add assistant response to conversation history
                if final_state.formatted_response:
                    final_state.add_message("assistant", final_state.formatted_response)
                
                return final_state
            else:
                return state
                
        except Exception as e:
            self.logger.error(f"Approval continuation failed: {e}")
            # Create error state
            error_state = SharedState()
            error_state.set_error(f"Approval continuation failed: {e}")
            return error_state
    
    async def get_conversation_state(self, conversation_id: str) -> Optional[SharedState]:
        """Get the current conversation state.
        
        Args:
            conversation_id: Conversation ID to retrieve
            
        Returns:
            Current conversation state or None if not found
        """
        config = {
            "configurable": {
                "thread_id": conversation_id
            }
        }
        
        try:
            current_state = await self.graph.aget_state(config)
            if current_state and current_state.values:
                return current_state.values["state"]
            return None
        except Exception as e:
            self.logger.error(f"Failed to get conversation state: {e}")
            return None
    
    def get_workflow_info(self) -> Dict[str, Any]:
        """Get information about the workflow configuration.
        
        Returns:
            Dictionary containing workflow information
        """
        return {
            "agent_name": self.config.name,
            "workflow_strategy": self.config.workflow.strategy,
            "max_iterations": self.config.workflow.max_iterations,
            "require_approval": self.config.workflow.require_approval,
            "auto_approve_safe": self.config.workflow.auto_approve_safe,
            "conversation_memory": self.config.workflow.conversation_memory,
            "enable_visualization": self.config.workflow.enable_visualization,
            "nodes": [
                "query_processor",
                "context_retriever", 
                "code_generator",
                "approval_gate",
                "executor",
                "response_formatter"
            ]
        }


================================================
FILE: dataqa/cli/__init__.py
================================================
"""Command-line interface for DataQA."""

from .main import main

__all__ = ["main"]


================================================
FILE: dataqa/cli/main.py
================================================
"""Main CLI entry point for DataQA."""

import asyncio
import logging
import sys
from pathlib import Path
from typing import List, Optional

import typer
from rich.console import Console
from rich.logging import RichHandler
from rich.panel import Panel
from rich.prompt import Confirm, Prompt
from rich.table import Table
from rich.text import Text

from ..agent.agent import DataAgent, create_agent_from_config
from ..config.loader import (
    ConfigurationError,
    create_example_config,
    load_agent_config,
    validate_environment,
)
from ..models.document import Document

# Initialize Rich console for beautiful output
console = Console()

# Create the main Typer app
app = typer.Typer(
    name="dataqa",
    help="DataQA - A composable data agent framework for natural language data interaction",
    rich_markup_mode="rich",
    no_args_is_help=True,
)


def setup_logging(log_level: str = "INFO", debug: bool = False) -> None:
    """Set up logging with Rich handler."""
    level = logging.DEBUG if debug else getattr(logging, log_level.upper(), logging.INFO)
    
    logging.basicConfig(
        level=level,
        format="%(message)s",
        datefmt="[%X]",
        handlers=[RichHandler(console=console, rich_tracebacks=True)]
    )
    
    # Reduce noise from external libraries
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("openai").setLevel(logging.WARNING)
    logging.getLogger("sentence_transformers").setLevel(logging.WARNING)


@app.command()
def run(
    config_path: Path = typer.Option(
        Path("config/example_agent.yaml"),
        "--config", "-c",
        help="Path to agent configuration file",
        exists=True,
        file_okay=True,
        dir_okay=False,
    ),
    conversation_id: Optional[str] = typer.Option(
        None,
        "--conversation", "-conv",
        help="Conversation ID for session persistence"
    ),
    debug: bool = typer.Option(
        False,
        "--debug", "-d",
        help="Enable debug logging"
    ),
    auto_approve: bool = typer.Option(
        False,
        "--auto-approve", "-y",
        help="Auto-approve all operations (use with caution)"
    )
) -> None:
    """Run an interactive DataQA agent session."""
    
    setup_logging(debug=debug)
    
    try:
        # Load configuration
        console.print(f"[blue]Loading configuration from {config_path}[/blue]")
        config = load_agent_config(config_path)
        
        # Override approval setting if auto-approve is enabled
        if auto_approve:
            config.workflow.require_approval = False
            console.print("[yellow]Warning: Auto-approval enabled. All operations will execute without confirmation.[/yellow]")
        
        # Run the interactive session
        asyncio.run(_run_interactive_session(config, conversation_id))
        
    except ConfigurationError as e:
        console.print(f"[red]Configuration Error: {e}[/red]")
        raise typer.Exit(1)
    except KeyboardInterrupt:
        console.print("\n[yellow]Session interrupted by user[/yellow]")
        raise typer.Exit(0)
    except Exception as e:
        console.print(f"[red]Unexpected error: {e}[/red]")
        if debug:
            console.print_exception()
        raise typer.Exit(1)


async def _run_interactive_session(config, conversation_id: Optional[str]) -> None:
    """Run the interactive agent session."""
    
    # Create agent
    console.print("[blue]Initializing DataQA agent...[/blue]")
    agent = await create_agent_from_config(config)
    
    # Display agent info
    info = agent.get_agent_info()
    
    info_table = Table(title="Agent Information")
    info_table.add_column("Property", style="cyan")
    info_table.add_column("Value", style="green")
    
    info_table.add_row("Name", info["name"])
    info_table.add_row("Description", info.get("description", "N/A"))
    info_table.add_row("LLM Provider", f"{info['llm_provider']} ({info['llm_model']})")
    info_table.add_row("Knowledge Provider", info["knowledge_provider"])
    info_table.add_row("Executor Provider", info["executor_provider"])
    
    console.print(info_table)
    console.print()
    
    # Health check
    console.print("[blue]Performing health check...[/blue]")
    health = await agent.health_check()
    
    health_table = Table(title="Component Health")
    health_table.add_column("Component", style="cyan")
    health_table.add_column("Status", style="green")
    
    for component, status in health.items():
        if component == "timestamp":
            continue
        
        if status == "healthy":
            status_text = "[green]✓ Healthy[/green]"
        elif isinstance(status, str) and status.startswith("unhealthy"):
            status_text = f"[red]✗ {status}[/red]"
        else:
            status_text = f"[yellow]? {status}[/yellow]"
        
        health_table.add_row(component.title(), status_text)
    
    console.print(health_table)
    console.print()
    
    # Interactive loop
    console.print("[green]DataQA agent is ready! Type your questions or 'quit' to exit.[/green]")
    console.print("[dim]Commands: /help, /status, /history, /clear, /quit[/dim]")
    console.print()
    
    try:
        while True:
            # Get user input
            query = Prompt.ask("[bold blue]You[/bold blue]")
            
            if not query.strip():
                continue
            
            # Handle special commands
            if query.lower() in ["/quit", "/exit", "quit", "exit"]:
                break
            elif query.lower() == "/help":
                _show_help()
                continue
            elif query.lower() == "/status":
                await _show_status(agent, conversation_id)
                continue
            elif query.lower() == "/history":
                await _show_history(agent, conversation_id)
                continue
            elif query.lower() == "/clear":
                if conversation_id:
                    cleared = await agent.clear_conversation(conversation_id)
                    if cleared:
                        console.print("[green]Conversation history cleared.[/green]")
                    else:
                        console.print("[yellow]No conversation history to clear.[/yellow]")
                else:
                    console.print("[yellow]No conversation ID specified.[/yellow]")
                continue
            
            # Process the query
            console.print("[blue]Agent[/blue]: Processing your query...")
            
            try:
                response = await agent.query(query, conversation_id)
                
                # Display response in a panel
                console.print(Panel(
                    response,
                    title="[bold green]Agent Response[/bold green]",
                    border_style="green"
                ))
                
                # Check if there's a pending approval
                if conversation_id:
                    status = await agent.get_conversation_status(conversation_id)
                    if status.get("pending_approval"):
                        await _handle_approval(agent, conversation_id)
                
            except Exception as e:
                console.print(f"[red]Error processing query: {e}[/red]")
                logging.exception("Query processing error")
    
    finally:
        # Cleanup
        console.print("[blue]Shutting down agent...[/blue]")
        await agent.shutdown()
        console.print("[green]Goodbye![/green]")


def _show_help() -> None:
    """Show help information."""
    help_text = """
[bold]Available Commands:[/bold]

[cyan]/help[/cyan]     - Show this help message
[cyan]/status[/cyan]   - Show conversation status
[cyan]/history[/cyan]  - Show conversation history
[cyan]/clear[/cyan]    - Clear conversation history
[cyan]/quit[/cyan]     - Exit the session

[bold]Usage Tips:[/bold]

• Ask questions about your data in natural language
• The agent will generate and execute SQL/Python code
• Review generated code before approving execution
• Use conversation ID to maintain context across sessions
"""
    console.print(Panel(help_text, title="Help", border_style="blue"))


async def _show_status(agent: DataAgent, conversation_id: Optional[str]) -> None:
    """Show conversation status."""
    if not conversation_id:
        console.print("[yellow]No conversation ID specified.[/yellow]")
        return
    
    status = await agent.get_conversation_status(conversation_id)
    
    if not status.get("exists"):
        console.print("[yellow]No active conversation found.[/yellow]")
        return
    
    status_table = Table(title="Conversation Status")
    status_table.add_column("Property", style="cyan")
    status_table.add_column("Value", style="green")
    
    status_table.add_row("Current Step", status.get("current_step", "N/A"))
    status_table.add_row("Workflow Complete", str(status.get("workflow_complete", False)))
    status_table.add_row("Error Occurred", str(status.get("error_occurred", False)))
    status_table.add_row("Pending Approval", str(status.get("pending_approval", False)))
    status_table.add_row("Iteration Count", str(status.get("iteration_count", 0)))
    status_table.add_row("Message Count", str(status.get("message_count", 0)))
    
    if status.get("error_message"):
        status_table.add_row("Error Message", status["error_message"])
    
    console.print(status_table)


async def _show_history(agent: DataAgent, conversation_id: Optional[str]) -> None:
    """Show conversation history."""
    if not conversation_id:
        console.print("[yellow]No conversation ID specified.[/yellow]")
        return
    
    history = await agent.get_conversation_history(conversation_id)
    
    if not history:
        console.print("[yellow]No conversation history found.[/yellow]")
        return
    
    console.print(f"[bold]Conversation History ({len(history)} messages):[/bold]")
    console.print()
    
    for i, message in enumerate(history[-10:], 1):  # Show last 10 messages
        role_color = {
            "user": "blue",
            "assistant": "green",
            "system": "yellow"
        }.get(message.role, "white")
        
        console.print(f"[{role_color}]{message.role.title()}[/{role_color}]: {message.content[:200]}...")
        if i < len(history[-10:]):
            console.print()


async def _handle_approval(agent: DataAgent, conversation_id: str) -> None:
    """Handle pending approval requests."""
    console.print("[yellow]⚠️  Operation requires approval[/yellow]")
    
    approved = Confirm.ask("Do you want to proceed with the operation?")
    
    try:
        response = await agent.approve_operation(conversation_id, approved)
        console.print(Panel(
            response,
            title="[bold green]Operation Result[/bold green]" if approved else "[bold red]Operation Cancelled[/bold red]",
            border_style="green" if approved else "red"
        ))
    except Exception as e:
        console.print(f"[red]Error processing approval: {e}[/red]")


@app.command()
def ingest(
    config_path: Path = typer.Option(
        Path("config/example_agent.yaml"),
        "--config", "-c",
        help="Path to agent configuration file",
        exists=True,
        file_okay=True,
        dir_okay=False,
    ),
    documents: List[Path] = typer.Argument(
        ...,
        help="Paths to documents to ingest into knowledge base"
    ),
    recursive: bool = typer.Option(
        False,
        "--recursive", "-r",
        help="Recursively process directories"
    ),
    file_pattern: str = typer.Option(
        "*.txt,*.md,*.pdf,*.docx",
        "--pattern", "-p",
        help="File pattern for recursive processing (comma-separated)"
    ),
    debug: bool = typer.Option(
        False,
        "--debug", "-d",
        help="Enable debug logging"
    )
) -> None:
    """Ingest documents into the agent's knowledge base."""
    
    setup_logging(debug=debug)
    
    try:
        # Load configuration
        console.print(f"[blue]Loading configuration from {config_path}[/blue]")
        config = load_agent_config(config_path)
        
        # Run the ingestion
        asyncio.run(_run_ingestion(config, documents, recursive, file_pattern))
        
    except ConfigurationError as e:
        console.print(f"[red]Configuration Error: {e}[/red]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]Ingestion failed: {e}[/red]")
        if debug:
            console.print_exception()
        raise typer.Exit(1)


async def _run_ingestion(config, documents: List[Path], recursive: bool, file_pattern: str) -> None:
    """Run the document ingestion process."""
    
    # Create agent
    console.print("[blue]Initializing DataQA agent...[/blue]")
    agent = await create_agent_from_config(config)
    
    try:
        # Collect files to process
        files_to_process = []
        
        for doc_path in documents:
            if doc_path.is_file():
                files_to_process.append(doc_path)
            elif doc_path.is_dir() and recursive:
                # Use glob pattern to find files
                # Handle comma-separated patterns like "*.txt,*.md,*.pdf"
                patterns = [p.strip() for p in file_pattern.split(',')]
                for pattern in patterns:
                    files_to_process.extend(doc_path.rglob(pattern))
            else:
                console.print(f"[yellow]Skipping {doc_path} (not a file or recursive not enabled)[/yellow]")
        
        if not files_to_process:
            console.print("[yellow]No files found to process.[/yellow]")
            return
        
        console.print(f"[green]Found {len(files_to_process)} files to process[/green]")
        
        # Process files
        documents_to_ingest = []
        
        for file_path in files_to_process:
            try:
                console.print(f"[blue]Processing {file_path}[/blue]")
                
                # Read file content
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # Create document
                doc = Document(
                    content=content,
                    metadata={
                        "source": str(file_path),
                        "filename": file_path.name,
                        "file_type": file_path.suffix,
                        "size": len(content)
                    },
                    source=str(file_path)
                )
                
                documents_to_ingest.append(doc)
                
            except Exception as e:
                console.print(f"[red]Error processing {file_path}: {e}[/red]")
        
        if not documents_to_ingest:
            console.print("[yellow]No documents could be processed.[/yellow]")
            return
        
        # Ingest documents
        console.print(f"[blue]Ingesting {len(documents_to_ingest)} documents...[/blue]")
        await agent.ingest_knowledge(documents_to_ingest)
        
        console.print(f"[green]Successfully ingested {len(documents_to_ingest)} documents![/green]")
        
    finally:
        await agent.shutdown()


@app.command()
def benchmark(
    config_path: Path = typer.Option(
        Path("config/example_agent.yaml"),
        "--config", "-c",
        help="Path to agent configuration file",
        exists=True,
        file_okay=True,
        dir_okay=False,
    ),
    benchmark_file: Optional[Path] = typer.Option(
        None,
        "--benchmark", "-b",
        help="Path to benchmark questions file (JSON/YAML)"
    ),
    output_file: Optional[Path] = typer.Option(
        None,
        "--output", "-o",
        help="Path to save benchmark results"
    ),
    iterations: int = typer.Option(
        1,
        "--iterations", "-i",
        help="Number of iterations to run each benchmark"
    ),
    debug: bool = typer.Option(
        False,
        "--debug", "-d",
        help="Enable debug logging"
    )
) -> None:
    """Run benchmarks against the DataQA agent."""
    
    setup_logging(debug=debug)
    
    try:
        # Load configuration
        console.print(f"[blue]Loading configuration from {config_path}[/blue]")
        config = load_agent_config(config_path)
        
        # Run benchmarks
        asyncio.run(_run_benchmarks(config, benchmark_file, output_file, iterations))
        
    except ConfigurationError as e:
        console.print(f"[red]Configuration Error: {e}[/red]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]Benchmark failed: {e}[/red]")
        if debug:
            console.print_exception()
        raise typer.Exit(1)


async def _run_benchmarks(config, benchmark_file: Optional[Path], output_file: Optional[Path], iterations: int) -> None:
    """Run the benchmark process."""
    
    # Create agent
    console.print("[blue]Initializing DataQA agent for benchmarking...[/blue]")
    agent = await create_agent_from_config(config)
    
    try:
        # Load benchmark questions
        if benchmark_file and benchmark_file.exists():
            import json
            import yaml
            
            with open(benchmark_file, 'r') as f:
                if benchmark_file.suffix.lower() == '.json':
                    benchmark_data = json.load(f)
                else:
                    benchmark_data = yaml.safe_load(f)
            
            questions = benchmark_data.get('questions', [])
        else:
            # Use default benchmark questions
            questions = [
                "What tables are available in the database?",
                "Show me the first 5 rows of data",
                "What are the column names and types?",
                "Generate a simple visualization of the data",
                "Calculate basic statistics for numeric columns"
            ]
        
        if not questions:
            console.print("[yellow]No benchmark questions found.[/yellow]")
            return
        
        console.print(f"[green]Running {len(questions)} benchmark questions with {iterations} iterations each[/green]")
        
        results = []
        
        for i, question in enumerate(questions, 1):
            console.print(f"[blue]Benchmark {i}/{len(questions)}: {question}[/blue]")
            
            question_results = []
            
            for iteration in range(iterations):
                console.print(f"[dim]  Iteration {iteration + 1}/{iterations}[/dim]")
                
                import time
                start_time = time.time()
                
                try:
                    response = await agent.query(question, f"benchmark_{i}_{iteration}")
                    end_time = time.time()
                    
                    result = {
                        "question": question,
                        "iteration": iteration + 1,
                        "success": True,
                        "response": response,
                        "execution_time": end_time - start_time,
                        "error": None
                    }
                    
                except Exception as e:
                    end_time = time.time()
                    result = {
                        "question": question,
                        "iteration": iteration + 1,
                        "success": False,
                        "response": None,
                        "execution_time": end_time - start_time,
                        "error": str(e)
                    }
                
                question_results.append(result)
            
            results.extend(question_results)
            
            # Show summary for this question
            successful = sum(1 for r in question_results if r["success"])
            avg_time = sum(r["execution_time"] for r in question_results) / len(question_results)
            
            console.print(f"[green]  Success rate: {successful}/{iterations} ({successful/iterations*100:.1f}%)[/green]")
            console.print(f"[green]  Average time: {avg_time:.2f}s[/green]")
            console.print()
        
        # Overall summary
        total_successful = sum(1 for r in results if r["success"])
        total_tests = len(results)
        overall_avg_time = sum(r["execution_time"] for r in results) / len(results)
        
        summary_table = Table(title="Benchmark Summary")
        summary_table.add_column("Metric", style="cyan")
        summary_table.add_column("Value", style="green")
        
        summary_table.add_row("Total Tests", str(total_tests))
        summary_table.add_row("Successful", str(total_successful))
        summary_table.add_row("Success Rate", f"{total_successful/total_tests*100:.1f}%")
        summary_table.add_row("Average Time", f"{overall_avg_time:.2f}s")
        
        console.print(summary_table)
        
        # Save results if output file specified
        if output_file:
            import json
            from datetime import datetime
            
            output_data = {
                "timestamp": datetime.now().isoformat(),
                "config": config.model_dump(mode='json'),
                "summary": {
                    "total_tests": total_tests,
                    "successful": total_successful,
                    "success_rate": total_successful/total_tests,
                    "average_time": overall_avg_time
                },
                "results": results
            }
            
            with open(output_file, 'w') as f:
                json.dump(output_data, f, indent=2)
            
            console.print(f"[green]Results saved to {output_file}[/green]")
    
    finally:
        await agent.shutdown()


@app.command()
def config(
    action: str = typer.Argument(
        ...,
        help="Action to perform: create, validate, show-env"
    ),
    config_path: Optional[Path] = typer.Option(
        None,
        "--config", "-c",
        help="Path to configuration file"
    ),
    output_path: Optional[Path] = typer.Option(
        None,
        "--output", "-o",
        help="Output path for created configuration"
    )
) -> None:
    """Manage DataQA agent configurations."""
    
    if action == "create":
        _create_config(output_path or Path("config/new_agent.yaml"))
    elif action == "validate":
        if not config_path:
            console.print("[red]Configuration path required for validation[/red]")
            raise typer.Exit(1)
        _validate_config(config_path)
    elif action == "show-env":
        _show_environment()
    else:
        console.print(f"[red]Unknown action: {action}[/red]")
        console.print("Available actions: create, validate, show-env")
        raise typer.Exit(1)


def _create_config(output_path: Path) -> None:
    """Create a new example configuration."""
    try:
        config = create_example_config(output_path)
        console.print(f"[green]Created example configuration at {output_path}[/green]")
        console.print(f"[blue]Agent name: {config.name}[/blue]")
        console.print("[yellow]Remember to set your API keys in environment variables![/yellow]")
    except Exception as e:
        console.print(f"[red]Failed to create configuration: {e}[/red]")
        raise typer.Exit(1)


def _validate_config(config_path: Path) -> None:
    """Validate a configuration file."""
    try:
        config = load_agent_config(config_path)
        console.print(f"[green]✓ Configuration is valid[/green]")
        console.print(f"[blue]Agent: {config.name}[/blue]")
        console.print(f"[blue]LLM: {config.llm.provider.value} ({config.llm.model})[/blue]")
        console.print(f"[blue]Knowledge: {config.knowledge.provider.value}[/blue]")
        console.print(f"[blue]Executor: {config.executor.provider.value}[/blue]")
    except ConfigurationError as e:
        console.print(f"[red]✗ Configuration is invalid: {e}[/red]")
        raise typer.Exit(1)


def _show_environment() -> None:
    """Show environment variable status."""
    env_status = validate_environment()
    
    env_table = Table(title="Environment Variables")
    env_table.add_column("Variable", style="cyan")
    env_table.add_column("Status", style="green")
    
    for var_name, is_set in env_status.items():
        status = "[green]✓ Set[/green]" if is_set else "[red]✗ Not Set[/red]"
        env_table.add_row(var_name, status)
    
    console.print(env_table)


@app.command()
def version() -> None:
    """Show DataQA version information."""
    try:
        import dataqa
        version = getattr(dataqa, '__version__', '0.1.0')
    except ImportError:
        version = '0.1.0'
    
    console.print(f"[bold green]DataQA version {version}[/bold green]")
    console.print("A composable data agent framework for natural language data interaction")


def main() -> None:
    """Main CLI entry point."""
    app()


if __name__ == "__main__":
    main()


================================================
FILE: dataqa/config/__init__.py
================================================
"""
DataQA configuration system.

This package provides Pydantic models and utilities for loading and validating
DataQA agent configurations from YAML files with environment variable support.
"""

from .loader import (
    ConfigurationError,
    create_example_config,
    get_config_from_env,
    load_agent_config,
    load_yaml_config,
    save_agent_config,
    substitute_env_vars,
    validate_environment,
)
from .models import (
    AgentConfig,
    ExecutorConfig,
    ExecutorProvider,
    KnowledgeConfig,
    KnowledgeProvider,
    LLMConfig,
    LLMProvider,
    WorkflowConfig,
)

__all__ = [
    # Models
    "AgentConfig",
    "LLMConfig",
    "KnowledgeConfig", 
    "ExecutorConfig",
    "WorkflowConfig",
    "LLMProvider",
    "KnowledgeProvider",
    "ExecutorProvider",
    # Loader functions
    "load_agent_config",
    "save_agent_config",
    "load_yaml_config",
    "create_example_config",
    "get_config_from_env",
    "substitute_env_vars",
    "validate_environment",
    "ConfigurationError",
]


================================================
FILE: dataqa/config/loader.py
================================================
"""
Configuration loading and management utilities.

This module provides functions to load and validate DataQA configurations
from YAML files with environment variable substitution support.
"""

import os
import re
from pathlib import Path
from typing import Any, Dict, Optional, Union

import yaml
from pydantic import ValidationError

from .models import AgentConfig


class ConfigurationError(Exception):
    """Raised when configuration loading or validation fails."""
    pass


def substitute_env_vars(data: Any) -> Any:
    """
    Recursively substitute environment variables in configuration data.
    
    Supports the format ${VAR_NAME} or ${VAR_NAME:default_value}.
    
    Args:
        data: Configuration data (dict, list, str, or other)
        
    Returns:
        Configuration data with environment variables substituted
    """
    if isinstance(data, dict):
        return {key: substitute_env_vars(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [substitute_env_vars(item) for item in data]
    elif isinstance(data, str):
        # Pattern to match ${VAR_NAME} or ${VAR_NAME:default}
        pattern = r'\$\{([^}:]+)(?::([^}]*))?\}'
        
        def replace_var(match):
            var_name = match.group(1)
            default_value = match.group(2) if match.group(2) is not None else ""
            return os.getenv(var_name, default_value)
        
        return re.sub(pattern, replace_var, data)
    else:
        return data


def load_yaml_config(config_path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load YAML configuration file with environment variable substitution.
    
    Args:
        config_path: Path to YAML configuration file
        
    Returns:
        Parsed configuration dictionary
        
    Raises:
        ConfigurationError: If file cannot be loaded or parsed
    """
    config_path = Path(config_path)
    
    if not config_path.exists():
        raise ConfigurationError(f"Configuration file not found: {config_path}")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            raw_config = yaml.safe_load(f)
    except yaml.YAMLError as e:
        raise ConfigurationError(f"Failed to parse YAML configuration: {e}")
    except Exception as e:
        raise ConfigurationError(f"Failed to read configuration file: {e}")
    
    if raw_config is None:
        raise ConfigurationError("Configuration file is empty")
    
    # Substitute environment variables
    config = substitute_env_vars(raw_config)
    
    return config


def load_agent_config(config_path: Union[str, Path]) -> AgentConfig:
    """
    Load and validate agent configuration from YAML file.
    
    Args:
        config_path: Path to YAML configuration file
        
    Returns:
        Validated AgentConfig instance
        
    Raises:
        ConfigurationError: If configuration is invalid
    """
    try:
        config_data = load_yaml_config(config_path)
        return AgentConfig(**config_data)
    except ValidationError as e:
        error_details = []
        for error in e.errors():
            loc = " -> ".join(str(x) for x in error['loc'])
            error_details.append(f"{loc}: {error['msg']}")
        
        raise ConfigurationError(
            f"Configuration validation failed:\n" + "\n".join(error_details)
        )
    except Exception as e:
        raise ConfigurationError(f"Failed to load agent configuration: {e}")


def save_agent_config(config: AgentConfig, config_path: Union[str, Path]) -> None:
    """
    Save agent configuration to YAML file.
    
    Args:
        config: AgentConfig instance to save
        config_path: Path where to save the configuration
        
    Raises:
        ConfigurationError: If configuration cannot be saved
    """
    config_path = Path(config_path)
    
    try:
        # Create parent directories if they don't exist
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert to dict and handle special types
        config_dict = config.model_dump(
            exclude_none=True,
            by_alias=True,
            mode='json'
        )
        
        # Convert Path objects to strings for YAML serialization
        def convert_paths(data):
            if isinstance(data, dict):
                return {key: convert_paths(value) for key, value in data.items()}
            elif isinstance(data, list):
                return [convert_paths(item) for item in data]
            elif hasattr(data, '__fspath__'):  # Path-like object
                return str(data)
            else:
                return data
        
        config_dict = convert_paths(config_dict)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(
                config_dict,
                f,
                default_flow_style=False,
                sort_keys=False,
                indent=2
            )
    except Exception as e:
        raise ConfigurationError(f"Failed to save configuration: {e}")


def create_example_config(config_path: Union[str, Path]) -> AgentConfig:
    """
    Create an example configuration file with default values.
    
    Args:
        config_path: Path where to save the example configuration
        
    Returns:
        Example AgentConfig instance
    """
    example_config = AgentConfig(
        name="example-agent",
        description="Example DataQA agent configuration",
        llm={
            "provider": "openai",
            "model": "gpt-4",
            "api_key": "${OPENAI_API_KEY}",
            "temperature": 0.1,
            "max_tokens": 2000
        },
        knowledge={
            "provider": "faiss",
            "embedding_model": "all-MiniLM-L6-v2",
            "chunk_size": 512,
            "top_k": 5
        },
        executor={
            "provider": "inmemory",
            "database_type": "duckdb",
            "max_execution_time": 30.0,
            "require_approval": True
        },
        workflow={
            "strategy": "react",
            "max_iterations": 10,
            "require_approval": True,
            "conversation_memory": True
        }
    )
    
    save_agent_config(example_config, config_path)
    return example_config


def validate_environment() -> Dict[str, bool]:
    """
    Validate that required environment variables are set.
    
    Returns:
        Dictionary mapping environment variable names to availability status
    """
    required_vars = {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY") is not None,
        "ANTHROPIC_API_KEY": os.getenv("ANTHROPIC_API_KEY") is not None,
    }
    
    optional_vars = {
        "DATAQA_LOG_LEVEL": os.getenv("DATAQA_LOG_LEVEL") is not None,
        "DATAQA_DATA_DIR": os.getenv("DATAQA_DATA_DIR") is not None,
        "DATAQA_CACHE_DIR": os.getenv("DATAQA_CACHE_DIR") is not None,
    }
    
    return {**required_vars, **optional_vars}


def get_config_from_env() -> Optional[AgentConfig]:
    """
    Create agent configuration from environment variables.
    
    This is useful for containerized deployments where configuration
    is provided through environment variables rather than files.
    
    Returns:
        AgentConfig instance if sufficient environment variables are set,
        None otherwise
    """
    # Check if we have minimum required configuration
    if not os.getenv("DATAQA_AGENT_NAME"):
        return None
    
    config_data = {
        "name": os.getenv("DATAQA_AGENT_NAME"),
        "description": os.getenv("DATAQA_AGENT_DESCRIPTION"),
    }
    
    # LLM configuration
    llm_config = {}
    if os.getenv("DATAQA_LLM_PROVIDER"):
        llm_config["provider"] = os.getenv("DATAQA_LLM_PROVIDER")
    if os.getenv("DATAQA_LLM_MODEL"):
        llm_config["model"] = os.getenv("DATAQA_LLM_MODEL")
    if os.getenv("DATAQA_LLM_API_KEY"):
        llm_config["api_key"] = os.getenv("DATAQA_LLM_API_KEY")
    if os.getenv("DATAQA_LLM_TEMPERATURE"):
        try:
            llm_config["temperature"] = float(os.getenv("DATAQA_LLM_TEMPERATURE"))
        except ValueError:
            return None
    
    if llm_config:
        config_data["llm"] = llm_config
    
    # Knowledge configuration
    knowledge_config = {}
    if os.getenv("DATAQA_KNOWLEDGE_PROVIDER"):
        knowledge_config["provider"] = os.getenv("DATAQA_KNOWLEDGE_PROVIDER")
    if os.getenv("DATAQA_KNOWLEDGE_INDEX_PATH"):
        knowledge_config["index_path"] = os.getenv("DATAQA_KNOWLEDGE_INDEX_PATH")
    
    if knowledge_config:
        config_data["knowledge"] = knowledge_config
    
    # Executor configuration
    executor_config = {}
    if os.getenv("DATAQA_EXECUTOR_PROVIDER"):
        executor_config["provider"] = os.getenv("DATAQA_EXECUTOR_PROVIDER")
    if os.getenv("DATAQA_DATABASE_URL"):
        executor_config["database_url"] = os.getenv("DATAQA_DATABASE_URL")
    
    if executor_config:
        config_data["executor"] = executor_config
    
    # Global settings
    if os.getenv("DATAQA_LOG_LEVEL"):
        config_data["log_level"] = os.getenv("DATAQA_LOG_LEVEL")
    if os.getenv("DATAQA_DATA_DIR"):
        config_data["data_dir"] = os.getenv("DATAQA_DATA_DIR")
    if os.getenv("DATAQA_CACHE_DIR"):
        config_data["cache_dir"] = os.getenv("DATAQA_CACHE_DIR")
    
    try:
        return AgentConfig(**config_data)
    except ValidationError:
        return None


================================================
FILE: dataqa/config/models.py
================================================
"""
Pydantic configuration models for DataQA agents.

This module defines the configuration schema for all DataQA components including
agents, LLMs, knowledge bases, and executors. All configurations support YAML
loading and environment variable substitution.
"""

import os
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from pydantic import BaseModel, Field, SecretStr, field_validator, model_validator


class LLMProvider(str, Enum):
    """Supported LLM providers."""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    LOCAL = "local"


class KnowledgeProvider(str, Enum):
    """Supported knowledge base providers."""
    FAISS = "faiss"
    OPENSEARCH = "opensearch"
    MEMORY = "memory"


class ExecutorProvider(str, Enum):
    """Supported executor providers."""
    INMEMORY = "inmemory"
    API = "api"
    DOCKER = "docker"


class LLMConfig(BaseModel):
    """Configuration for LLM providers."""
    
    provider: LLMProvider = Field(
        default=LLMProvider.OPENAI,
        description="LLM provider to use"
    )
    model: str = Field(
        default="gpt-4",
        description="Model name/identifier"
    )
    api_key: Optional[SecretStr] = Field(
        default=None,
        description="API key for the LLM provider"
    )
    api_base: Optional[str] = Field(
        default=None,
        description="Custom API base URL"
    )
    temperature: float = Field(
        default=0.1,
        ge=0.0,
        le=2.0,
        description="Temperature for response generation"
    )
    max_tokens: Optional[int] = Field(
        default=None,
        gt=0,
        description="Maximum tokens in response"
    )
    timeout: float = Field(
        default=30.0,
        gt=0,
        description="Request timeout in seconds"
    )
    max_retries: int = Field(
        default=3,
        ge=0,
        description="Maximum number of retries for failed requests"
    )
    extra_params: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional provider-specific parameters"
    )

    @field_validator('api_key', mode='before')
    @classmethod
    def resolve_api_key(cls, v: Any) -> Optional[SecretStr]:
        """Resolve API key from environment variables if needed."""
        if v is None:
            return None
        if isinstance(v, str) and v.startswith('${') and v.endswith('}'):
            env_var = v[2:-1]
            env_value = os.getenv(env_var)
            if env_value:
                return SecretStr(env_value)
            return None
        return SecretStr(str(v)) if v else None


class KnowledgeConfig(BaseModel):
    """Configuration for knowledge base providers."""
    
    provider: KnowledgeProvider = Field(
        default=KnowledgeProvider.FAISS,
        description="Knowledge base provider to use"
    )
    index_path: Optional[Path] = Field(
        default=None,
        description="Path to knowledge base index"
    )
    embedding_model: str = Field(
        default="all-MiniLM-L6-v2",
        description="Sentence transformer model for embeddings"
    )
    chunk_size: int = Field(
        default=512,
        gt=0,
        description="Text chunk size for document processing"
    )
    chunk_overlap: int = Field(
        default=50,
        ge=0,
        description="Overlap between text chunks"
    )
    top_k: int = Field(
        default=5,
        gt=0,
        description="Number of top results to retrieve"
    )
    similarity_threshold: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="Minimum similarity threshold for retrieval"
    )
    
    # OpenSearch specific settings
    opensearch_host: Optional[str] = Field(
        default=None,
        description="OpenSearch host URL"
    )
    opensearch_port: Optional[int] = Field(
        default=9200,
        gt=0,
        le=65535,
        description="OpenSearch port"
    )
    opensearch_username: Optional[str] = Field(
        default=None,
        description="OpenSearch username"
    )
    opensearch_password: Optional[SecretStr] = Field(
        default=None,
        description="OpenSearch password"
    )
    opensearch_index: str = Field(
        default="dataqa-knowledge",
        description="OpenSearch index name"
    )
    
    extra_params: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional provider-specific parameters"
    )

    @field_validator('opensearch_password', mode='before')
    @classmethod
    def resolve_opensearch_password(cls, v: Any) -> Optional[SecretStr]:
        """Resolve OpenSearch password from environment variables if needed."""
        if v is None:
            return None
        if isinstance(v, str) and v.startswith('${') and v.endswith('}'):
            env_var = v[2:-1]
            env_value = os.getenv(env_var)
            if env_value:
                return SecretStr(env_value)
            return None
        return SecretStr(str(v)) if v else None


class ExecutorConfig(BaseModel):
    """Configuration for code execution environments."""
    
    provider: ExecutorProvider = Field(
        default=ExecutorProvider.INMEMORY,
        description="Executor provider to use"
    )
    
    # Database connection settings
    database_url: Optional[SecretStr] = Field(
        default=None,
        description="Database connection URL"
    )
    database_type: str = Field(
        default="duckdb",
        description="Database type (duckdb, postgresql, mysql, etc.)"
    )
    
    # Execution limits
    max_execution_time: float = Field(
        default=30.0,
        gt=0,
        description="Maximum execution time in seconds"
    )
    max_memory_mb: int = Field(
        default=512,
        gt=0,
        description="Maximum memory usage in MB"
    )
    max_rows: int = Field(
        default=10000,
        gt=0,
        description="Maximum number of rows to return"
    )
    
    # Security settings
    allow_file_access: bool = Field(
        default=False,
        description="Allow file system access in code execution"
    )
    allowed_imports: List[str] = Field(
        default_factory=lambda: [
            "pandas", "numpy", "matplotlib", "seaborn", 
            "datetime", "math", "statistics"
        ],
        description="List of allowed Python imports"
    )
    blocked_functions: List[str] = Field(
        default_factory=lambda: [
            "exec", "eval", "open", "__import__", "compile"
        ],
        description="List of blocked Python functions"
    )
    
    # API executor settings
    api_url: Optional[str] = Field(
        default=None,
        description="API URL for remote execution"
    )
    api_key: Optional[SecretStr] = Field(
        default=None,
        description="API key for remote execution"
    )
    
    extra_params: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional provider-specific parameters"
    )

    @field_validator('database_url', 'api_key', mode='before')
    @classmethod
    def resolve_secrets(cls, v: Any) -> Optional[SecretStr]:
        """Resolve secrets from environment variables if needed."""
        if v is None:
            return None
        if isinstance(v, str) and v.startswith('${') and v.endswith('}'):
            env_var = v[2:-1]
            env_value = os.getenv(env_var)
            if env_value:
                return SecretStr(env_value)
            return None
        return SecretStr(str(v)) if v else None


class WorkflowConfig(BaseModel):
    """Configuration for agent workflow behavior."""
    
    strategy: str = Field(
        default="react",
        description="Agent reasoning strategy (react, workflow, plan_execute)"
    )
    max_iterations: int = Field(
        default=10,
        gt=0,
        description="Maximum number of workflow iterations"
    )
    require_approval: bool = Field(
        default=True,
        description="Require human approval for code execution"
    )
    auto_approve_safe: bool = Field(
        default=False,
        description="Auto-approve operations deemed safe"
    )
    conversation_memory: bool = Field(
        default=True,
        description="Enable conversation memory and context"
    )
    max_context_length: int = Field(
        default=4000,
        gt=0,
        description="Maximum context length for LLM prompts"
    )
    enable_visualization: bool = Field(
        default=True,
        description="Enable automatic visualization generation"
    )
    debug_mode: bool = Field(
        default=False,
        description="Enable debug logging and verbose output"
    )


class AgentConfig(BaseModel):
    """Main configuration for DataQA agents."""
    
    name: str = Field(
        description="Agent name/identifier"
    )
    description: Optional[str] = Field(
        default=None,
        description="Agent description"
    )
    version: str = Field(
        default="1.0.0",
        description="Agent configuration version"
    )
    
    # Component configurations
    llm: LLMConfig = Field(
        default_factory=LLMConfig,
        description="LLM configuration"
    )
    knowledge: KnowledgeConfig = Field(
        default_factory=KnowledgeConfig,
        description="Knowledge base configuration"
    )
    executor: ExecutorConfig = Field(
        default_factory=ExecutorConfig,
        description="Executor configuration"
    )
    workflow: WorkflowConfig = Field(
        default_factory=WorkflowConfig,
        description="Workflow configuration"
    )
    
    # Global settings
    log_level: str = Field(
        default="INFO",
        description="Logging level"
    )
    data_dir: Path = Field(
        default=Path("./data"),
        description="Data directory path"
    )
    cache_dir: Path = Field(
        default=Path("./cache"),
        description="Cache directory path"
    )
    
    extra_config: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional configuration parameters"
    )

    @field_validator('log_level')
    @classmethod
    def validate_log_level(cls, v: str) -> str:
        """Validate log level."""
        valid_levels = {"DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"}
        if v.upper() not in valid_levels:
            raise ValueError(f"Invalid log level: {v}. Must be one of {valid_levels}")
        return v.upper()

    @model_validator(mode='after')
    def validate_config_consistency(self) -> 'AgentConfig':
        """Validate configuration consistency across components."""
        # Ensure directories exist
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Validate knowledge base path
        if (self.knowledge.provider == KnowledgeProvider.FAISS and 
            self.knowledge.index_path is None):
            self.knowledge.index_path = self.data_dir / "knowledge" / "faiss_index"
        
        return self


================================================
FILE: dataqa/models/__init__.py
================================================
"""Shared data models for DataQA."""

from .message import Message
from .document import Document
from .execution import ExecutionResult

__all__ = ["Message", "Document", "ExecutionResult"]


================================================
FILE: dataqa/models/document.py
================================================
"""Document data model for knowledge base operations."""

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, ConfigDict, Field


class Document(BaseModel):
    """Represents a document in the knowledge base.
    
    This model is used for storing and retrieving contextual information
    that helps ground the agent's responses in relevant domain knowledge.
    """
    
    content: str = Field(
        description="The main text content of the document"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional metadata about the document (e.g., source, tags, date)"
    )
    embedding: Optional[List[float]] = Field(
        default=None,
        description="Vector embedding of the document content for similarity search"
    )
    source: str = Field(
        description="The source or origin of this document"
    )
    
    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )


================================================
FILE: dataqa/models/execution.py
================================================
"""Execution result data model for code execution operations."""

from typing import Any, Dict, Optional

from pydantic import BaseModel, ConfigDict, Field


class ExecutionResult(BaseModel):
    """Represents the result of executing generated code.
    
    This model captures both successful execution results and error information,
    providing a consistent interface for handling execution outcomes.
    """
    
    success: bool = Field(
        description="Whether the execution completed successfully"
    )
    data: Optional[Dict[str, Any]] = Field(
        default=None,
        description="The resulting data from execution (e.g., DataFrame as dict, plot data)"
    )
    error: Optional[str] = Field(
        default=None,
        description="Error message if execution failed"
    )
    execution_time: float = Field(
        description="Time taken to execute the code in seconds"
    )
    code_executed: str = Field(
        description="The actual code that was executed"
    )
    output_type: Optional[str] = Field(
        default=None,
        description="Type of output produced (e.g., 'dataframe', 'plot', 'scalar')"
    )
    
    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )


================================================
FILE: dataqa/models/message.py
================================================
"""Message data model for conversation handling."""

from datetime import datetime
from typing import Any, Dict, Literal, Optional

from pydantic import BaseModel, ConfigDict, Field


class Message(BaseModel):
    """Represents a message in a conversation between user and agent.
    
    This model is used to maintain conversation history and context
    throughout agent interactions.
    """
    
    role: Literal["user", "assistant", "system"] = Field(
        description="The role of the message sender"
    )
    content: str = Field(
        description="The actual message content"
    )
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="When the message was created"
    )
    metadata: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Additional metadata associated with the message"
    )
    
    model_config = ConfigDict(
        json_encoders={
            datetime: lambda v: v.isoformat()
        }
    )


================================================
FILE: dataqa/orchestration/__init__.py
================================================
"""
Advanced Multi-Agent Orchestration Framework

This package provides sophisticated multi-agent workflow coordination with:
- Hierarchical agent management
- Dynamic planning and replanning
- Domain knowledge integration
- Human-in-the-loop approval workflows
"""

from .agents import ManagerAgent, WorkerAgent, AgentHierarchy
from .models import (
    AgentCapability,
    MultiAgentWorkflow,
    ExecutionSession,
    ExecutionState,
    DomainContext,
)
from .planning import AdaptivePlanner, ReplanningEngine
from .domain import DomainKnowledgeManager, BusinessRulesEngine

__all__ = [
    "ManagerAgent",
    "WorkerAgent", 
    "AgentHierarchy",
    "AgentCapability",
    "MultiAgentWorkflow",
    "ExecutionSession",
    "ExecutionState",
    "DomainContext",
    "AdaptivePlanner",
    "ReplanningEngine",
    "DomainKnowledgeManager",
    "BusinessRulesEngine",
]


================================================
FILE: dataqa/orchestration/config.py
================================================
"""
Configuration schemas for multi-agent workflows and domain contexts.
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Union
from uuid import uuid4

from pydantic import BaseModel, Field, validator

from .models import (
    AgentCapability,
    AgentConfiguration,
    AgentRole,
    ApprovalRequirement,
    BusinessRule,
    CapabilityType,
    DomainContext,
    MonitoringConfig,
    Policy,
    RegulatoryRequirement,
    SchemaConstraint,
)


class WorkflowTrigger(str, Enum):
    """Types of workflow triggers."""
    MANUAL = "manual"
    SCHEDULED = "scheduled"
    EVENT_DRIVEN = "event_driven"
    API_REQUEST = "api_request"


class WorkflowStatus(str, Enum):
    """Workflow status values."""
    DRAFT = "draft"
    ACTIVE = "active"
    PAUSED = "paused"
    ARCHIVED = "archived"


class CapabilityConfigSchema(BaseModel):
    """Configuration schema for agent capabilities."""
    capability_type: CapabilityType
    name: str
    description: str
    version: str = "1.0.0"
    
    # Input/Output specifications
    input_schema: Optional[Dict[str, Any]] = None
    output_schema: Optional[Dict[str, Any]] = None
    
    # Resource requirements
    cpu_cores: Optional[int] = None
    memory_mb: Optional[int] = None
    gpu_required: bool = False
    network_access: bool = True
    storage_mb: Optional[int] = None
    execution_timeout_seconds: int = 300
    
    # Quality guarantees
    accuracy_threshold: Optional[float] = None
    response_time_ms: Optional[int] = None
    availability_percentage: Optional[float] = None
    error_rate_threshold: Optional[float] = None
    
    # Dependencies and metadata
    dependencies: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        use_enum_values = True


class AgentConfigSchema(BaseModel):
    """Configuration schema for individual agents."""
    name: str
    role: AgentRole
    agent_type: str = "worker"  # "manager", "worker", "specialist"
    specialization: Optional[str] = None
    
    # Capabilities
    capabilities: List[CapabilityConfigSchema] = Field(default_factory=list)
    
    # Operational settings
    max_concurrent_tasks: int = 1
    priority_level: int = 1
    enabled: bool = True
    
    # Hierarchy settings
    parent_agent: Optional[str] = None  # Name or ID of parent agent
    subordinates: List[str] = Field(default_factory=list)  # Names or IDs of subordinate agents
    
    # Configuration overrides
    config_overrides: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        use_enum_values = True
    
    @validator('agent_type')
    def validate_role_consistency(cls, v, values):
        """Ensure role is consistent with agent_type."""
        role = values.get('role')
        
        if v == 'manager' and role != AgentRole.MANAGER:
            raise ValueError("Manager agent_type must have MANAGER role")
        
        if v in ['worker', 'specialist'] and role == AgentRole.MANAGER:
            raise ValueError("Worker/specialist agent_type cannot have MANAGER role")
        
        return v


class BusinessRuleConfigSchema(BaseModel):
    """Configuration schema for business rules."""
    name: str
    description: str
    rule_type: str = "validation"  # "validation", "transformation", "routing", "approval"
    
    # Rule definition
    condition: str  # Rule condition expression (e.g., JSONPath, Python expression)
    action: str  # Action to take when rule applies
    
    # Rule properties
    priority: int = 1
    enabled: bool = True
    domain: Optional[str] = None
    
    # Error handling
    error_message: Optional[str] = None
    severity: str = "error"  # "error", "warning", "info"
    
    # Metadata
    tags: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class DomainConfigSchema(BaseModel):
    """Configuration schema for domain contexts."""
    domain_name: str
    description: str
    version: str = "1.0.0"
    
    # Business rules
    business_rules: List[BusinessRuleConfigSchema] = Field(default_factory=list)
    
    # Schema constraints
    schema_constraints: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Regulatory requirements
    regulatory_requirements: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Organizational policies
    organizational_policies: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Domain-specific settings
    default_data_sources: List[str] = Field(default_factory=list)
    required_approvals: List[str] = Field(default_factory=list)
    compliance_frameworks: List[str] = Field(default_factory=list)
    
    # Metadata
    tags: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ApprovalConfigSchema(BaseModel):
    """Configuration schema for approval requirements."""
    operation_type: str
    description: str
    
    # Risk assessment
    risk_level: str = "medium"  # "low", "medium", "high", "critical"
    risk_factors: List[str] = Field(default_factory=list)
    
    # Approval settings
    required_approvers: List[str] = Field(default_factory=list)  # Roles or specific users
    approval_threshold: int = 1  # Number of approvals required
    timeout_minutes: int = 60
    
    # Escalation
    escalation_policy: Optional[str] = None
    escalation_timeout_minutes: int = 120
    
    # Conditions
    conditions: List[str] = Field(default_factory=list)  # When this approval is required
    
    # Metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)


class MonitoringConfigSchema(BaseModel):
    """Configuration schema for monitoring and observability."""
    # Telemetry settings
    enable_telemetry: bool = True
    telemetry_endpoint: Optional[str] = None
    telemetry_interval_seconds: int = 30
    
    # Logging settings
    enable_structured_logging: bool = True
    log_level: str = "INFO"
    log_format: str = "json"
    
    # Metrics settings
    enable_performance_metrics: bool = True
    metrics_endpoint: Optional[str] = None
    custom_metrics: List[str] = Field(default_factory=list)
    
    # Health checks
    enable_health_checks: bool = True
    health_check_interval_seconds: int = 60
    health_check_timeout_seconds: int = 10
    
    # Alerting
    enable_alerting: bool = False
    alert_thresholds: Dict[str, float] = Field(default_factory=dict)
    alert_endpoints: List[str] = Field(default_factory=list)
    
    # Correlation and tracing
    correlation_id_header: str = "X-Correlation-ID"
    enable_distributed_tracing: bool = False
    tracing_endpoint: Optional[str] = None
    
    # Metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)


class WorkflowConfigSchema(BaseModel):
    """Configuration schema for complete multi-agent workflows."""
    # Basic information
    name: str
    description: str
    version: str = "1.0.0"
    
    # Workflow properties
    workflow_type: str = "data_analysis"  # "data_analysis", "report_generation", "compliance_check", etc.
    trigger: WorkflowTrigger = WorkflowTrigger.MANUAL
    status: WorkflowStatus = WorkflowStatus.DRAFT
    
    # Agent configuration
    agents: List[AgentConfigSchema] = Field(default_factory=list)
    
    # Domain and business context
    domain_context: Optional[DomainConfigSchema] = None
    
    # Approval and compliance
    approval_requirements: List[ApprovalConfigSchema] = Field(default_factory=list)
    
    # Monitoring and observability
    monitoring_config: MonitoringConfigSchema = Field(default_factory=MonitoringConfigSchema)
    
    # Execution settings
    max_execution_time_minutes: int = 60
    max_retries: int = 3
    enable_checkpointing: bool = True
    enable_rollback: bool = True
    
    # Resource limits
    max_concurrent_agents: int = 10
    resource_limits: Dict[str, Any] = Field(default_factory=dict)
    
    # Scheduling (for scheduled workflows)
    schedule_expression: Optional[str] = None  # Cron expression
    timezone: str = "UTC"
    
    # Event handling (for event-driven workflows)
    event_sources: List[str] = Field(default_factory=list)
    event_filters: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Metadata and tags
    tags: List[str] = Field(default_factory=list)
    owner: Optional[str] = None
    team: Optional[str] = None
    environment: str = "development"  # "development", "staging", "production"
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    # Timestamps
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    
    class Config:
        use_enum_values = True
    
    @validator('agents')
    def validate_agent_hierarchy(cls, v):
        """Validate agent hierarchy consistency."""
        agent_names = {agent.name for agent in v}
        
        for agent in v:
            # Check parent references
            if agent.parent_agent and agent.parent_agent not in agent_names:
                raise ValueError(f"Agent {agent.name} references non-existent parent {agent.parent_agent}")
            
            # Check subordinate references
            for subordinate in agent.subordinates:
                if subordinate not in agent_names:
                    raise ValueError(f"Agent {agent.name} references non-existent subordinate {subordinate}")
        
        return v
    
    @validator('schedule_expression')
    def validate_schedule_expression(cls, v, values):
        """Validate cron expression for scheduled workflows."""
        if v and values.get('trigger') != WorkflowTrigger.SCHEDULED:
            raise ValueError("Schedule expression only valid for scheduled workflows")
        
        if values.get('trigger') == WorkflowTrigger.SCHEDULED and not v:
            raise ValueError("Schedule expression required for scheduled workflows")
        
        # Basic cron validation (simplified)
        if v:
            parts = v.split()
            if len(parts) not in [5, 6]:  # Standard cron (5) or with seconds (6)
                raise ValueError("Invalid cron expression format")
        
        return v


class DeploymentConfigSchema(BaseModel):
    """Configuration schema for workflow deployment."""
    # Deployment target
    target_environment: str = "local"  # "local", "docker", "kubernetes", "cloud"
    
    # Resource allocation
    cpu_limit: Optional[str] = None  # e.g., "2", "500m"
    memory_limit: Optional[str] = None  # e.g., "1Gi", "512Mi"
    storage_limit: Optional[str] = None
    
    # Scaling settings
    min_replicas: int = 1
    max_replicas: int = 1
    auto_scaling_enabled: bool = False
    scaling_metrics: List[str] = Field(default_factory=list)
    
    # Network settings
    expose_ports: List[int] = Field(default_factory=list)
    ingress_enabled: bool = False
    ingress_host: Optional[str] = None
    
    # Security settings
    security_context: Dict[str, Any] = Field(default_factory=dict)
    secrets: List[str] = Field(default_factory=list)
    config_maps: List[str] = Field(default_factory=list)
    
    # Health and readiness
    health_check_path: str = "/health"
    readiness_check_path: str = "/ready"
    startup_timeout_seconds: int = 300
    
    # Metadata
    labels: Dict[str, str] = Field(default_factory=dict)
    annotations: Dict[str, str] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class MultiAgentWorkflowConfig(BaseModel):
    """Complete configuration for a multi-agent workflow system."""
    # Workflow definition
    workflow: WorkflowConfigSchema
    
    # Deployment configuration
    deployment: Optional[DeploymentConfigSchema] = None
    
    # Global settings
    global_settings: Dict[str, Any] = Field(default_factory=dict)
    
    # Configuration metadata
    config_version: str = "1.0"
    config_id: str = Field(default_factory=lambda: str(uuid4()))
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    
    def to_runtime_config(self) -> Dict[str, Any]:
        """Convert configuration to runtime format."""
        # This would convert the Pydantic models to the runtime models
        # used by the actual orchestration system
        return {
            "workflow_id": self.config_id,
            "workflow": self.workflow.dict(),
            "deployment": self.deployment.dict() if self.deployment else None,
            "global_settings": self.global_settings,
            "config_metadata": {
                "version": self.config_version,
                "created_at": self.created_at.isoformat(),
                "updated_at": self.updated_at.isoformat()
            }
        }


================================================
FILE: dataqa/orchestration/models.py
================================================
"""
Core Pydantic models for advanced multi-agent orchestration.
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Set, Union
from uuid import UUID, uuid4

from pydantic import BaseModel, Field, validator


class CapabilityType(str, Enum):
    """Types of agent capabilities."""
    DATA_RETRIEVAL = "data_retrieval"
    DATA_ANALYSIS = "data_analysis" 
    VISUALIZATION = "visualization"
    CODE_GENERATION = "code_generation"
    DOMAIN_EXPERTISE = "domain_expertise"
    COORDINATION = "coordination"
    APPROVAL = "approval"


class AgentRole(str, Enum):
    """Agent roles in the hierarchy."""
    MANAGER = "manager"
    WORKER = "worker"
    SPECIALIST = "specialist"


class ExecutionStatus(str, Enum):
    """Execution status values."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"
    CANCELLED = "cancelled"


class ResourceRequirements(BaseModel):
    """Resource requirements for agent capabilities."""
    cpu_cores: Optional[int] = None
    memory_mb: Optional[int] = None
    gpu_required: bool = False
    network_access: bool = True
    storage_mb: Optional[int] = None
    execution_timeout_seconds: int = 300


class QualityGuarantee(BaseModel):
    """Quality guarantees for agent capabilities."""
    accuracy_threshold: Optional[float] = None
    response_time_ms: Optional[int] = None
    availability_percentage: Optional[float] = None
    error_rate_threshold: Optional[float] = None


class InputRequirement(BaseModel):
    """Input requirements for agent capabilities."""
    name: str
    type: str
    required: bool = True
    description: Optional[str] = None
    validation_schema: Optional[Dict[str, Any]] = None


class OutputSpecification(BaseModel):
    """Output specifications for agent capabilities."""
    name: str
    type: str
    description: Optional[str] = None
    output_schema: Optional[Dict[str, Any]] = None


class AgentCapability(BaseModel):
    """Defines a specific capability that an agent can perform."""
    capability_id: str = Field(default_factory=lambda: str(uuid4()))
    capability_type: CapabilityType
    name: str
    description: str
    input_requirements: List[InputRequirement] = Field(default_factory=list)
    output_specifications: List[OutputSpecification] = Field(default_factory=list)
    resource_requirements: ResourceRequirements = Field(default_factory=ResourceRequirements)
    quality_guarantees: List[QualityGuarantee] = Field(default_factory=list)
    dependencies: List[str] = Field(default_factory=list)  # Other capability IDs
    version: str = "1.0.0"
    
    class Config:
        use_enum_values = True


class AgentConfiguration(BaseModel):
    """Configuration for an individual agent."""
    agent_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    role: AgentRole
    capabilities: List[AgentCapability] = Field(default_factory=list)
    specialization: Optional[str] = None
    max_concurrent_tasks: int = 1
    priority_level: int = 1  # 1 = highest, 10 = lowest
    enabled: bool = True
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        use_enum_values = True


class TaskAssignment(BaseModel):
    """Assignment of a task to a specific agent."""
    assignment_id: str = Field(default_factory=lambda: str(uuid4()))
    task_id: str
    agent_id: str
    assigned_at: datetime = Field(default_factory=datetime.utcnow)
    priority: int = 1
    deadline: Optional[datetime] = None
    context: Dict[str, Any] = Field(default_factory=dict)
    status: ExecutionStatus = ExecutionStatus.PENDING
    
    class Config:
        use_enum_values = True


class ExecutionStep(BaseModel):
    """Individual step in an execution plan."""
    step_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    agent_id: Optional[str] = None
    capability_required: Optional[CapabilityType] = None
    inputs: Dict[str, Any] = Field(default_factory=dict)
    outputs: Dict[str, Any] = Field(default_factory=dict)
    status: ExecutionStatus = ExecutionStatus.PENDING
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    max_retries: int = 3
    
    class Config:
        use_enum_values = True


class ExecutionMetrics(BaseModel):
    """Metrics collected during execution."""
    total_steps: int = 0
    completed_steps: int = 0
    failed_steps: int = 0
    total_execution_time_seconds: float = 0.0
    average_step_time_seconds: float = 0.0
    resource_utilization: Dict[str, float] = Field(default_factory=dict)
    quality_scores: Dict[str, float] = Field(default_factory=dict)


class ReplanningEvent(BaseModel):
    """Event that triggered replanning."""
    event_id: str = Field(default_factory=lambda: str(uuid4()))
    trigger_type: str
    trigger_description: str
    occurred_at: datetime = Field(default_factory=datetime.utcnow)
    context: Dict[str, Any] = Field(default_factory=dict)
    replanning_successful: bool = False
    new_plan_id: Optional[str] = None


class EscalationPoint(BaseModel):
    """Point where execution was escalated to human oversight."""
    escalation_id: str = Field(default_factory=lambda: str(uuid4()))
    reason: str
    escalated_at: datetime = Field(default_factory=datetime.utcnow)
    escalated_by: str  # agent_id
    context: Dict[str, Any] = Field(default_factory=dict)
    resolution: Optional[str] = None
    resolved_at: Optional[datetime] = None
    resolved_by: Optional[str] = None


class ExecutionState(BaseModel):
    """Current state of multi-agent execution."""
    session_id: str = Field(default_factory=lambda: str(uuid4()))
    current_plan_id: Optional[str] = None
    completed_steps: List[ExecutionStep] = Field(default_factory=list)
    intermediate_results: Dict[str, Any] = Field(default_factory=dict)
    execution_metrics: ExecutionMetrics = Field(default_factory=ExecutionMetrics)
    replanning_history: List[ReplanningEvent] = Field(default_factory=list)
    escalation_points: List[EscalationPoint] = Field(default_factory=list)
    status: ExecutionStatus = ExecutionStatus.PENDING
    started_at: Optional[datetime] = None
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    
    class Config:
        use_enum_values = True


class BusinessRule(BaseModel):
    """Individual business rule definition."""
    rule_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    rule_type: str
    condition: str  # Rule condition expression
    action: str  # Action to take when rule applies
    priority: int = 1
    enabled: bool = True
    domain: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)


class SchemaConstraint(BaseModel):
    """Schema constraint definition."""
    constraint_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    schema_path: str  # JSONPath or similar
    constraint_type: str  # "required", "type", "range", etc.
    constraint_value: Any
    error_message: str
    severity: str = "error"  # "error", "warning", "info"


class RegulatoryRequirement(BaseModel):
    """Regulatory requirement definition."""
    requirement_id: str = Field(default_factory=lambda: str(uuid4()))
    regulation_name: str
    requirement_text: str
    compliance_check: str  # How to verify compliance
    applicable_domains: List[str] = Field(default_factory=list)
    severity: str = "critical"  # "critical", "high", "medium", "low"
    metadata: Dict[str, Any] = Field(default_factory=dict)


class Policy(BaseModel):
    """Organizational policy definition."""
    policy_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    policy_text: str
    enforcement_level: str = "mandatory"  # "mandatory", "recommended", "optional"
    applicable_roles: List[str] = Field(default_factory=list)
    exceptions: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class DomainContext(BaseModel):
    """Context for domain-specific execution."""
    domain_name: str
    applicable_rules: List[BusinessRule] = Field(default_factory=list)
    schema_constraints: List[SchemaConstraint] = Field(default_factory=list)
    regulatory_requirements: List[RegulatoryRequirement] = Field(default_factory=list)
    organizational_policies: List[Policy] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ApprovalRequirement(BaseModel):
    """Requirement for human approval."""
    requirement_id: str = Field(default_factory=lambda: str(uuid4()))
    operation_type: str
    risk_level: str = "medium"  # "low", "medium", "high", "critical"
    required_approvers: List[str] = Field(default_factory=list)
    timeout_minutes: int = 60
    escalation_policy: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)


class MonitoringConfig(BaseModel):
    """Configuration for monitoring and observability."""
    enable_telemetry: bool = True
    enable_structured_logging: bool = True
    enable_performance_metrics: bool = True
    enable_health_checks: bool = True
    metrics_collection_interval_seconds: int = 30
    log_level: str = "INFO"
    correlation_id_header: str = "X-Correlation-ID"
    metadata: Dict[str, Any] = Field(default_factory=dict)


class MultiAgentWorkflow(BaseModel):
    """Complete multi-agent workflow definition."""
    workflow_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    version: str = "1.0.0"
    agents: List[AgentConfiguration] = Field(default_factory=list)
    domain_context: Optional[DomainContext] = None
    approval_requirements: List[ApprovalRequirement] = Field(default_factory=list)
    monitoring_config: MonitoringConfig = Field(default_factory=MonitoringConfig)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ExecutionSession(BaseModel):
    """Active execution session for a multi-agent workflow."""
    session_id: str = Field(default_factory=lambda: str(uuid4()))
    workflow_id: str
    execution_state: ExecutionState = Field(default_factory=ExecutionState)
    task_assignments: List[TaskAssignment] = Field(default_factory=list)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    @validator('execution_state', pre=True, always=True)
    def sync_session_id(cls, v, values):
        """Ensure execution state session_id matches session_id."""
        if isinstance(v, ExecutionState):
            v.session_id = values.get('session_id', v.session_id)
        return v


================================================
FILE: dataqa/orchestration/agents/__init__.py
================================================
"""
Agent management and hierarchy components for multi-agent orchestration.
"""

from .base import BaseAgent, Task, TaskResult, ExecutionContext, ProgressUpdate, AssistanceRequest
from .manager import ManagerAgent, Escalation, Resolution, DelegationStrategy, CoordinationProtocol
from .worker import WorkerAgent
from .hierarchy import AgentHierarchy

__all__ = [
    "BaseAgent",
    "ManagerAgent", 
    "WorkerAgent",
    "AgentHierarchy",
    "Task",
    "TaskResult",
    "ExecutionContext",
    "ProgressUpdate",
    "AssistanceRequest",
    "Escalation",
    "Resolution",
    "DelegationStrategy",
    "CoordinationProtocol",
]


================================================
FILE: dataqa/orchestration/agents/base.py
================================================
"""
Base agent class for multi-agent orchestration.
"""

from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
from uuid import uuid4

from pydantic import BaseModel, Field

from ..models import (
    AgentCapability,
    AgentConfiguration,
    AgentRole,
    ExecutionStatus,
    TaskAssignment,
)


class Task(BaseModel):
    """Represents a task to be executed by an agent."""
    task_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    inputs: Dict[str, Any] = Field(default_factory=dict)
    required_capabilities: List[str] = Field(default_factory=list)
    priority: int = 1
    deadline: Optional[datetime] = None
    context: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.utcnow)


class TaskResult(BaseModel):
    """Result of task execution."""
    task_id: str
    agent_id: str
    status: ExecutionStatus
    outputs: Dict[str, Any] = Field(default_factory=dict)
    execution_time_seconds: float = 0.0
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    completed_at: datetime = Field(default_factory=datetime.utcnow)
    
    class Config:
        use_enum_values = True


class ProgressUpdate(BaseModel):
    """Progress update from an agent."""
    agent_id: str
    task_id: str
    progress_percentage: float = 0.0
    status_message: str
    intermediate_results: Dict[str, Any] = Field(default_factory=dict)
    timestamp: datetime = Field(default_factory=datetime.utcnow)


class AssistanceRequest(BaseModel):
    """Request for assistance from another agent."""
    requesting_agent_id: str
    task_id: str
    assistance_type: str
    description: str
    required_capabilities: List[str] = Field(default_factory=list)
    context: Dict[str, Any] = Field(default_factory=dict)
    urgency: str = "normal"  # "low", "normal", "high", "critical"
    created_at: datetime = Field(default_factory=datetime.utcnow)


class ExecutionContext(BaseModel):
    """Context for task execution."""
    session_id: str
    workflow_id: str
    domain_context: Optional[Dict[str, Any]] = None
    available_resources: Dict[str, Any] = Field(default_factory=dict)
    constraints: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class BaseAgent(ABC):
    """
    Abstract base class for all agents in the multi-agent orchestration system.
    
    Provides common functionality for agent identification, capability management,
    and basic lifecycle operations.
    """
    
    def __init__(self, config: AgentConfiguration):
        """Initialize the agent with configuration."""
        self.config = config
        self.agent_id = config.agent_id
        self.name = config.name
        self.role = config.role
        self.capabilities = {cap.capability_id: cap for cap in config.capabilities}
        self.specialization = config.specialization
        self.max_concurrent_tasks = config.max_concurrent_tasks
        self.priority_level = config.priority_level
        self.enabled = config.enabled
        self.metadata = config.metadata.copy()
        
        # Runtime state
        self._active_tasks: Dict[str, Task] = {}
        self._task_history: List[TaskResult] = []
        self._status = ExecutionStatus.PENDING
        self._last_heartbeat = datetime.utcnow()
    
    @property
    def is_available(self) -> bool:
        """Check if agent is available for new tasks."""
        return (
            self.enabled and 
            self._status in [ExecutionStatus.PENDING, ExecutionStatus.RUNNING] and
            len(self._active_tasks) < self.max_concurrent_tasks
        )
    
    @property
    def active_task_count(self) -> int:
        """Get number of currently active tasks."""
        return len(self._active_tasks)
    
    @property
    def capability_types(self) -> Set[str]:
        """Get set of capability types this agent supports."""
        return {
            cap.capability_type if isinstance(cap.capability_type, str) else cap.capability_type.value
            for cap in self.capabilities.values()
        }
    
    def has_capability(self, capability_type: str) -> bool:
        """Check if agent has a specific capability type."""
        return capability_type in self.capability_types
    
    def get_capability(self, capability_id: str) -> Optional[AgentCapability]:
        """Get a specific capability by ID."""
        return self.capabilities.get(capability_id)
    
    def can_handle_task(self, task: Task) -> bool:
        """
        Check if this agent can handle the given task.
        
        Args:
            task: Task to evaluate
            
        Returns:
            True if agent can handle the task, False otherwise
        """
        if not self.is_available:
            return False
        
        # Check if agent has required capabilities
        for required_cap in task.required_capabilities:
            if not self.has_capability(required_cap):
                return False
        
        return True
    
    @abstractmethod
    async def execute_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """
        Execute a task with the given context.
        
        Args:
            task: Task to execute
            context: Execution context
            
        Returns:
            Task execution result
        """
        pass
    
    async def start_task(self, task: Task, context: ExecutionContext) -> None:
        """
        Start executing a task (non-blocking).
        
        Args:
            task: Task to start
            context: Execution context
        """
        if not self.can_handle_task(task):
            raise ValueError(f"Agent {self.agent_id} cannot handle task {task.task_id}")
        
        self._active_tasks[task.task_id] = task
        self._status = ExecutionStatus.RUNNING
    
    async def complete_task(self, task_id: str, result: TaskResult) -> None:
        """
        Mark a task as completed and update internal state.
        
        Args:
            task_id: ID of completed task
            result: Task execution result
        """
        if task_id in self._active_tasks:
            del self._active_tasks[task_id]
            self._task_history.append(result)
        
        if not self._active_tasks:
            self._status = ExecutionStatus.PENDING
    
    async def cancel_task(self, task_id: str, reason: str = "Cancelled") -> None:
        """
        Cancel an active task.
        
        Args:
            task_id: ID of task to cancel
            reason: Reason for cancellation
        """
        if task_id in self._active_tasks:
            task = self._active_tasks[task_id]
            result = TaskResult(
                task_id=task_id,
                agent_id=self.agent_id,
                status=ExecutionStatus.CANCELLED,
                error_message=reason
            )
            await self.complete_task(task_id, result)
    
    async def report_progress(self, progress: ProgressUpdate) -> None:
        """
        Report progress on a task.
        
        Args:
            progress: Progress update information
        """
        # Default implementation - can be overridden by subclasses
        self._last_heartbeat = datetime.utcnow()
    
    async def request_assistance(self, assistance_request: AssistanceRequest) -> None:
        """
        Request assistance from other agents.
        
        Args:
            assistance_request: Details of assistance needed
        """
        # Default implementation - can be overridden by subclasses
        pass
    
    def get_status_summary(self) -> Dict[str, Any]:
        """Get a summary of agent status."""
        return {
            "agent_id": self.agent_id,
            "name": self.name,
            "role": self.role.value if hasattr(self.role, 'value') else self.role,
            "status": self._status.value if hasattr(self._status, 'value') else self._status,
            "is_available": self.is_available,
            "active_tasks": len(self._active_tasks),
            "max_concurrent_tasks": self.max_concurrent_tasks,
            "capabilities": list(self.capability_types),
            "last_heartbeat": self._last_heartbeat.isoformat(),
            "task_history_count": len(self._task_history)
        }
    
    def __repr__(self) -> str:
        role_value = self.role.value if hasattr(self.role, 'value') else self.role
        return f"{self.__class__.__name__}(id={self.agent_id}, name={self.name}, role={role_value})"


================================================
FILE: dataqa/orchestration/agents/hierarchy.py
================================================
"""
Agent hierarchy management for multi-agent orchestration.
"""

from typing import Any, Dict, List, Optional, Set, Tuple
from uuid import uuid4

from pydantic import BaseModel, Field

from .base import BaseAgent
from .manager import ManagerAgent
from .worker import WorkerAgent
from ..models import AgentConfiguration, AgentRole


class HierarchyNode(BaseModel):
    """Node in the agent hierarchy."""
    node_id: str = Field(default_factory=lambda: str(uuid4()))
    agent_id: str
    parent_id: Optional[str] = None
    children_ids: List[str] = Field(default_factory=list)
    depth: int = 0
    metadata: Dict[str, Any] = Field(default_factory=dict)


class AgentHierarchy:
    """
    Manages hierarchical relationships between agents in a multi-agent system.
    
    Provides functionality for:
    - Building and validating agent hierarchies
    - Managing parent-child relationships
    - Capability-based routing and delegation
    - Hierarchy traversal and querying
    """
    
    def __init__(self):
        """Initialize empty agent hierarchy."""
        self.agents: Dict[str, BaseAgent] = {}
        self.hierarchy_nodes: Dict[str, HierarchyNode] = {}
        self.root_agents: Set[str] = set()
        self._hierarchy_valid = True
        self._validation_errors: List[str] = []
    
    def add_agent(self, agent: BaseAgent, parent_id: Optional[str] = None) -> None:
        """
        Add an agent to the hierarchy.
        
        Args:
            agent: Agent to add
            parent_id: ID of parent agent (None for root agents)
            
        Raises:
            ValueError: If agent already exists or parent doesn't exist
        """
        if agent.agent_id in self.agents:
            raise ValueError(f"Agent {agent.agent_id} already exists in hierarchy")
        
        if parent_id and parent_id not in self.agents:
            raise ValueError(f"Parent agent {parent_id} not found in hierarchy")
        
        # Add agent
        self.agents[agent.agent_id] = agent
        
        # Create hierarchy node
        depth = 0
        if parent_id:
            parent_node = self.hierarchy_nodes[parent_id]
            depth = parent_node.depth + 1
            parent_node.children_ids.append(agent.agent_id)
        else:
            self.root_agents.add(agent.agent_id)
        
        node = HierarchyNode(
            agent_id=agent.agent_id,
            parent_id=parent_id,
            depth=depth
        )
        self.hierarchy_nodes[agent.agent_id] = node
        
        # Set up agent relationships
        if parent_id and isinstance(agent, WorkerAgent):
            parent_agent = self.agents[parent_id]
            if isinstance(parent_agent, ManagerAgent):
                parent_agent.add_subordinate(agent)
        
        # Invalidate hierarchy validation
        self._hierarchy_valid = False
    
    def remove_agent(self, agent_id: str) -> None:
        """
        Remove an agent from the hierarchy.
        
        Args:
            agent_id: ID of agent to remove
            
        Raises:
            ValueError: If agent doesn't exist or has children
        """
        if agent_id not in self.agents:
            raise ValueError(f"Agent {agent_id} not found in hierarchy")
        
        node = self.hierarchy_nodes[agent_id]
        
        if node.children_ids:
            raise ValueError(f"Cannot remove agent {agent_id} with children. Remove children first.")
        
        # Remove from parent's children
        if node.parent_id:
            parent_node = self.hierarchy_nodes[node.parent_id]
            parent_node.children_ids.remove(agent_id)
            
            # Remove from manager's subordinates
            parent_agent = self.agents[node.parent_id]
            if isinstance(parent_agent, ManagerAgent):
                parent_agent.remove_subordinate(agent_id)
        else:
            self.root_agents.discard(agent_id)
        
        # Remove agent and node
        del self.agents[agent_id]
        del self.hierarchy_nodes[agent_id]
        
        # Invalidate hierarchy validation
        self._hierarchy_valid = False
    
    def get_agent(self, agent_id: str) -> Optional[BaseAgent]:
        """Get agent by ID."""
        return self.agents.get(agent_id)
    
    def get_parent(self, agent_id: str) -> Optional[BaseAgent]:
        """Get parent agent of the specified agent."""
        if agent_id not in self.hierarchy_nodes:
            return None
        
        parent_id = self.hierarchy_nodes[agent_id].parent_id
        return self.agents.get(parent_id) if parent_id else None
    
    def get_children(self, agent_id: str) -> List[BaseAgent]:
        """Get direct children of the specified agent."""
        if agent_id not in self.hierarchy_nodes:
            return []
        
        children_ids = self.hierarchy_nodes[agent_id].children_ids
        return [self.agents[child_id] for child_id in children_ids if child_id in self.agents]
    
    def get_descendants(self, agent_id: str) -> List[BaseAgent]:
        """Get all descendants (children, grandchildren, etc.) of the specified agent."""
        descendants = []
        children = self.get_children(agent_id)
        
        for child in children:
            descendants.append(child)
            descendants.extend(self.get_descendants(child.agent_id))
        
        return descendants
    
    def get_ancestors(self, agent_id: str) -> List[BaseAgent]:
        """Get all ancestors (parent, grandparent, etc.) of the specified agent."""
        ancestors = []
        current_id = agent_id
        
        while current_id in self.hierarchy_nodes:
            parent_id = self.hierarchy_nodes[current_id].parent_id
            if not parent_id:
                break
            
            parent_agent = self.agents.get(parent_id)
            if parent_agent:
                ancestors.append(parent_agent)
            
            current_id = parent_id
        
        return ancestors
    
    def get_siblings(self, agent_id: str) -> List[BaseAgent]:
        """Get sibling agents (same parent) of the specified agent."""
        if agent_id not in self.hierarchy_nodes:
            return []
        
        parent_id = self.hierarchy_nodes[agent_id].parent_id
        if not parent_id:
            # Root agent - siblings are other root agents
            return [self.agents[root_id] for root_id in self.root_agents if root_id != agent_id]
        
        # Get parent's children excluding this agent
        parent_children = self.get_children(parent_id)
        return [child for child in parent_children if child.agent_id != agent_id]
    
    def find_agents_by_capability(self, capability_type: str) -> List[BaseAgent]:
        """
        Find all agents that have a specific capability.
        
        Args:
            capability_type: Type of capability to search for
            
        Returns:
            List of agents with the specified capability
        """
        return [agent for agent in self.agents.values() if agent.has_capability(capability_type)]
    
    def find_available_agents_by_capability(self, capability_type: str) -> List[BaseAgent]:
        """
        Find available agents that have a specific capability.
        
        Args:
            capability_type: Type of capability to search for
            
        Returns:
            List of available agents with the specified capability
        """
        return [
            agent for agent in self.agents.values() 
            if agent.has_capability(capability_type) and agent.is_available
        ]
    
    def find_best_agent_for_task(self, task: 'Task', routing_strategy: str = "capability_based") -> Optional[BaseAgent]:
        """
        Find the best available agent for a specific task using advanced routing.
        
        Args:
            task: Task to find agent for
            routing_strategy: Strategy for agent selection ("capability_based", "load_balanced", "priority_based", "round_robin")
            
        Returns:
            Best available agent or None if no suitable agent found
        """
        from .base import Task  # Import here to avoid circular imports
        
        if not task.required_capabilities:
            return None
        
        # Find agents with all required capabilities
        capable_agents = []
        for agent in self.agents.values():
            if agent.is_available and all(agent.has_capability(cap) for cap in task.required_capabilities):
                capable_agents.append(agent)
        
        if not capable_agents:
            return None
        
        # Apply routing strategy
        if routing_strategy == "load_balanced":
            return min(capable_agents, key=lambda a: a.active_task_count)
        elif routing_strategy == "priority_based":
            return min(capable_agents, key=lambda a: a.priority_level)
        elif routing_strategy == "round_robin":
            # Simple round-robin based on agent ID hash
            return capable_agents[hash(task.task_id) % len(capable_agents)]
        else:  # capability_based (default)
            # Score agents based on multiple factors
            scored_agents = []
            for agent in capable_agents:
                score = self._calculate_agent_score(agent, task)
                scored_agents.append((agent, score))
            
            # Return agent with highest score
            scored_agents.sort(key=lambda x: x[1], reverse=True)
            return scored_agents[0][0]
    
    def _calculate_agent_score(self, agent: BaseAgent, task: 'Task') -> float:
        """
        Calculate a score for how well an agent matches a task.
        
        Args:
            agent: Agent to score
            task: Task to match against
            
        Returns:
            Score (higher is better)
        """
        score = 0.0
        
        # Base score for having required capabilities
        score += len(task.required_capabilities) * 10
        
        # Bonus for specialization match
        if agent.specialization and task.context.get("domain") == agent.specialization:
            score += 20
        
        # Penalty for high current load
        load_penalty = agent.active_task_count * 5
        score -= load_penalty
        
        # Bonus for higher priority agents
        priority_bonus = (11 - agent.priority_level) * 2  # Higher priority = lower number
        score += priority_bonus
        
        # Bonus for task priority match
        if hasattr(task, 'priority'):
            if task.priority <= 3 and agent.priority_level <= 3:  # High priority task + high priority agent
                score += 15
        
        # Penalty for agents with recent failures (if we track this)
        if hasattr(agent, '_task_history'):
            recent_failures = sum(1 for result in agent._task_history[-5:] if result.status.value == 'failed')
            score -= recent_failures * 3
        
        return score
    
    def discover_agents_by_pattern(self, pattern: Dict[str, Any]) -> List[BaseAgent]:
        """
        Discover agents matching a complex pattern.
        
        Args:
            pattern: Dictionary with search criteria
                - capabilities: List of required capabilities
                - role: Required agent role
                - specialization: Required specialization
                - available_only: Whether to include only available agents
                - max_load: Maximum current task load
                - min_priority: Minimum priority level (lower number = higher priority)
                
        Returns:
            List of matching agents
        """
        matching_agents = []
        
        for agent in self.agents.values():
            # Check availability if required
            if pattern.get('available_only', False) and not agent.is_available:
                continue
            
            # Check capabilities
            required_caps = pattern.get('capabilities', [])
            if required_caps and not all(agent.has_capability(cap) for cap in required_caps):
                continue
            
            # Check role
            required_role = pattern.get('role')
            if required_role and agent.role != required_role:
                continue
            
            # Check specialization
            required_spec = pattern.get('specialization')
            if required_spec and agent.specialization != required_spec:
                continue
            
            # Check maximum load
            max_load = pattern.get('max_load')
            if max_load is not None and agent.active_task_count > max_load:
                continue
            
            # Check minimum priority
            min_priority = pattern.get('min_priority')
            if min_priority is not None and agent.priority_level > min_priority:
                continue
            
            matching_agents.append(agent)
        
        return matching_agents
    
    def get_load_balanced_agent(self, capability_type: str) -> Optional[BaseAgent]:
        """
        Get the least loaded available agent with a specific capability.
        
        Args:
            capability_type: Required capability type
            
        Returns:
            Least loaded agent or None if no suitable agent found
        """
        available_agents = self.find_available_agents_by_capability(capability_type)
        
        if not available_agents:
            return None
        
        # Return agent with lowest current load
        return min(available_agents, key=lambda a: a.active_task_count)
    
    def get_capability_coverage_report(self) -> Dict[str, Any]:
        """
        Generate a report on capability coverage across the hierarchy.
        
        Returns:
            Dictionary with capability coverage information
        """
        capability_coverage = {}
        
        # Collect all capabilities
        all_capabilities = set()
        for agent in self.agents.values():
            all_capabilities.update(agent.capability_types)
        
        # Analyze coverage for each capability
        for capability in all_capabilities:
            agents_with_cap = self.find_agents_by_capability(capability)
            available_agents_with_cap = self.find_available_agents_by_capability(capability)
            
            capability_coverage[capability] = {
                "total_agents": len(agents_with_cap),
                "available_agents": len(available_agents_with_cap),
                "agent_ids": [a.agent_id for a in agents_with_cap],
                "available_agent_ids": [a.agent_id for a in available_agents_with_cap],
                "coverage_percentage": (len(available_agents_with_cap) / max(len(agents_with_cap), 1)) * 100
            }
        
        return {
            "capabilities": capability_coverage,
            "total_capabilities": len(all_capabilities),
            "total_agents": len(self.agents),
            "available_agents": len([a for a in self.agents.values() if a.is_available])
        }
    
    def find_agents_by_role(self, role: AgentRole) -> List[BaseAgent]:
        """
        Find all agents with a specific role.
        
        Args:
            role: Agent role to search for
            
        Returns:
            List of agents with the specified role
        """
        return [agent for agent in self.agents.values() if agent.role == role]
    
    def find_agents_by_specialization(self, specialization: str) -> List[BaseAgent]:
        """
        Find all agents with a specific specialization.
        
        Args:
            specialization: Specialization to search for
            
        Returns:
            List of agents with the specified specialization
        """
        return [
            agent for agent in self.agents.values() 
            if agent.specialization == specialization
        ]
    
    def get_hierarchy_depth(self) -> int:
        """Get the maximum depth of the hierarchy."""
        if not self.hierarchy_nodes:
            return 0
        return max(node.depth for node in self.hierarchy_nodes.values()) + 1
    
    def get_agents_at_depth(self, depth: int) -> List[BaseAgent]:
        """
        Get all agents at a specific depth in the hierarchy.
        
        Args:
            depth: Depth level (0 = root level)
            
        Returns:
            List of agents at the specified depth
        """
        agent_ids = [
            node.agent_id for node in self.hierarchy_nodes.values() 
            if node.depth == depth
        ]
        return [self.agents[agent_id] for agent_id in agent_ids]
    
    def validate_hierarchy(self) -> Tuple[bool, List[str]]:
        """
        Validate the hierarchy structure.
        
        Returns:
            Tuple of (is_valid, list_of_errors)
        """
        if self._hierarchy_valid:
            return True, []
        
        errors = []
        
        # Check for circular dependencies
        for agent_id in self.agents:
            if self._has_circular_dependency(agent_id):
                errors.append(f"Circular dependency detected involving agent {agent_id}")
        
        # Check for orphaned nodes
        for agent_id, node in self.hierarchy_nodes.items():
            if node.parent_id and node.parent_id not in self.agents:
                errors.append(f"Agent {agent_id} has non-existent parent {node.parent_id}")
        
        # Check role consistency
        for agent_id, agent in self.agents.items():
            children = self.get_children(agent_id)
            if children and agent.role != AgentRole.MANAGER:
                errors.append(f"Agent {agent_id} has children but is not a manager")
            
            if agent.role == AgentRole.MANAGER and not isinstance(agent, ManagerAgent):
                errors.append(f"Agent {agent_id} has manager role but is not a ManagerAgent instance")
            
            if agent.role == AgentRole.WORKER and not isinstance(agent, WorkerAgent):
                errors.append(f"Agent {agent_id} has worker role but is not a WorkerAgent instance")
        
        self._hierarchy_valid = len(errors) == 0
        self._validation_errors = errors
        
        return self._hierarchy_valid, errors
    
    def _has_circular_dependency(self, agent_id: str, visited: Optional[Set[str]] = None) -> bool:
        """Check if an agent has circular dependencies in its ancestry."""
        if visited is None:
            visited = set()
        
        if agent_id in visited:
            return True
        
        visited.add(agent_id)
        
        if agent_id not in self.hierarchy_nodes:
            return False
        
        parent_id = self.hierarchy_nodes[agent_id].parent_id
        if parent_id:
            return self._has_circular_dependency(parent_id, visited.copy())
        
        return False
    
    def get_hierarchy_summary(self) -> Dict[str, Any]:
        """Get a summary of the hierarchy structure."""
        is_valid, errors = self.validate_hierarchy()
        
        return {
            "total_agents": len(self.agents),
            "root_agents": len(self.root_agents),
            "max_depth": self.get_hierarchy_depth(),
            "manager_count": len(self.find_agents_by_role(AgentRole.MANAGER)),
            "worker_count": len(self.find_agents_by_role(AgentRole.WORKER)),
            "is_valid": is_valid,
            "validation_errors": errors,
            "agents_by_depth": {
                depth: len(self.get_agents_at_depth(depth))
                for depth in range(self.get_hierarchy_depth())
            }
        }
    
    def discover_agents_with_multiple_capabilities(self, capabilities: List[str], match_all: bool = True) -> List[BaseAgent]:
        """
        Discover agents that have multiple capabilities.
        
        Args:
            capabilities: List of capability types to search for
            match_all: If True, agent must have ALL capabilities. If False, agent must have ANY capability.
            
        Returns:
            List of agents matching the capability criteria
        """
        matching_agents = []
        
        for agent in self.agents.values():
            if match_all:
                # Agent must have all capabilities
                if all(agent.has_capability(cap) for cap in capabilities):
                    matching_agents.append(agent)
            else:
                # Agent must have at least one capability
                if any(agent.has_capability(cap) for cap in capabilities):
                    matching_agents.append(agent)
        
        return matching_agents
    
    def get_agent_routing_recommendations(self, task: 'Task') -> Dict[str, Any]:
        """
        Get routing recommendations for a task with detailed analysis.
        
        Args:
            task: Task to analyze
            
        Returns:
            Dictionary with routing recommendations and analysis
        """
        from .base import Task  # Import here to avoid circular imports
        
        recommendations = {
            "task_id": task.task_id,
            "required_capabilities": task.required_capabilities,
            "routing_options": [],
            "best_recommendation": None,
            "analysis": {}
        }
        
        # Find all capable agents
        capable_agents = []
        for agent in self.agents.values():
            if all(agent.has_capability(cap) for cap in task.required_capabilities):
                capable_agents.append(agent)
        
        if not capable_agents:
            recommendations["analysis"]["no_capable_agents"] = True
            return recommendations
        
        # Analyze each routing strategy
        strategies = ["capability_based", "load_balanced", "priority_based", "round_robin"]
        
        for strategy in strategies:
            best_agent = self.find_best_agent_for_task(task, strategy)
            if best_agent:
                option = {
                    "strategy": strategy,
                    "agent_id": best_agent.agent_id,
                    "agent_name": best_agent.name,
                    "current_load": best_agent.active_task_count,
                    "priority_level": best_agent.priority_level,
                    "specialization": best_agent.specialization,
                    "is_available": best_agent.is_available
                }
                
                if strategy == "capability_based":
                    option["score"] = self._calculate_agent_score(best_agent, task)
                
                recommendations["routing_options"].append(option)
        
        # Set best recommendation (capability_based by default)
        if recommendations["routing_options"]:
            recommendations["best_recommendation"] = recommendations["routing_options"][0]
        
        # Add analysis
        recommendations["analysis"] = {
            "total_capable_agents": len(capable_agents),
            "available_capable_agents": len([a for a in capable_agents if a.is_available]),
            "capability_coverage": {
                cap: len(self.find_agents_by_capability(cap))
                for cap in task.required_capabilities
            },
            "load_distribution": {
                "min_load": min(a.active_task_count for a in capable_agents),
                "max_load": max(a.active_task_count for a in capable_agents),
                "avg_load": sum(a.active_task_count for a in capable_agents) / len(capable_agents)
            }
        }
        
        return recommendations
    
    def get_agent_network_topology(self) -> Dict[str, Any]:
        """
        Get network topology information for the agent hierarchy.
        
        Returns:
            Dictionary with topology information
        """
        topology = {
            "nodes": [],
            "edges": [],
            "clusters": {},
            "metrics": {}
        }
        
        # Build nodes
        for agent_id, agent in self.agents.items():
            node = {
                "id": agent_id,
                "name": agent.name,
                "role": agent.role.value if hasattr(agent.role, 'value') else agent.role,
                "capabilities": list(agent.capability_types),
                "specialization": agent.specialization,
                "depth": self.hierarchy_nodes[agent_id].depth,
                "is_available": agent.is_available,
                "active_tasks": agent.active_task_count
            }
            topology["nodes"].append(node)
        
        # Build edges (parent-child relationships)
        for agent_id, node in self.hierarchy_nodes.items():
            if node.parent_id:
                edge = {
                    "source": node.parent_id,
                    "target": agent_id,
                    "type": "hierarchy"
                }
                topology["edges"].append(edge)
        
        # Build capability clusters
        capability_clusters = {}
        for agent in self.agents.values():
            for cap in agent.capability_types:
                if cap not in capability_clusters:
                    capability_clusters[cap] = []
                capability_clusters[cap].append(agent.agent_id)
        
        topology["clusters"]["capabilities"] = capability_clusters
        
        # Build specialization clusters
        specialization_clusters = {}
        for agent in self.agents.values():
            if agent.specialization:
                if agent.specialization not in specialization_clusters:
                    specialization_clusters[agent.specialization] = []
                specialization_clusters[agent.specialization].append(agent.agent_id)
        
        topology["clusters"]["specializations"] = specialization_clusters
        
        # Calculate metrics
        topology["metrics"] = {
            "total_nodes": len(self.agents),
            "total_edges": len(topology["edges"]),
            "max_depth": self.get_hierarchy_depth(),
            "branching_factor": len(topology["edges"]) / max(len(self.root_agents), 1),
            "capability_diversity": len(capability_clusters),
            "specialization_diversity": len(specialization_clusters)
        }
        
        return topology
    
    def find_optimal_delegation_path(self, task: 'Task', start_agent_id: str) -> Optional[List[str]]:
        """
        Find optimal delegation path from a starting agent to execute a task.
        
        Args:
            task: Task to execute
            start_agent_id: Starting agent ID
            
        Returns:
            List of agent IDs representing the delegation path, or None if no path found
        """
        from .base import Task  # Import here to avoid circular imports
        
        if start_agent_id not in self.agents:
            return None
        
        start_agent = self.agents[start_agent_id]
        
        # If start agent can handle the task directly
        if start_agent.can_handle_task(task):
            return [start_agent_id]
        
        # If start agent is a manager, try to delegate to subordinates
        if isinstance(start_agent, ManagerAgent):
            # Find capable subordinates
            capable_subordinates = []
            for subordinate in start_agent.subordinates:
                if subordinate.can_handle_task(task):
                    capable_subordinates.append(subordinate)
            
            if capable_subordinates:
                # Select best subordinate using scoring
                best_subordinate = max(
                    capable_subordinates,
                    key=lambda a: self._calculate_agent_score(a, task)
                )
                return [start_agent_id, best_subordinate.agent_id]
            
            # Try deeper delegation
            for subordinate in start_agent.subordinates:
                if isinstance(subordinate, ManagerAgent):
                    sub_path = self.find_optimal_delegation_path(task, subordinate.agent_id)
                    if sub_path:
                        return [start_agent_id] + sub_path
        
        # Try escalation to parent
        parent = self.get_parent(start_agent_id)
        if parent and isinstance(parent, ManagerAgent):
            parent_path = self.find_optimal_delegation_path(task, parent.agent_id)
            if parent_path and parent_path[0] != start_agent_id:  # Avoid cycles
                return parent_path
        
        return None
    
    def get_agent_collaboration_matrix(self) -> Dict[str, Dict[str, float]]:
        """
        Generate a collaboration matrix showing potential collaboration scores between agents.
        
        Returns:
            Matrix of collaboration scores between agents
        """
        matrix = {}
        
        for agent1_id, agent1 in self.agents.items():
            matrix[agent1_id] = {}
            
            for agent2_id, agent2 in self.agents.items():
                if agent1_id == agent2_id:
                    matrix[agent1_id][agent2_id] = 0.0
                    continue
                
                # Calculate collaboration score
                score = 0.0
                
                # Capability complementarity
                agent1_caps = agent1.capability_types
                agent2_caps = agent2.capability_types
                
                # Bonus for complementary capabilities
                unique_caps = len(agent1_caps.union(agent2_caps))
                common_caps = len(agent1_caps.intersection(agent2_caps))
                if unique_caps > 0:
                    score += (unique_caps - common_caps) / unique_caps * 50
                
                # Bonus for same specialization
                if agent1.specialization and agent1.specialization == agent2.specialization:
                    score += 20
                
                # Penalty for hierarchy distance
                if self._are_agents_related(agent1_id, agent2_id):
                    distance = self._calculate_hierarchy_distance(agent1_id, agent2_id)
                    score -= distance * 5
                else:
                    score -= 10  # Penalty for unrelated agents
                
                # Load balancing consideration
                avg_load = (agent1.active_task_count + agent2.active_task_count) / 2
                if avg_load < 2:  # Both agents are lightly loaded
                    score += 10
                
                matrix[agent1_id][agent2_id] = max(0.0, score)
        
        return matrix
    
    def _are_agents_related(self, agent1_id: str, agent2_id: str) -> bool:
        """Check if two agents are related in the hierarchy."""
        # Check if one is ancestor of the other
        ancestors1 = [a.agent_id for a in self.get_ancestors(agent1_id)]
        ancestors2 = [a.agent_id for a in self.get_ancestors(agent2_id)]
        
        return (agent1_id in ancestors2 or 
                agent2_id in ancestors1 or 
                len(set(ancestors1).intersection(set(ancestors2))) > 0)
    
    def _calculate_hierarchy_distance(self, agent1_id: str, agent2_id: str) -> int:
        """Calculate the hierarchy distance between two agents."""
        if agent1_id not in self.hierarchy_nodes or agent2_id not in self.hierarchy_nodes:
            return float('inf')
        
        depth1 = self.hierarchy_nodes[agent1_id].depth
        depth2 = self.hierarchy_nodes[agent2_id].depth
        
        # Simple distance calculation based on depth difference
        # In a real implementation, this would use graph algorithms
        return abs(depth1 - depth2) + 1
    
    def print_hierarchy(self, agent_id: Optional[str] = None, indent: int = 0) -> None:
        """
        Print a visual representation of the hierarchy.
        
        Args:
            agent_id: Starting agent ID (None for all root agents)
            indent: Current indentation level
        """
        if agent_id is None:
            # Print all root agents
            for root_id in self.root_agents:
                self.print_hierarchy(root_id, 0)
            return
        
        if agent_id not in self.agents:
            return
        
        agent = self.agents[agent_id]
        prefix = "  " * indent
        role_value = agent.role.value if hasattr(agent.role, 'value') else agent.role
        status = "✓" if agent.is_available else "✗"
        capabilities = ", ".join(list(agent.capability_types)[:3])  # Show first 3 capabilities
        if len(agent.capability_types) > 3:
            capabilities += "..."
        
        print(f"{prefix}{status} {agent.name} ({role_value}) [{capabilities}] - {agent.agent_id}")
        
        # Print children
        children = self.get_children(agent_id)
        for child in children:
            self.print_hierarchy(child.agent_id, indent + 1)


================================================
FILE: dataqa/orchestration/agents/manager.py
================================================
"""
Manager agent implementation for hierarchical multi-agent orchestration.
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional
from uuid import uuid4

from pydantic import BaseModel, Field

from .base import BaseAgent, Task, TaskResult, ExecutionContext, ProgressUpdate, AssistanceRequest
from .worker import WorkerAgent
from ..models import ExecutionStatus, TaskAssignment


class DelegationStrategy(str, Enum):
    """Strategies for task delegation."""
    ROUND_ROBIN = "round_robin"
    CAPABILITY_BASED = "capability_based"
    LOAD_BALANCED = "load_balanced"
    PRIORITY_BASED = "priority_based"


class CoordinationProtocol(str, Enum):
    """Protocols for coordinating subordinate agents."""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    PIPELINE = "pipeline"
    ADAPTIVE = "adaptive"


class Escalation(BaseModel):
    """Escalation from a subordinate agent."""
    escalation_id: str = Field(default_factory=lambda: str(uuid4()))
    from_agent_id: str
    task_id: str
    escalation_type: str
    reason: str
    context: Dict[str, Any] = Field(default_factory=dict)
    severity: str = "medium"  # "low", "medium", "high", "critical"
    created_at: datetime = Field(default_factory=datetime.utcnow)


class Resolution(BaseModel):
    """Resolution for an escalation."""
    escalation_id: str
    resolution_type: str
    action_taken: str
    new_assignment: Optional[TaskAssignment] = None
    additional_resources: Dict[str, Any] = Field(default_factory=dict)
    resolved_at: datetime = Field(default_factory=datetime.utcnow)


class ManagerAgent(BaseAgent):
    """
    Manager agent that coordinates and delegates tasks to subordinate worker agents.
    
    Responsibilities:
    - Task delegation based on agent capabilities and availability
    - Coordination of multi-agent workflows
    - Escalation handling and conflict resolution
    - Resource allocation and load balancing
    """
    
    def __init__(self, config, subordinates: Optional[List[WorkerAgent]] = None):
        """
        Initialize manager agent.
        
        Args:
            config: Agent configuration
            subordinates: List of subordinate worker agents
        """
        super().__init__(config)
        self.subordinates: List[WorkerAgent] = subordinates or []
        self.delegation_strategy = DelegationStrategy.CAPABILITY_BASED
        self.coordination_protocol = CoordinationProtocol.ADAPTIVE
        
        # Manager-specific state
        self._task_assignments: Dict[str, TaskAssignment] = {}
        self._escalations: Dict[str, Escalation] = {}
        self._subordinate_status: Dict[str, Dict[str, Any]] = {}
        self._delegation_history: List[Dict[str, Any]] = []
    
    def add_subordinate(self, worker: WorkerAgent) -> None:
        """Add a subordinate worker agent."""
        if worker not in self.subordinates:
            self.subordinates.append(worker)
            worker.set_manager(self)
            self._subordinate_status[worker.agent_id] = worker.get_status_summary()
    
    def remove_subordinate(self, worker_id: str) -> None:
        """Remove a subordinate worker agent."""
        self.subordinates = [w for w in self.subordinates if w.agent_id != worker_id]
        if worker_id in self._subordinate_status:
            del self._subordinate_status[worker_id]
    
    def get_available_subordinates(self) -> List[WorkerAgent]:
        """Get list of available subordinate agents."""
        return [worker for worker in self.subordinates if worker.is_available]
    
    def find_capable_subordinates(self, required_capabilities: List[str]) -> List[WorkerAgent]:
        """
        Find subordinates that have the required capabilities.
        
        Args:
            required_capabilities: List of required capability types
            
        Returns:
            List of capable subordinate agents
        """
        capable_agents = []
        for worker in self.subordinates:
            if worker.is_available and all(worker.has_capability(cap) for cap in required_capabilities):
                capable_agents.append(worker)
        return capable_agents
    
    async def delegate_task(self, task: Task) -> TaskAssignment:
        """
        Delegate a task to an appropriate subordinate agent.
        
        Args:
            task: Task to delegate
            
        Returns:
            Task assignment details
            
        Raises:
            ValueError: If no suitable subordinate is available
        """
        # Find capable subordinates
        capable_agents = self.find_capable_subordinates(task.required_capabilities)
        
        if not capable_agents:
            raise ValueError(f"No capable subordinates available for task {task.task_id}")
        
        # Select agent based on delegation strategy
        selected_agent = self._select_agent_for_delegation(capable_agents, task)
        
        # Create task assignment
        assignment = TaskAssignment(
            task_id=task.task_id,
            agent_id=selected_agent.agent_id,
            priority=task.priority,
            deadline=task.deadline,
            context=task.context
        )
        
        # Store assignment
        self._task_assignments[task.task_id] = assignment
        
        # Record delegation history
        self._delegation_history.append({
            "task_id": task.task_id,
            "assigned_to": selected_agent.agent_id,
            "strategy": self.delegation_strategy.value,
            "timestamp": datetime.utcnow().isoformat(),
            "available_agents": len(capable_agents)
        })
        
        return assignment
    
    def _select_agent_for_delegation(self, capable_agents: List[WorkerAgent], task: Task) -> WorkerAgent:
        """
        Select the best agent for task delegation based on strategy.
        
        Args:
            capable_agents: List of capable agents
            task: Task to assign
            
        Returns:
            Selected worker agent
        """
        if self.delegation_strategy == DelegationStrategy.ROUND_ROBIN:
            # Simple round-robin selection
            return capable_agents[len(self._delegation_history) % len(capable_agents)]
        
        elif self.delegation_strategy == DelegationStrategy.LOAD_BALANCED:
            # Select agent with lowest current load
            return min(capable_agents, key=lambda a: a.active_task_count)
        
        elif self.delegation_strategy == DelegationStrategy.PRIORITY_BASED:
            # Select agent with highest priority level
            return min(capable_agents, key=lambda a: a.priority_level)
        
        else:  # CAPABILITY_BASED (default)
            # Select agent with most specific capabilities for this task
            best_agent = capable_agents[0]
            best_score = 0
            
            for agent in capable_agents:
                # Score based on capability match and specialization
                score = 0
                for req_cap in task.required_capabilities:
                    if agent.has_capability(req_cap):
                        score += 1
                
                # Bonus for specialization match
                if agent.specialization and task.context.get("domain") == agent.specialization:
                    score += 2
                
                # Penalty for high load
                score -= agent.active_task_count * 0.5
                
                if score > best_score:
                    best_score = score
                    best_agent = agent
            
            return best_agent
    
    async def coordinate_execution(self, assignments: List[TaskAssignment]) -> List[TaskResult]:
        """
        Coordinate execution of multiple task assignments.
        
        Args:
            assignments: List of task assignments to coordinate
            
        Returns:
            List of task execution results
        """
        results = []
        
        if self.coordination_protocol == CoordinationProtocol.SEQUENTIAL:
            # Execute tasks sequentially
            for assignment in assignments:
                result = await self._execute_assignment(assignment)
                results.append(result)
        
        elif self.coordination_protocol == CoordinationProtocol.PARALLEL:
            # Execute tasks in parallel (simplified - would use asyncio.gather in real implementation)
            import asyncio
            tasks = [self._execute_assignment(assignment) for assignment in assignments]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Convert exceptions to failed TaskResults
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    results[i] = TaskResult(
                        task_id=assignments[i].task_id,
                        agent_id=assignments[i].agent_id,
                        status=ExecutionStatus.FAILED,
                        error_message=str(result)
                    )
        
        else:  # ADAPTIVE or PIPELINE
            # Adaptive coordination based on task dependencies and agent availability
            remaining_assignments = assignments.copy()
            
            while remaining_assignments:
                # Find assignments that can be executed now
                ready_assignments = [a for a in remaining_assignments if self._is_assignment_ready(a, results)]
                
                if not ready_assignments:
                    # No assignments ready - might indicate dependency cycle or resource constraints
                    break
                
                # Execute ready assignments in parallel if possible
                if len(ready_assignments) > 1:
                    import asyncio
                    batch_tasks = [self._execute_assignment(assignment) for assignment in ready_assignments]
                    batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                    
                    # Process batch results
                    for i, result in enumerate(batch_results):
                        if isinstance(result, Exception):
                            result = TaskResult(
                                task_id=ready_assignments[i].task_id,
                                agent_id=ready_assignments[i].agent_id,
                                status=ExecutionStatus.FAILED,
                                error_message=str(result)
                            )
                        results.append(result)
                        remaining_assignments.remove(ready_assignments[i])
                else:
                    # Execute single assignment
                    result = await self._execute_assignment(ready_assignments[0])
                    results.append(result)
                    remaining_assignments.remove(ready_assignments[0])
        
        return results
    
    async def _execute_assignment(self, assignment: TaskAssignment) -> TaskResult:
        """Execute a single task assignment."""
        # Find the assigned agent
        assigned_agent = next((a for a in self.subordinates if a.agent_id == assignment.agent_id), None)
        
        if not assigned_agent:
            return TaskResult(
                task_id=assignment.task_id,
                agent_id=assignment.agent_id,
                status=ExecutionStatus.FAILED,
                error_message=f"Assigned agent {assignment.agent_id} not found"
            )
        
        # Create task from assignment
        task = Task(
            task_id=assignment.task_id,
            name=f"Delegated task {assignment.task_id}",
            description="Task delegated by manager",
            priority=assignment.priority,
            deadline=assignment.deadline,
            context=assignment.context
        )
        
        # Create execution context
        context = ExecutionContext(
            session_id=assignment.context.get("session_id", "unknown"),
            workflow_id=assignment.context.get("workflow_id", "unknown"),
            domain_context=assignment.context.get("domain_context"),
            metadata=assignment.context
        )
        
        # Execute task through subordinate
        try:
            result = await assigned_agent.execute_task(task, context)
            assignment.status = result.status
            return result
        except Exception as e:
            assignment.status = ExecutionStatus.FAILED
            return TaskResult(
                task_id=assignment.task_id,
                agent_id=assignment.agent_id,
                status=ExecutionStatus.FAILED,
                error_message=str(e)
            )
    
    def _is_assignment_ready(self, assignment: TaskAssignment, completed_results: List[TaskResult]) -> bool:
        """Check if an assignment is ready to execute based on dependencies."""
        # Simplified dependency checking - would be more sophisticated in real implementation
        return True
    
    async def handle_escalation(self, escalation: Escalation) -> Resolution:
        """
        Handle escalation from a subordinate agent.
        
        Args:
            escalation: Escalation details
            
        Returns:
            Resolution for the escalation
        """
        self._escalations[escalation.escalation_id] = escalation
        
        # Determine resolution strategy based on escalation type and severity
        if escalation.escalation_type == "resource_shortage":
            return await self._handle_resource_escalation(escalation)
        elif escalation.escalation_type == "capability_gap":
            return await self._handle_capability_escalation(escalation)
        elif escalation.escalation_type == "quality_issue":
            return await self._handle_quality_escalation(escalation)
        else:
            return await self._handle_generic_escalation(escalation)
    
    async def _handle_resource_escalation(self, escalation: Escalation) -> Resolution:
        """Handle resource shortage escalation."""
        # Try to reallocate resources or reassign task
        return Resolution(
            escalation_id=escalation.escalation_id,
            resolution_type="resource_reallocation",
            action_taken="Attempted resource reallocation"
        )
    
    async def _handle_capability_escalation(self, escalation: Escalation) -> Resolution:
        """Handle capability gap escalation."""
        # Try to find alternative agent or request human intervention
        return Resolution(
            escalation_id=escalation.escalation_id,
            resolution_type="capability_substitution",
            action_taken="Searched for alternative capable agent"
        )
    
    async def _handle_quality_escalation(self, escalation: Escalation) -> Resolution:
        """Handle quality issue escalation."""
        # Review and potentially retry with different parameters
        return Resolution(
            escalation_id=escalation.escalation_id,
            resolution_type="quality_review",
            action_taken="Initiated quality review process"
        )
    
    async def _handle_generic_escalation(self, escalation: Escalation) -> Resolution:
        """Handle generic escalation."""
        return Resolution(
            escalation_id=escalation.escalation_id,
            resolution_type="manual_review",
            action_taken="Escalated to human oversight"
        )
    
    async def execute_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """
        Execute a task by delegating to subordinates and coordinating execution.
        
        Args:
            task: Task to execute
            context: Execution context
            
        Returns:
            Task execution result
        """
        try:
            # Start task execution
            await self.start_task(task, context)
            
            # Delegate task to subordinate
            assignment = await self.delegate_task(task)
            
            # Coordinate execution
            results = await self.coordinate_execution([assignment])
            
            # Aggregate results
            if results and results[0].status == ExecutionStatus.COMPLETED:
                result = TaskResult(
                    task_id=task.task_id,
                    agent_id=self.agent_id,
                    status=ExecutionStatus.COMPLETED,
                    outputs=results[0].outputs,
                    execution_time_seconds=results[0].execution_time_seconds,
                    metadata={"delegated_to": assignment.agent_id, "delegation_results": results}
                )
            else:
                result = TaskResult(
                    task_id=task.task_id,
                    agent_id=self.agent_id,
                    status=ExecutionStatus.FAILED,
                    error_message="Delegation failed or subordinate execution failed",
                    metadata={"delegation_results": results}
                )
            
            # Complete task
            await self.complete_task(task.task_id, result)
            return result
            
        except Exception as e:
            result = TaskResult(
                task_id=task.task_id,
                agent_id=self.agent_id,
                status=ExecutionStatus.FAILED,
                error_message=str(e)
            )
            await self.complete_task(task.task_id, result)
            return result
    
    async def receive_progress_update(self, progress: ProgressUpdate) -> None:
        """
        Receive progress update from a subordinate agent.
        
        Args:
            progress: Progress update from subordinate
        """
        # Update subordinate status
        if progress.agent_id in self._subordinate_status:
            self._subordinate_status[progress.agent_id].update({
                "last_progress": progress.progress_percentage,
                "last_status": progress.status_message,
                "last_update": progress.timestamp.isoformat()
            })
        
        # Update task assignment status if applicable
        if progress.task_id in self._task_assignments:
            assignment = self._task_assignments[progress.task_id]
            if progress.progress_percentage >= 100.0:
                assignment.status = ExecutionStatus.COMPLETED
            elif progress.progress_percentage > 0:
                assignment.status = ExecutionStatus.RUNNING
    
    def get_management_summary(self) -> Dict[str, Any]:
        """Get summary of management activities."""
        return {
            **self.get_status_summary(),
            "subordinate_count": len(self.subordinates),
            "available_subordinates": len(self.get_available_subordinates()),
            "active_assignments": len([a for a in self._task_assignments.values() if a.status == ExecutionStatus.RUNNING]),
            "total_assignments": len(self._task_assignments),
            "escalations_handled": len(self._escalations),
            "delegation_strategy": self.delegation_strategy.value if hasattr(self.delegation_strategy, 'value') else self.delegation_strategy,
            "coordination_protocol": self.coordination_protocol.value if hasattr(self.coordination_protocol, 'value') else self.coordination_protocol
        }


================================================
FILE: dataqa/orchestration/agents/worker.py
================================================
"""
Worker agent implementation for hierarchical multi-agent orchestration.
"""

from datetime import datetime
from typing import Any, Dict, List, Optional, TYPE_CHECKING
from uuid import uuid4

from pydantic import BaseModel, Field

from .base import BaseAgent, Task, TaskResult, ExecutionContext, ProgressUpdate, AssistanceRequest
from ..models import ExecutionStatus

if TYPE_CHECKING:
    from .manager import ManagerAgent


class WorkerAgent(BaseAgent):
    """
    Worker agent that executes specific tasks within its capabilities.
    
    Responsibilities:
    - Execute assigned tasks using available capabilities
    - Report progress to manager agents
    - Request assistance when encountering issues
    - Maintain execution history and performance metrics
    """
    
    def __init__(self, config):
        """
        Initialize worker agent.
        
        Args:
            config: Agent configuration
        """
        super().__init__(config)
        self.manager: Optional['ManagerAgent'] = None
        
        # Worker-specific state
        self._execution_history: List[Dict[str, Any]] = []
        self._performance_metrics: Dict[str, float] = {
            "average_execution_time": 0.0,
            "success_rate": 1.0,
            "tasks_completed": 0,
            "tasks_failed": 0
        }
        self._assistance_requests: List[AssistanceRequest] = []
    
    def set_manager(self, manager: 'ManagerAgent') -> None:
        """Set the manager agent for this worker."""
        self.manager = manager
    
    async def execute_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """
        Execute a task using the worker's capabilities.
        
        Args:
            task: Task to execute
            context: Execution context
            
        Returns:
            Task execution result
        """
        start_time = datetime.utcnow()
        
        try:
            # Start task execution
            await self.start_task(task, context)
            
            # Report initial progress
            await self.report_progress(ProgressUpdate(
                agent_id=self.agent_id,
                task_id=task.task_id,
                progress_percentage=0.0,
                status_message="Task execution started"
            ))
            
            # Execute task based on required capabilities
            result = await self._execute_task_by_capability(task, context)
            
            # Calculate execution time
            end_time = datetime.utcnow()
            execution_time = (end_time - start_time).total_seconds()
            result.execution_time_seconds = execution_time
            
            # Update performance metrics
            self._update_performance_metrics(result, execution_time)
            
            # Record execution history
            self._execution_history.append({
                "task_id": task.task_id,
                "task_name": task.name,
                "status": result.status if isinstance(result.status, str) else result.status.value,
                "execution_time": execution_time,
                "timestamp": end_time.isoformat(),
                "capabilities_used": task.required_capabilities
            })
            
            # Report final progress
            await self.report_progress(ProgressUpdate(
                agent_id=self.agent_id,
                task_id=task.task_id,
                progress_percentage=100.0,
                status_message=f"Task completed with status: {result.status if isinstance(result.status, str) else result.status.value}",
                intermediate_results=result.outputs
            ))
            
            # Complete task
            await self.complete_task(task.task_id, result)
            return result
            
        except Exception as e:
            # Handle execution error
            end_time = datetime.utcnow()
            execution_time = (end_time - start_time).total_seconds()
            
            result = TaskResult(
                task_id=task.task_id,
                agent_id=self.agent_id,
                status=ExecutionStatus.FAILED,
                error_message=str(e),
                execution_time_seconds=execution_time
            )
            
            # Update performance metrics
            self._update_performance_metrics(result, execution_time)
            
            # Record execution history
            self._execution_history.append({
                "task_id": task.task_id,
                "task_name": task.name,
                "status": "failed",
                "error": str(e),
                "execution_time": execution_time,
                "timestamp": end_time.isoformat(),
                "capabilities_used": task.required_capabilities
            })
            
            # Complete task with error
            await self.complete_task(task.task_id, result)
            return result
    
    async def _execute_task_by_capability(self, task: Task, context: ExecutionContext) -> TaskResult:
        """
        Execute task based on the primary required capability.
        
        Args:
            task: Task to execute
            context: Execution context
            
        Returns:
            Task execution result
        """
        # Determine primary capability needed
        primary_capability = task.required_capabilities[0] if task.required_capabilities else None
        
        if not primary_capability:
            raise ValueError(f"Task {task.task_id} has no required capabilities specified")
        
        if not self.has_capability(primary_capability):
            raise ValueError(f"Agent {self.agent_id} does not have required capability: {primary_capability}")
        
        # Execute based on capability type
        if primary_capability == "data_retrieval":
            return await self._execute_data_retrieval_task(task, context)
        elif primary_capability == "data_analysis":
            return await self._execute_data_analysis_task(task, context)
        elif primary_capability == "visualization":
            return await self._execute_visualization_task(task, context)
        elif primary_capability == "code_generation":
            return await self._execute_code_generation_task(task, context)
        elif primary_capability == "domain_expertise":
            return await self._execute_domain_expertise_task(task, context)
        else:
            return await self._execute_generic_task(task, context)
    
    async def _execute_data_retrieval_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """Execute a data retrieval task."""
        # Simulate data retrieval execution
        await self._simulate_work_progress(task.task_id, "Retrieving data")
        
        return TaskResult(
            task_id=task.task_id,
            agent_id=self.agent_id,
            status=ExecutionStatus.COMPLETED,
            outputs={
                "data_retrieved": True,
                "records_count": 1000,
                "data_source": task.inputs.get("source", "unknown"),
                "retrieval_method": "simulated"
            },
            metadata={"capability_used": "data_retrieval"}
        )
    
    async def _execute_data_analysis_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """Execute a data analysis task."""
        # Simulate data analysis execution
        await self._simulate_work_progress(task.task_id, "Analyzing data")
        
        return TaskResult(
            task_id=task.task_id,
            agent_id=self.agent_id,
            status=ExecutionStatus.COMPLETED,
            outputs={
                "analysis_completed": True,
                "insights_generated": 5,
                "analysis_type": task.inputs.get("analysis_type", "descriptive"),
                "confidence_score": 0.85
            },
            metadata={"capability_used": "data_analysis"}
        )
    
    async def _execute_visualization_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """Execute a visualization task."""
        # Simulate visualization creation
        await self._simulate_work_progress(task.task_id, "Creating visualization")
        
        return TaskResult(
            task_id=task.task_id,
            agent_id=self.agent_id,
            status=ExecutionStatus.COMPLETED,
            outputs={
                "visualization_created": True,
                "chart_type": task.inputs.get("chart_type", "bar"),
                "data_points": 50,
                "format": "png"
            },
            metadata={"capability_used": "visualization"}
        )
    
    async def _execute_code_generation_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """Execute a code generation task."""
        # Simulate code generation
        await self._simulate_work_progress(task.task_id, "Generating code")
        
        return TaskResult(
            task_id=task.task_id,
            agent_id=self.agent_id,
            status=ExecutionStatus.COMPLETED,
            outputs={
                "code_generated": True,
                "language": task.inputs.get("language", "python"),
                "lines_of_code": 150,
                "functions_created": 3
            },
            metadata={"capability_used": "code_generation"}
        )
    
    async def _execute_domain_expertise_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """Execute a domain expertise task."""
        # Simulate domain-specific analysis
        await self._simulate_work_progress(task.task_id, "Applying domain expertise")
        
        return TaskResult(
            task_id=task.task_id,
            agent_id=self.agent_id,
            status=ExecutionStatus.COMPLETED,
            outputs={
                "expertise_applied": True,
                "domain": self.specialization or "general",
                "recommendations": 3,
                "compliance_checked": True
            },
            metadata={"capability_used": "domain_expertise"}
        )
    
    async def _execute_generic_task(self, task: Task, context: ExecutionContext) -> TaskResult:
        """Execute a generic task."""
        # Simulate generic task execution
        await self._simulate_work_progress(task.task_id, "Executing task")
        
        return TaskResult(
            task_id=task.task_id,
            agent_id=self.agent_id,
            status=ExecutionStatus.COMPLETED,
            outputs={
                "task_completed": True,
                "processing_time": 2.5,
                "result_quality": "good"
            },
            metadata={"capability_used": "generic"}
        )
    
    async def _simulate_work_progress(self, task_id: str, work_description: str) -> None:
        """Simulate work progress with periodic updates."""
        # Simulate progress reporting
        progress_steps = [25.0, 50.0, 75.0]
        
        for progress in progress_steps:
            await self.report_progress(ProgressUpdate(
                agent_id=self.agent_id,
                task_id=task_id,
                progress_percentage=progress,
                status_message=f"{work_description} - {progress}% complete"
            ))
    
    def _update_performance_metrics(self, result: TaskResult, execution_time: float) -> None:
        """Update performance metrics based on task result."""
        # Update task counts
        if result.status == ExecutionStatus.COMPLETED:
            self._performance_metrics["tasks_completed"] += 1
        else:
            self._performance_metrics["tasks_failed"] += 1
        
        # Update success rate
        total_tasks = self._performance_metrics["tasks_completed"] + self._performance_metrics["tasks_failed"]
        self._performance_metrics["success_rate"] = self._performance_metrics["tasks_completed"] / total_tasks
        
        # Update average execution time
        current_avg = self._performance_metrics["average_execution_time"]
        self._performance_metrics["average_execution_time"] = (
            (current_avg * (total_tasks - 1) + execution_time) / total_tasks
        )
    
    async def report_progress(self, progress: ProgressUpdate) -> None:
        """
        Report progress to manager agent.
        
        Args:
            progress: Progress update information
        """
        # Update last heartbeat
        await super().report_progress(progress)
        
        # If we have a manager, report progress to them
        if self.manager:
            await self.manager.receive_progress_update(progress)
    
    async def request_assistance(self, assistance_request: AssistanceRequest) -> None:
        """
        Request assistance from manager or peer agents.
        
        Args:
            assistance_request: Details of assistance needed
        """
        self._assistance_requests.append(assistance_request)
        
        # If we have a manager, escalate the assistance request
        if self.manager:
            # In a real implementation, this would create an escalation to the manager
            pass
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get summary of worker performance."""
        return {
            **self.get_status_summary(),
            "performance_metrics": self._performance_metrics.copy(),
            "execution_history_count": len(self._execution_history),
            "assistance_requests_count": len(self._assistance_requests),
            "has_manager": self.manager is not None,
            "manager_id": self.manager.agent_id if self.manager else None
        }


================================================
FILE: dataqa/orchestration/approval/__init__.py
================================================
"""
Human-in-the-loop approval system for multi-agent orchestration.

This module provides components for managing human approval workflows,
including approval requests, timeout handling, and feedback integration.
"""

from .models import (
    ApprovalRequest,
    ApprovalResponse,
    ApprovalStatus,
    FeedbackType,
    HumanFeedback,
    OperationType,
    RiskAssessment,
    RiskLevel,
    TimeoutPolicy,
)
from .workflow import ApprovalWorkflow
from .manager import HumanInteractionManager

__all__ = [
    "ApprovalRequest",
    "ApprovalResponse", 
    "ApprovalStatus",
    "ApprovalWorkflow",
    "FeedbackType",
    "HumanFeedback",
    "HumanInteractionManager",
    "OperationType",
    "RiskAssessment",
    "RiskLevel",
    "TimeoutPolicy",
]


================================================
FILE: dataqa/orchestration/approval/manager.py
================================================
"""
HumanInteractionManager for managing approval queues and human feedback integration.
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Any, Callable, Dict, List, Optional, Set

from ...exceptions import DataQAError
from .models import (
    ApprovalPolicy,
    ApprovalQueue,
    ApprovalRequest,
    ApprovalResponse,
    ApprovalStatus,
    EscalationRule,
    FeedbackType,
    HumanFeedback,
    TimeoutEvent,
    TimeoutResolution,
)
from .workflow import ApprovalWorkflow

logger = logging.getLogger(__name__)


class HumanInteractionError(DataQAError):
    """Errors related to human interaction management."""
    pass


class FeedbackIntegrator:
    """Integrates human feedback for continuous learning."""
    
    def __init__(self):
        self.feedback_history: List[HumanFeedback] = []
        self.learning_patterns: Dict[str, Any] = {}
    
    async def process_feedback(self, feedback: HumanFeedback) -> None:
        """Process and learn from human feedback."""
        try:
            self.feedback_history.append(feedback)
            
            # Extract learning patterns
            await self._extract_learning_patterns(feedback)
            
            # Update risk assessment models (simplified)
            await self._update_risk_models(feedback)
            
            logger.info(f"Processed feedback {feedback.feedback_id} of type {feedback.feedback_type}")
            
        except Exception as e:
            logger.error(f"Error processing feedback: {e}")
            raise HumanInteractionError(f"Failed to process feedback: {e}")
    
    async def get_learning_insights(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Get learning insights for similar contexts."""
        insights = {
            "similar_cases": [],
            "common_patterns": [],
            "risk_adjustments": {},
            "approval_likelihood": 0.5,
        }
        
        try:
            # Find similar feedback cases
            similar_feedback = [
                fb for fb in self.feedback_history
                if any(tag in context.get("tags", []) for tag in fb.context_tags)
            ]
            
            if similar_feedback:
                # Calculate approval likelihood
                approvals = sum(1 for fb in similar_feedback if fb.feedback_type == FeedbackType.APPROVAL)
                insights["approval_likelihood"] = approvals / len(similar_feedback)
                
                # Extract common patterns
                all_tags = []
                for fb in similar_feedback:
                    all_tags.extend(fb.context_tags)
                
                tag_counts = {}
                for tag in all_tags:
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1
                
                insights["common_patterns"] = [
                    tag for tag, count in tag_counts.items()
                    if count >= len(similar_feedback) * 0.5  # Appears in 50%+ of cases
                ]
            
            return insights
            
        except Exception as e:
            logger.error(f"Error getting learning insights: {e}")
            return insights
    
    async def _extract_learning_patterns(self, feedback: HumanFeedback) -> None:
        """Extract learning patterns from feedback."""
        # Update pattern tracking
        for tag in feedback.context_tags:
            if tag not in self.learning_patterns:
                self.learning_patterns[tag] = {
                    "approval_count": 0,
                    "rejection_count": 0,
                    "modification_count": 0,
                    "total_count": 0,
                }
            
            pattern = self.learning_patterns[tag]
            pattern["total_count"] += 1
            
            if feedback.feedback_type == FeedbackType.APPROVAL:
                pattern["approval_count"] += 1
            elif feedback.feedback_type == FeedbackType.REJECTION:
                pattern["rejection_count"] += 1
            elif feedback.feedback_type == FeedbackType.MODIFICATION:
                pattern["modification_count"] += 1
    
    async def _update_risk_models(self, feedback: HumanFeedback) -> None:
        """Update risk assessment models based on feedback."""
        # This would integrate with ML models in a real implementation
        # For now, we just log the learning opportunity
        logger.info(f"Learning opportunity: {feedback.feedback_type} for tags {feedback.context_tags}")


class HumanInteractionManager:
    """
    Manages human approval queues, timeout handling, and feedback integration.
    
    This class handles:
    - Managing approval request queues
    - Routing requests to appropriate approvers
    - Handling timeouts and escalations
    - Integrating human feedback for learning
    """
    
    def __init__(
        self,
        approval_policies: Optional[List[ApprovalPolicy]] = None,
        escalation_rules: Optional[List[EscalationRule]] = None,
        notification_callback: Optional[Callable] = None,
    ):
        """
        Initialize the human interaction manager.
        
        Args:
            approval_policies: List of approval policies
            escalation_rules: List of escalation rules
            notification_callback: Callback function for sending notifications
        """
        self.approval_workflow = ApprovalWorkflow(
            policies=approval_policies or [],
            escalation_rules=escalation_rules or [],
        )
        self.feedback_integrator = FeedbackIntegrator()
        self.notification_callback = notification_callback
        
        # Approval queues
        self.approval_queues: Dict[str, ApprovalQueue] = {}
        self.active_requests: Dict[str, ApprovalRequest] = {}
        self.processed_requests: Dict[str, ApprovalResponse] = {}
        
        # Metrics
        self.metrics = {
            "total_requests": 0,
            "approved_requests": 0,
            "rejected_requests": 0,
            "timed_out_requests": 0,
            "average_response_time_minutes": 0.0,
        }
        
        logger.info("HumanInteractionManager initialized")
    
    async def request_approval(
        self,
        approval_request: ApprovalRequest,
        queue_name: Optional[str] = None,
    ) -> str:
        """
        Submit an approval request for human review.
        
        Args:
            approval_request: The approval request to submit
            queue_name: Optional specific queue to route to
            
        Returns:
            Request ID for tracking
        """
        try:
            # Store the request
            self.active_requests[approval_request.request_id] = approval_request
            self.metrics["total_requests"] += 1
            
            # Also store in approval workflow for processing
            self.approval_workflow._pending_requests[approval_request.request_id] = approval_request
            
            # Route to appropriate queue
            target_queue = await self._route_to_queue(approval_request, queue_name)
            
            # Add to queue
            if target_queue:
                target_queue.pending_requests.append(approval_request.request_id)
                target_queue.updated_at = datetime.utcnow()
            
            # Send notifications
            if self.notification_callback:
                await self._send_notifications(approval_request, target_queue)
            
            # Get learning insights
            context = {
                "operation_type": approval_request.operation_type,
                "risk_level": approval_request.risk_assessment.risk_level,
                "tags": approval_request.context_explanation.split(),
            }
            insights = await self.feedback_integrator.get_learning_insights(context)
            
            logger.info(
                f"Approval request {approval_request.request_id} submitted to queue "
                f"{target_queue.name if target_queue else 'default'} "
                f"(predicted approval likelihood: {insights['approval_likelihood']:.2f})"
            )
            
            return approval_request.request_id
            
        except Exception as e:
            logger.error(f"Error requesting approval: {e}")
            raise HumanInteractionError(f"Failed to request approval: {e}")
    
    async def process_approval_response(
        self,
        request_id: str,
        response: ApprovalResponse,
    ) -> None:
        """
        Process a response to an approval request.
        
        Args:
            request_id: ID of the approval request
            response: Response from the approver
        """
        try:
            if request_id not in self.active_requests:
                raise HumanInteractionError(f"Active approval request {request_id} not found")
            
            request = self.active_requests[request_id]
            
            # Update metrics
            if response.status == ApprovalStatus.APPROVED:
                self.metrics["approved_requests"] += 1
            elif response.status == ApprovalStatus.REJECTED:
                self.metrics["rejected_requests"] += 1
            elif response.status == ApprovalStatus.TIMEOUT:
                self.metrics["timed_out_requests"] += 1
            
            # Calculate response time
            response_time = (response.responded_at - request.requested_at).total_seconds() / 60
            self._update_average_response_time(response_time)
            
            # Remove from active requests and queues
            del self.active_requests[request_id]
            await self._remove_from_queues(request_id)
            
            # Store processed response
            self.processed_requests[request_id] = response
            
            # Process with approval workflow
            await self.approval_workflow.process_response(request_id, response)
            
            # Generate learning feedback
            await self._generate_learning_feedback(request, response)
            
            logger.info(
                f"Processed approval response for {request_id}: {response.status} "
                f"(response time: {response_time:.1f} minutes)"
            )
            
        except Exception as e:
            logger.error(f"Error processing approval response: {e}")
            raise HumanInteractionError(f"Failed to process approval response: {e}")
    
    async def handle_timeout(
        self,
        request_id: str,
        timeout_event: TimeoutEvent,
    ) -> TimeoutResolution:
        """
        Handle timeout for an approval request.
        
        Args:
            request_id: ID of the timed-out request
            timeout_event: Details of the timeout event
            
        Returns:
            TimeoutResolution describing how the timeout was handled
        """
        try:
            if request_id not in self.active_requests:
                raise HumanInteractionError(f"Active approval request {request_id} not found")
            
            request = self.active_requests[request_id]
            
            # Determine resolution type
            resolution_type = "timeout"
            resolution_description = f"Request timed out after {timeout_event.timeout_duration_minutes} minutes"
            
            if timeout_event.escalation_triggered:
                resolution_type = "escalated"
                resolution_description += " and was escalated"
                
                # Handle escalation
                await self._handle_escalation(request, timeout_event)
            
            elif timeout_event.auto_rejected:
                resolution_type = "auto_rejected"
                resolution_description += " and was automatically rejected"
                
                # Create auto-rejection response
                auto_response = ApprovalResponse(
                    request_id=request_id,
                    status=ApprovalStatus.TIMEOUT,
                    comments="Automatically rejected due to timeout",
                )
                await self.process_approval_response(request_id, auto_response)
            
            elif timeout_event.fallback_action_taken:
                resolution_type = "fallback"
                resolution_description += f" and fallback action '{timeout_event.fallback_action_taken}' was taken"
            
            # Create resolution
            resolution = TimeoutResolution(
                timeout_event_id=timeout_event.event_id,
                resolution_type=resolution_type,
                resolution_description=resolution_description,
            )
            
            logger.warning(f"Handled timeout for request {request_id}: {resolution_type}")
            
            return resolution
            
        except Exception as e:
            logger.error(f"Error handling timeout: {e}")
            raise HumanInteractionError(f"Failed to handle timeout: {e}")
    
    async def integrate_feedback(self, feedback: HumanFeedback) -> None:
        """
        Integrate human feedback for continuous learning.
        
        Args:
            feedback: Human feedback to integrate
        """
        try:
            await self.feedback_integrator.process_feedback(feedback)
            
            logger.info(f"Integrated feedback {feedback.feedback_id}")
            
        except Exception as e:
            logger.error(f"Error integrating feedback: {e}")
            raise HumanInteractionError(f"Failed to integrate feedback: {e}")
    
    async def get_queue_status(self, queue_name: Optional[str] = None) -> Dict[str, Any]:
        """
        Get status of approval queues.
        
        Args:
            queue_name: Optional specific queue name
            
        Returns:
            Queue status information
        """
        if queue_name:
            if queue_name not in self.approval_queues:
                return {"error": f"Queue '{queue_name}' not found"}
            
            queue = self.approval_queues[queue_name]
            return {
                "queue_name": queue.name,
                "pending_requests": len(queue.pending_requests),
                "active_requests": len(queue.active_requests),
                "total_processed": queue.total_processed,
                "average_response_time_minutes": queue.average_response_time_minutes,
                "approval_rate": queue.approval_rate,
            }
        else:
            # Return status for all queues
            return {
                "total_queues": len(self.approval_queues),
                "queues": {
                    name: {
                        "pending_requests": len(queue.pending_requests),
                        "active_requests": len(queue.active_requests),
                        "total_processed": queue.total_processed,
                    }
                    for name, queue in self.approval_queues.items()
                },
                "overall_metrics": self.metrics,
            }
    
    async def create_queue(
        self,
        name: str,
        description: Optional[str] = None,
        assigned_roles: Optional[List[str]] = None,
        assigned_users: Optional[List[str]] = None,
    ) -> ApprovalQueue:
        """
        Create a new approval queue.
        
        Args:
            name: Name of the queue
            description: Optional description
            assigned_roles: Roles assigned to this queue
            assigned_users: Users assigned to this queue
            
        Returns:
            Created ApprovalQueue
        """
        if name in self.approval_queues:
            raise HumanInteractionError(f"Queue '{name}' already exists")
        
        queue = ApprovalQueue(
            name=name,
            description=description,
            assigned_roles=assigned_roles or [],
            assigned_users=assigned_users or [],
        )
        
        self.approval_queues[name] = queue
        
        logger.info(f"Created approval queue '{name}'")
        
        return queue
    
    async def _route_to_queue(
        self,
        request: ApprovalRequest,
        preferred_queue: Optional[str] = None,
    ) -> Optional[ApprovalQueue]:
        """Route an approval request to the appropriate queue."""
        if preferred_queue and preferred_queue in self.approval_queues:
            return self.approval_queues[preferred_queue]
        
        # Find queue based on required approvers
        for queue in self.approval_queues.values():
            # Check if any required approvers match queue assignments
            if (any(role in queue.assigned_roles for role in request.approval_roles) or
                any(user in queue.assigned_users for user in request.required_approvers)):
                return queue
        
        # Create default queue if none exists
        if not self.approval_queues:
            return await self.create_queue(
                name="default",
                description="Default approval queue",
            )
        
        # Return first available queue
        return next(iter(self.approval_queues.values()))
    
    async def _send_notifications(
        self,
        request: ApprovalRequest,
        queue: Optional[ApprovalQueue],
    ) -> None:
        """Send notifications for a new approval request."""
        if not self.notification_callback:
            return
        
        notification_data = {
            "type": "approval_request",
            "request_id": request.request_id,
            "operation_type": request.operation_type,
            "risk_level": request.risk_assessment.risk_level,
            "queue_name": queue.name if queue else "default",
            "timeout_minutes": request.timeout_policy.timeout_minutes,
        }
        
        try:
            await self.notification_callback(notification_data)
        except Exception as e:
            logger.error(f"Error sending notification: {e}")
    
    async def _remove_from_queues(self, request_id: str) -> None:
        """Remove a request from all queues."""
        for queue in self.approval_queues.values():
            if request_id in queue.pending_requests:
                queue.pending_requests.remove(request_id)
            if request_id in queue.active_requests:
                queue.active_requests.remove(request_id)
            queue.updated_at = datetime.utcnow()
    
    async def _handle_escalation(
        self,
        request: ApprovalRequest,
        timeout_event: TimeoutEvent,
    ) -> None:
        """Handle escalation of a timed-out request."""
        # In a real implementation, this would:
        # 1. Move request to higher-priority queue
        # 2. Notify escalation contacts
        # 3. Adjust timeout policies
        # 4. Update request metadata
        
        logger.info(f"Escalating request {request.request_id}")
        
        # Find or create escalation queue
        escalation_queue_name = "escalation"
        if escalation_queue_name not in self.approval_queues:
            await self.create_queue(
                name=escalation_queue_name,
                description="Escalated approval requests",
                assigned_roles=["senior_approver", "manager"],
            )
        
        escalation_queue = self.approval_queues[escalation_queue_name]
        escalation_queue.pending_requests.append(request.request_id)
        escalation_queue.updated_at = datetime.utcnow()
    
    async def _generate_learning_feedback(
        self,
        request: ApprovalRequest,
        response: ApprovalResponse,
    ) -> None:
        """Generate learning feedback from approval interactions."""
        feedback_type = {
            ApprovalStatus.APPROVED: FeedbackType.APPROVAL,
            ApprovalStatus.REJECTED: FeedbackType.REJECTION,
            ApprovalStatus.TIMEOUT: FeedbackType.ESCALATION,
        }.get(response.status, FeedbackType.LEARNING)
        
        # Extract context tags from request
        context_tags = [
            request.operation_type,
            request.risk_assessment.risk_level,
        ]
        context_tags.extend(request.risk_assessment.risk_factors)
        
        if request.data_sensitivity_level:
            context_tags.append(request.data_sensitivity_level)
        
        # Create learning feedback
        learning_feedback = HumanFeedback(
            feedback_type=feedback_type,
            request_id=request.request_id,
            response_id=response.response_id,
            feedback_text=response.comments or f"System-generated feedback for {response.status}",
            context_tags=context_tags,
            session_id=request.session_id,
            provided_by="system",  # System-generated feedback
        )
        
        await self.feedback_integrator.process_feedback(learning_feedback)
    
    def _update_average_response_time(self, new_response_time: float) -> None:
        """Update the running average response time."""
        total_responses = (
            self.metrics["approved_requests"] + 
            self.metrics["rejected_requests"] + 
            self.metrics["timed_out_requests"]
        )
        
        if total_responses == 1:
            self.metrics["average_response_time_minutes"] = new_response_time
        else:
            current_avg = self.metrics["average_response_time_minutes"]
            self.metrics["average_response_time_minutes"] = (
                (current_avg * (total_responses - 1) + new_response_time) / total_responses
            )
    
    async def cleanup(self) -> None:
        """Clean up resources."""
        await self.approval_workflow.cleanup()
        
        self.active_requests.clear()
        self.processed_requests.clear()
        self.approval_queues.clear()
        
        logger.info("HumanInteractionManager cleanup completed")


================================================
FILE: dataqa/orchestration/approval/models.py
================================================
"""
Pydantic models for human-in-the-loop approval system.
"""

from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional
from uuid import UUID, uuid4

from pydantic import BaseModel, Field, validator


class OperationType(str, Enum):
    """Types of operations that may require approval."""
    DATA_MODIFICATION = "data_modification"
    SCHEMA_CHANGE = "schema_change"
    EXTERNAL_API_CALL = "external_api_call"
    SENSITIVE_DATA_ACCESS = "sensitive_data_access"
    FINANCIAL_CALCULATION = "financial_calculation"
    REGULATORY_COMPLIANCE = "regulatory_compliance"
    SYSTEM_CONFIGURATION = "system_configuration"
    USER_DATA_EXPORT = "user_data_export"


class RiskLevel(str, Enum):
    """Risk levels for operations requiring approval."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ApprovalStatus(str, Enum):
    """Status of approval requests."""
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    TIMEOUT = "timeout"
    CANCELLED = "cancelled"


class FeedbackType(str, Enum):
    """Types of human feedback."""
    APPROVAL = "approval"
    REJECTION = "rejection"
    MODIFICATION = "modification"
    ESCALATION = "escalation"
    LEARNING = "learning"


class TimeoutPolicy(BaseModel):
    """Policy for handling approval timeouts."""
    timeout_minutes: int = 60
    escalation_enabled: bool = True
    escalation_delay_minutes: int = 30
    auto_reject_on_timeout: bool = False
    fallback_action: Optional[str] = None
    notification_intervals: List[int] = Field(default_factory=lambda: [15, 30, 45])  # minutes


class RiskAssessment(BaseModel):
    """Risk assessment for operations requiring approval."""
    risk_level: RiskLevel
    risk_factors: List[str] = Field(default_factory=list)
    impact_description: str
    likelihood_score: float = Field(ge=0.0, le=1.0)  # 0.0 = unlikely, 1.0 = certain
    severity_score: float = Field(ge=0.0, le=1.0)  # 0.0 = minimal, 1.0 = severe
    mitigation_strategies: List[str] = Field(default_factory=list)
    compliance_implications: List[str] = Field(default_factory=list)
    
    @property
    def risk_score(self) -> float:
        """Calculate overall risk score."""
        return (self.likelihood_score * self.severity_score)


class AlternativeAction(BaseModel):
    """Alternative action that could be taken instead."""
    action_id: str = Field(default_factory=lambda: str(uuid4()))
    description: str
    risk_level: RiskLevel
    trade_offs: List[str] = Field(default_factory=list)
    implementation_complexity: str = "medium"  # "low", "medium", "high"


class ApprovalRequest(BaseModel):
    """Request for human approval of a sensitive operation."""
    request_id: str = Field(default_factory=lambda: str(uuid4()))
    operation_type: OperationType
    operation_description: str
    context_explanation: str
    risk_assessment: RiskAssessment
    alternative_options: List[AlternativeAction] = Field(default_factory=list)
    timeout_policy: TimeoutPolicy = Field(default_factory=TimeoutPolicy)
    
    # Request metadata
    requested_by: str  # agent_id or user_id
    requested_at: datetime = Field(default_factory=datetime.utcnow)
    session_id: str
    workflow_id: str
    step_id: Optional[str] = None
    
    # Approval requirements
    required_approvers: List[str] = Field(default_factory=list)
    minimum_approvals: int = 1
    approval_roles: List[str] = Field(default_factory=list)
    
    # Additional context
    affected_resources: List[str] = Field(default_factory=list)
    data_sensitivity_level: Optional[str] = None
    regulatory_context: List[str] = Field(default_factory=list)
    business_justification: Optional[str] = None
    
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        use_enum_values = True


class ApprovalResponse(BaseModel):
    """Response to an approval request."""
    response_id: str = Field(default_factory=lambda: str(uuid4()))
    request_id: str
    status: ApprovalStatus
    
    # Response details
    approved_by: Optional[str] = None  # user_id or role
    responded_at: datetime = Field(default_factory=datetime.utcnow)
    comments: Optional[str] = None
    conditions: List[str] = Field(default_factory=list)  # Conditions for approval
    
    # Modifications or alternatives
    suggested_modifications: List[str] = Field(default_factory=list)
    approved_alternative: Optional[str] = None  # alternative_action_id
    
    # Escalation info
    escalated_to: Optional[str] = None
    escalation_reason: Optional[str] = None
    
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        use_enum_values = True


class HumanFeedback(BaseModel):
    """Feedback from humans for continuous learning."""
    feedback_id: str = Field(default_factory=lambda: str(uuid4()))
    feedback_type: FeedbackType
    request_id: str
    response_id: Optional[str] = None
    
    # Feedback content
    feedback_text: str
    rating: Optional[int] = Field(None, ge=1, le=5)  # 1-5 star rating
    improvement_suggestions: List[str] = Field(default_factory=list)
    
    # Learning context
    context_tags: List[str] = Field(default_factory=list)
    similar_scenarios: List[str] = Field(default_factory=list)
    learning_priority: str = "medium"  # "low", "medium", "high"
    
    # Feedback metadata
    provided_by: str  # user_id
    provided_at: datetime = Field(default_factory=datetime.utcnow)
    session_id: str
    
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        use_enum_values = True


class ApprovalPolicy(BaseModel):
    """Policy defining when approvals are required."""
    policy_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    
    # Trigger conditions
    operation_types: List[OperationType] = Field(default_factory=list)
    risk_threshold: RiskLevel = RiskLevel.MEDIUM
    data_sensitivity_levels: List[str] = Field(default_factory=list)
    resource_patterns: List[str] = Field(default_factory=list)  # Regex patterns
    
    # Approval requirements
    required_roles: List[str] = Field(default_factory=list)
    minimum_approvals: int = 1
    timeout_policy: TimeoutPolicy = Field(default_factory=TimeoutPolicy)
    
    # Policy metadata
    enabled: bool = True
    priority: int = 1  # Higher number = higher priority
    applicable_domains: List[str] = Field(default_factory=list)
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        use_enum_values = True


class EscalationRule(BaseModel):
    """Rule for escalating approval requests."""
    rule_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    
    # Escalation triggers
    trigger_conditions: List[str] = Field(default_factory=list)
    timeout_minutes: int = 60
    risk_threshold: RiskLevel = RiskLevel.HIGH
    
    # Escalation actions
    escalate_to_roles: List[str] = Field(default_factory=list)
    escalate_to_users: List[str] = Field(default_factory=list)
    notification_channels: List[str] = Field(default_factory=list)
    
    # Rule metadata
    enabled: bool = True
    priority: int = 1
    
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        use_enum_values = True


class TimeoutEvent(BaseModel):
    """Event representing an approval timeout."""
    event_id: str = Field(default_factory=lambda: str(uuid4()))
    request_id: str
    timeout_occurred_at: datetime = Field(default_factory=datetime.utcnow)
    timeout_duration_minutes: int
    
    # Timeout handling
    escalation_triggered: bool = False
    fallback_action_taken: Optional[str] = None
    auto_rejected: bool = False
    
    # Context
    pending_approvers: List[str] = Field(default_factory=list)
    notifications_sent: int = 0
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


class TimeoutResolution(BaseModel):
    """Resolution for a timeout event."""
    resolution_id: str = Field(default_factory=lambda: str(uuid4()))
    timeout_event_id: str
    resolution_type: str  # "escalated", "auto_rejected", "fallback", "manual"
    resolution_description: str
    resolved_at: datetime = Field(default_factory=datetime.utcnow)
    resolved_by: Optional[str] = None
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ApprovalQueue(BaseModel):
    """Queue of pending approval requests."""
    queue_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: Optional[str] = None
    
    # Queue configuration
    assigned_roles: List[str] = Field(default_factory=list)
    assigned_users: List[str] = Field(default_factory=list)
    priority_rules: List[str] = Field(default_factory=list)
    
    # Queue state
    pending_requests: List[str] = Field(default_factory=list)  # request_ids
    active_requests: List[str] = Field(default_factory=list)  # request_ids being processed
    
    # Queue metrics
    total_processed: int = 0
    average_response_time_minutes: float = 0.0
    approval_rate: float = 0.0
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


================================================
FILE: dataqa/orchestration/approval/workflow.py
================================================
"""
ApprovalWorkflow for identifying and managing sensitive operations.
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set

from ...exceptions import DataQAError
from .models import (
    AlternativeAction,
    ApprovalPolicy,
    ApprovalRequest,
    ApprovalResponse,
    ApprovalStatus,
    EscalationRule,
    OperationType,
    RiskAssessment,
    RiskLevel,
    TimeoutEvent,
    TimeoutPolicy,
)

logger = logging.getLogger(__name__)


class ApprovalWorkflowError(DataQAError):
    """Errors related to approval workflow operations."""
    pass


class ApprovalWorkflow:
    """
    Workflow for identifying and managing sensitive operations that require human approval.
    
    This class handles:
    - Identifying operations that require approval based on policies
    - Generating approval requests with risk assessments
    - Managing approval queues and routing
    - Handling timeouts and escalations
    """
    
    def __init__(
        self,
        policies: Optional[List[ApprovalPolicy]] = None,
        escalation_rules: Optional[List[EscalationRule]] = None,
        default_timeout_minutes: int = 60,
    ):
        """
        Initialize the approval workflow.
        
        Args:
            policies: List of approval policies to apply
            escalation_rules: List of escalation rules for timeouts
            default_timeout_minutes: Default timeout for approval requests
        """
        self.policies = policies or []
        self.escalation_rules = escalation_rules or []
        self.default_timeout_minutes = default_timeout_minutes
        
        # Internal state
        self._pending_requests: Dict[str, ApprovalRequest] = {}
        self._request_timers: Dict[str, asyncio.Task] = {}
        
        logger.info(
            f"ApprovalWorkflow initialized with {len(self.policies)} policies "
            f"and {len(self.escalation_rules)} escalation rules"
        )
    
    async def requires_approval(
        self,
        operation_type: OperationType,
        context: Dict[str, Any],
        risk_assessment: Optional[RiskAssessment] = None,
    ) -> bool:
        """
        Determine if an operation requires human approval.
        
        Args:
            operation_type: Type of operation being performed
            context: Context information about the operation
            risk_assessment: Optional pre-computed risk assessment
            
        Returns:
            True if approval is required, False otherwise
        """
        try:
            # Check each policy to see if it applies
            for policy in self.policies:
                if not policy.enabled:
                    continue
                
                # Check operation type match
                if policy.operation_types and operation_type not in policy.operation_types:
                    continue
                
                # Check risk threshold
                if risk_assessment and risk_assessment.risk_level.value < policy.risk_threshold.value:
                    continue
                
                # Check data sensitivity levels
                data_sensitivity = context.get("data_sensitivity_level")
                if (policy.data_sensitivity_levels and 
                    data_sensitivity not in policy.data_sensitivity_levels):
                    continue
                
                # Check resource patterns
                if policy.resource_patterns:
                    affected_resources = context.get("affected_resources", [])
                    if not any(
                        any(pattern in resource for pattern in policy.resource_patterns)
                        for resource in affected_resources
                    ):
                        continue
                
                # Check domain applicability
                domain = context.get("domain")
                if policy.applicable_domains and domain not in policy.applicable_domains:
                    continue
                
                logger.info(f"Policy '{policy.name}' requires approval for {operation_type}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking approval requirements: {e}")
            # Fail safe - require approval if we can't determine
            return True
    
    async def create_approval_request(
        self,
        operation_type: OperationType,
        operation_description: str,
        context: Dict[str, Any],
        risk_assessment: Optional[RiskAssessment] = None,
        alternatives: Optional[List[AlternativeAction]] = None,
        session_id: str = "",
        workflow_id: str = "",
        requested_by: str = "",
    ) -> ApprovalRequest:
        """
        Create an approval request for a sensitive operation.
        
        Args:
            operation_type: Type of operation requiring approval
            operation_description: Description of the operation
            context: Context information about the operation
            risk_assessment: Risk assessment for the operation
            alternatives: Alternative actions that could be taken
            session_id: ID of the execution session
            workflow_id: ID of the workflow
            requested_by: ID of the agent or user requesting approval
            
        Returns:
            ApprovalRequest object
        """
        try:
            # Generate risk assessment if not provided
            if risk_assessment is None:
                risk_assessment = await self._generate_risk_assessment(
                    operation_type, context
                )
            
            # Find applicable policy for timeout and approval requirements
            applicable_policy = self._find_applicable_policy(operation_type, context)
            
            # Create timeout policy
            timeout_policy = TimeoutPolicy()
            if applicable_policy:
                timeout_policy = applicable_policy.timeout_policy
            else:
                timeout_policy.timeout_minutes = self.default_timeout_minutes
            
            # Generate context explanation
            context_explanation = self._generate_context_explanation(
                operation_type, operation_description, context, risk_assessment
            )
            
            # Create the approval request
            request = ApprovalRequest(
                operation_type=operation_type,
                operation_description=operation_description,
                context_explanation=context_explanation,
                risk_assessment=risk_assessment,
                alternative_options=alternatives or [],
                timeout_policy=timeout_policy,
                requested_by=requested_by,
                session_id=session_id,
                workflow_id=workflow_id,
                required_approvers=applicable_policy.required_roles if applicable_policy else [],
                minimum_approvals=applicable_policy.minimum_approvals if applicable_policy else 1,
                affected_resources=context.get("affected_resources", []),
                data_sensitivity_level=context.get("data_sensitivity_level"),
                regulatory_context=context.get("regulatory_context", []),
                business_justification=context.get("business_justification"),
                metadata=context.get("metadata", {}),
            )
            
            # Store the request
            self._pending_requests[request.request_id] = request
            
            # Start timeout timer
            await self._start_timeout_timer(request)
            
            logger.info(
                f"Created approval request {request.request_id} for {operation_type} "
                f"with risk level {risk_assessment.risk_level}"
            )
            
            return request
            
        except Exception as e:
            logger.error(f"Error creating approval request: {e}")
            raise ApprovalWorkflowError(f"Failed to create approval request: {e}")
    
    async def process_response(
        self,
        request_id: str,
        response: ApprovalResponse,
    ) -> None:
        """
        Process a response to an approval request.
        
        Args:
            request_id: ID of the approval request
            response: Response from the approver
        """
        try:
            if request_id not in self._pending_requests:
                raise ApprovalWorkflowError(f"Approval request {request_id} not found")
            
            request = self._pending_requests[request_id]
            
            # Cancel timeout timer
            if request_id in self._request_timers:
                self._request_timers[request_id].cancel()
                del self._request_timers[request_id]
            
            # Remove from pending requests
            del self._pending_requests[request_id]
            
            logger.info(
                f"Processed approval response for request {request_id}: {response.status}"
            )
            
        except Exception as e:
            logger.error(f"Error processing approval response: {e}")
            raise ApprovalWorkflowError(f"Failed to process approval response: {e}")
    
    async def handle_timeout(self, request_id: str) -> TimeoutEvent:
        """
        Handle timeout for an approval request.
        
        Args:
            request_id: ID of the timed-out request
            
        Returns:
            TimeoutEvent describing what happened
        """
        try:
            if request_id not in self._pending_requests:
                raise ApprovalWorkflowError(f"Approval request {request_id} not found")
            
            request = self._pending_requests[request_id]
            
            # Create timeout event
            timeout_event = TimeoutEvent(
                request_id=request_id,
                timeout_duration_minutes=request.timeout_policy.timeout_minutes,
                pending_approvers=request.required_approvers.copy(),
            )
            
            # Check if escalation should be triggered
            should_escalate = await self._should_escalate(request)
            if should_escalate:
                timeout_event.escalation_triggered = True
                await self._trigger_escalation(request)
            
            # Check if auto-rejection should occur
            if request.timeout_policy.auto_reject_on_timeout:
                timeout_event.auto_rejected = True
                # Create rejection response
                rejection_response = ApprovalResponse(
                    request_id=request_id,
                    status=ApprovalStatus.TIMEOUT,
                    comments="Request timed out and was automatically rejected",
                )
                await self.process_response(request_id, rejection_response)
            
            # Execute fallback action if specified
            if request.timeout_policy.fallback_action:
                timeout_event.fallback_action_taken = request.timeout_policy.fallback_action
                # Note: Actual fallback execution would be handled by the calling system
            
            logger.warning(
                f"Approval request {request_id} timed out after "
                f"{request.timeout_policy.timeout_minutes} minutes"
            )
            
            return timeout_event
            
        except Exception as e:
            logger.error(f"Error handling timeout for request {request_id}: {e}")
            raise ApprovalWorkflowError(f"Failed to handle timeout: {e}")
    
    async def get_pending_requests(
        self,
        approver_role: Optional[str] = None,
        risk_level: Optional[RiskLevel] = None,
    ) -> List[ApprovalRequest]:
        """
        Get pending approval requests, optionally filtered.
        
        Args:
            approver_role: Filter by required approver role
            risk_level: Filter by minimum risk level
            
        Returns:
            List of pending approval requests
        """
        requests = list(self._pending_requests.values())
        
        if approver_role:
            requests = [
                req for req in requests
                if approver_role in req.required_approvers or approver_role in req.approval_roles
            ]
        
        if risk_level:
            requests = [
                req for req in requests
                if self._risk_level_priority(req.risk_assessment.risk_level) >= self._risk_level_priority(risk_level)
            ]
        
        # Sort by risk level (highest first) and then by request time
        requests.sort(
            key=lambda r: (
                -self._risk_level_priority(r.risk_assessment.risk_level),
                r.requested_at
            )
        )
        
        return requests
    
    def _find_applicable_policy(
        self,
        operation_type: OperationType,
        context: Dict[str, Any],
    ) -> Optional[ApprovalPolicy]:
        """Find the most applicable policy for an operation."""
        applicable_policies = []
        
        for policy in self.policies:
            if not policy.enabled:
                continue
            
            if policy.operation_types and operation_type not in policy.operation_types:
                continue
            
            # Check domain applicability
            domain = context.get("domain")
            if policy.applicable_domains and domain not in policy.applicable_domains:
                continue
            
            applicable_policies.append(policy)
        
        # Return highest priority policy
        if applicable_policies:
            return max(applicable_policies, key=lambda p: p.priority)
        
        return None
    
    async def _generate_risk_assessment(
        self,
        operation_type: OperationType,
        context: Dict[str, Any],
    ) -> RiskAssessment:
        """Generate a risk assessment for an operation."""
        # This is a simplified risk assessment - in practice, this would be more sophisticated
        risk_factors = []
        impact_description = f"Operation of type {operation_type}"
        
        # Assess based on operation type
        base_risk = {
            OperationType.DATA_MODIFICATION: RiskLevel.HIGH,
            OperationType.SCHEMA_CHANGE: RiskLevel.CRITICAL,
            OperationType.EXTERNAL_API_CALL: RiskLevel.MEDIUM,
            OperationType.SENSITIVE_DATA_ACCESS: RiskLevel.HIGH,
            OperationType.FINANCIAL_CALCULATION: RiskLevel.HIGH,
            OperationType.REGULATORY_COMPLIANCE: RiskLevel.CRITICAL,
            OperationType.SYSTEM_CONFIGURATION: RiskLevel.HIGH,
            OperationType.USER_DATA_EXPORT: RiskLevel.HIGH,
        }.get(operation_type, RiskLevel.MEDIUM)
        
        # Adjust based on context
        data_sensitivity = context.get("data_sensitivity_level", "")
        if "sensitive" in data_sensitivity.lower() or "confidential" in data_sensitivity.lower():
            risk_factors.append("Sensitive data involved")
            if base_risk == RiskLevel.LOW:
                base_risk = RiskLevel.MEDIUM
            elif base_risk == RiskLevel.MEDIUM:
                base_risk = RiskLevel.HIGH
        
        if context.get("regulatory_context"):
            risk_factors.append("Regulatory implications")
            impact_description += " with regulatory implications"
        
        if context.get("affected_resources"):
            resource_count = len(context["affected_resources"])
            if resource_count > 10:
                risk_factors.append(f"Large number of affected resources ({resource_count})")
        
        # Calculate likelihood and severity scores
        likelihood_score = {
            RiskLevel.LOW: 0.2,
            RiskLevel.MEDIUM: 0.5,
            RiskLevel.HIGH: 0.7,
            RiskLevel.CRITICAL: 0.9,
        }[base_risk]
        
        severity_score = {
            RiskLevel.LOW: 0.3,
            RiskLevel.MEDIUM: 0.5,
            RiskLevel.HIGH: 0.8,
            RiskLevel.CRITICAL: 1.0,
        }[base_risk]
        
        return RiskAssessment(
            risk_level=base_risk,
            risk_factors=risk_factors,
            impact_description=impact_description,
            likelihood_score=likelihood_score,
            severity_score=severity_score,
            mitigation_strategies=[
                "Human approval required",
                "Audit trail maintained",
                "Rollback capability available",
            ],
            compliance_implications=context.get("regulatory_context", []),
        )
    
    def _generate_context_explanation(
        self,
        operation_type: OperationType,
        operation_description: str,
        context: Dict[str, Any],
        risk_assessment: RiskAssessment,
    ) -> str:
        """Generate a human-readable explanation of the operation context."""
        explanation_parts = [
            f"Operation: {operation_description}",
            f"Type: {operation_type.value.replace('_', ' ').title()}",
            f"Risk Level: {risk_assessment.risk_level.value.title()}",
        ]
        
        if risk_assessment.risk_factors:
            explanation_parts.append(f"Risk Factors: {', '.join(risk_assessment.risk_factors)}")
        
        if context.get("affected_resources"):
            resources = context["affected_resources"]
            if len(resources) <= 3:
                explanation_parts.append(f"Affected Resources: {', '.join(resources)}")
            else:
                explanation_parts.append(
                    f"Affected Resources: {', '.join(resources[:3])} and {len(resources) - 3} others"
                )
        
        if context.get("business_justification"):
            explanation_parts.append(f"Business Justification: {context['business_justification']}")
        
        if context.get("regulatory_context"):
            explanation_parts.append(f"Regulatory Context: {', '.join(context['regulatory_context'])}")
        
        return "\n".join(explanation_parts)
    
    async def _start_timeout_timer(self, request: ApprovalRequest) -> None:
        """Start a timeout timer for an approval request."""
        async def timeout_handler():
            try:
                await asyncio.sleep(request.timeout_policy.timeout_minutes * 60)
                await self.handle_timeout(request.request_id)
            except asyncio.CancelledError:
                # Timer was cancelled (normal when request is processed)
                pass
            except Exception as e:
                logger.error(f"Error in timeout handler for request {request.request_id}: {e}")
        
        timer_task = asyncio.create_task(timeout_handler())
        self._request_timers[request.request_id] = timer_task
    
    async def _should_escalate(self, request: ApprovalRequest) -> bool:
        """Determine if a request should be escalated based on escalation rules."""
        for rule in self.escalation_rules:
            if not rule.enabled:
                continue
            
            # Check risk threshold
            request_risk_value = self._risk_level_priority(request.risk_assessment.risk_level)
            rule_risk_value = self._risk_level_priority(rule.risk_threshold)
            if request_risk_value >= rule_risk_value:
                return True
            
            # Check other trigger conditions (simplified)
            if "high_value" in rule.trigger_conditions and request.metadata.get("high_value"):
                return True
        
        return False
    
    async def _trigger_escalation(self, request: ApprovalRequest) -> None:
        """Trigger escalation for a request."""
        # In a real implementation, this would send notifications, update queues, etc.
        logger.warning(f"Escalating approval request {request.request_id}")
        
        # Find applicable escalation rules
        for rule in self.escalation_rules:
            if not rule.enabled:
                continue
            
            request_risk_value = self._risk_level_priority(request.risk_assessment.risk_level)
            rule_risk_value = self._risk_level_priority(rule.risk_threshold)
            if request_risk_value >= rule_risk_value:
                # Add escalation roles to required approvers
                request.required_approvers.extend(rule.escalate_to_roles)
                request.required_approvers.extend(rule.escalate_to_users)
                
                # Remove duplicates
                request.required_approvers = list(set(request.required_approvers))
                
                logger.info(
                    f"Escalated request {request.request_id} to roles: {rule.escalate_to_roles}"
                )
                break
    
    def _risk_level_priority(self, risk_level: RiskLevel) -> int:
        """Get numeric priority for risk level (higher = more urgent)."""
        return {
            RiskLevel.LOW: 1,
            RiskLevel.MEDIUM: 2,
            RiskLevel.HIGH: 3,
            RiskLevel.CRITICAL: 4,
        }[risk_level]
    
    async def cleanup(self) -> None:
        """Clean up resources and cancel pending timers."""
        for timer in self._request_timers.values():
            timer.cancel()
        
        self._request_timers.clear()
        self._pending_requests.clear()
        
        logger.info("ApprovalWorkflow cleanup completed")


================================================
FILE: dataqa/orchestration/domain/__init__.py
================================================
"""
Domain knowledge integration and business rules management.
"""

from .knowledge import DomainKnowledgeManager
from .rules import BusinessRulesEngine
from .context import DomainContext

__all__ = [
    "DomainKnowledgeManager",
    "BusinessRulesEngine", 
    "DomainContext",
]


================================================
FILE: dataqa/orchestration/domain/context.py
================================================
"""
Domain context management and utilities with schema-driven rules and entity filtering.
"""

import json
from typing import Any, Dict, List, Optional, Set, Union
from uuid import uuid4

from pydantic import BaseModel, Field

from ..models import DomainContext, BusinessRule, SchemaConstraint, RegulatoryRequirement, Policy


class EntityFilter(BaseModel):
    """Filter configuration for domain entities."""
    filter_id: str = Field(default_factory=lambda: str(uuid4()))
    entity_type: str
    filter_expression: str
    include_fields: Optional[List[str]] = None
    exclude_fields: Optional[List[str]] = None
    conditions: Dict[str, Any] = Field(default_factory=dict)
    enabled: bool = True


class SchemaRule(BaseModel):
    """Schema-driven rule definition."""
    rule_id: str = Field(default_factory=lambda: str(uuid4()))
    schema_path: str
    rule_type: str  # "validation", "transformation", "filtering"
    rule_expression: str
    target_entities: List[str] = Field(default_factory=list)
    priority: int = 1
    enabled: bool = True
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ContextInjectionConfig(BaseModel):
    """Configuration for domain context injection."""
    config_id: str = Field(default_factory=lambda: str(uuid4()))
    domain_name: str
    injection_points: List[str] = Field(default_factory=list)  # Where to inject context
    entity_filters: List[EntityFilter] = Field(default_factory=list)
    schema_rules: List[SchemaRule] = Field(default_factory=list)
    auto_inject: bool = True
    metadata: Dict[str, Any] = Field(default_factory=dict)


class DomainContextInjector:
    """Handles injection of domain context into agent workflows."""
    
    def __init__(self):
        """Initialize the context injector."""
        self.injection_configs: Dict[str, ContextInjectionConfig] = {}
        self.entity_schemas: Dict[str, Dict[str, Any]] = {}
    
    def register_injection_config(self, config: ContextInjectionConfig) -> None:
        """Register a context injection configuration."""
        self.injection_configs[config.domain_name] = config
    
    def register_entity_schema(self, entity_type: str, schema: Dict[str, Any]) -> None:
        """Register an entity schema for validation and filtering."""
        self.entity_schemas[entity_type] = schema
    
    async def inject_context(self, workflow_data: Dict[str, Any], domain_name: str, context: DomainContext) -> Dict[str, Any]:
        """Inject domain context into workflow data."""
        config = self.injection_configs.get(domain_name)
        if not config or not config.auto_inject:
            return workflow_data
        
        injected_data = workflow_data.copy()
        
        # Apply entity filters
        injected_data = await self._apply_entity_filters(injected_data, config.entity_filters)
        
        # Apply schema rules
        injected_data = await self._apply_schema_rules(injected_data, config.schema_rules)
        
        # Inject context metadata
        injected_data["domain_context"] = {
            "domain_name": domain_name,
            "rules_count": len(context.applicable_rules),
            "constraints_count": len(context.schema_constraints),
            "policies_count": len(context.organizational_policies),
            "injection_config_id": config.config_id
        }
        
        return injected_data
    
    async def _apply_entity_filters(self, data: Dict[str, Any], filters: List[EntityFilter]) -> Dict[str, Any]:
        """Apply entity filters to workflow data."""
        filtered_data = data.copy()
        
        for entity_filter in filters:
            if not entity_filter.enabled:
                continue
            
            # Find entities of the specified type
            entities = self._find_entities(filtered_data, entity_filter.entity_type)
            
            # Apply filtering
            filtered_entities = []
            for entity in entities:
                if self._entity_matches_filter(entity, entity_filter):
                    filtered_entity = self._apply_field_filters(entity, entity_filter)
                    filtered_entities.append(filtered_entity)
            
            # Update data with filtered entities
            self._update_entities(filtered_data, entity_filter.entity_type, filtered_entities)
        
        return filtered_data
    
    def _find_entities(self, data: Dict[str, Any], entity_type: str) -> List[Dict[str, Any]]:
        """Find entities of a specific type in the data."""
        entities = []
        
        # Simple entity discovery - can be enhanced with more sophisticated logic
        if entity_type in data:
            entity_data = data[entity_type]
            if isinstance(entity_data, list):
                entities.extend(entity_data)
            elif isinstance(entity_data, dict):
                entities.append(entity_data)
        
        # Look for entities in nested structures
        for key, value in data.items():
            if isinstance(value, dict) and value.get("type") == entity_type:
                entities.append(value)
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, dict) and item.get("type") == entity_type:
                        entities.append(item)
        
        return entities
    
    def _entity_matches_filter(self, entity: Dict[str, Any], entity_filter: EntityFilter) -> bool:
        """Check if an entity matches the filter conditions."""
        # Evaluate filter expression
        if entity_filter.filter_expression:
            try:
                # Simple expression evaluation - enhance with proper expression engine
                if self._evaluate_filter_expression(entity, entity_filter.filter_expression):
                    return True
            except Exception:
                # If expression evaluation fails, fall back to condition matching
                pass
        
        # Check conditions
        for condition_key, condition_value in entity_filter.conditions.items():
            entity_value = entity.get(condition_key)
            if entity_value != condition_value:
                return False
        
        return True
    
    def _evaluate_filter_expression(self, entity: Dict[str, Any], expression: str) -> bool:
        """Evaluate a filter expression against an entity."""
        # Simple expression evaluation - replace with proper expression engine
        # For now, support basic comparisons
        
        if "==" in expression:
            parts = expression.split("==")
            if len(parts) == 2:
                field_name = parts[0].strip()
                expected_value = parts[1].strip().strip("'\"")
                return str(entity.get(field_name)) == expected_value
        
        if "!=" in expression:
            parts = expression.split("!=")
            if len(parts) == 2:
                field_name = parts[0].strip()
                expected_value = parts[1].strip().strip("'\"")
                return str(entity.get(field_name)) != expected_value
        
        if "contains" in expression:
            # Format: field_name contains 'value'
            parts = expression.split("contains")
            if len(parts) == 2:
                field_name = parts[0].strip()
                search_value = parts[1].strip().strip("'\"")
                field_value = entity.get(field_name, "")
                return search_value in str(field_value)
        
        return True  # Default to include if expression can't be evaluated
    
    def _apply_field_filters(self, entity: Dict[str, Any], entity_filter: EntityFilter) -> Dict[str, Any]:
        """Apply field inclusion/exclusion filters to an entity."""
        filtered_entity = entity.copy()
        
        # Apply field exclusions
        if entity_filter.exclude_fields:
            for field in entity_filter.exclude_fields:
                filtered_entity.pop(field, None)
        
        # Apply field inclusions (if specified, only include these fields)
        if entity_filter.include_fields:
            included_entity = {}
            for field in entity_filter.include_fields:
                if field in filtered_entity:
                    included_entity[field] = filtered_entity[field]
            filtered_entity = included_entity
        
        return filtered_entity
    
    def _update_entities(self, data: Dict[str, Any], entity_type: str, filtered_entities: List[Dict[str, Any]]) -> None:
        """Update the data with filtered entities."""
        if entity_type in data:
            data[entity_type] = filtered_entities
        
        # Update nested structures
        for key, value in data.items():
            if isinstance(value, list):
                updated_list = []
                for item in value:
                    if isinstance(item, dict) and item.get("type") == entity_type:
                        # Find matching filtered entity
                        item_id = item.get("id")
                        matching_entity = next(
                            (e for e in filtered_entities if e.get("id") == item_id),
                            None
                        )
                        if matching_entity:
                            updated_list.append(matching_entity)
                    else:
                        updated_list.append(item)
                data[key] = updated_list
    
    async def _apply_schema_rules(self, data: Dict[str, Any], rules: List[SchemaRule]) -> Dict[str, Any]:
        """Apply schema-driven rules to workflow data."""
        processed_data = data.copy()
        
        # Sort rules by priority
        sorted_rules = sorted(rules, key=lambda r: r.priority)
        
        for rule in sorted_rules:
            if not rule.enabled:
                continue
            
            try:
                if rule.rule_type == "validation":
                    # Validation rules don't modify data, just validate
                    self._validate_schema_rule(processed_data, rule)
                elif rule.rule_type == "transformation":
                    processed_data = self._apply_transformation_rule(processed_data, rule)
                elif rule.rule_type == "filtering":
                    processed_data = self._apply_filtering_rule(processed_data, rule)
            except Exception as e:
                # Log error but continue processing
                print(f"Error applying schema rule {rule.rule_id}: {e}")
        
        return processed_data
    
    def _validate_schema_rule(self, data: Dict[str, Any], rule: SchemaRule) -> None:
        """Validate data against a schema rule."""
        # Extract value using schema path
        value = self._extract_value_by_path(data, rule.schema_path)
        
        # Apply validation rule
        if not self._evaluate_rule_expression(value, rule.rule_expression):
            raise ValueError(f"Schema validation failed for rule {rule.rule_id}: {rule.rule_expression}")
    
    def _apply_transformation_rule(self, data: Dict[str, Any], rule: SchemaRule) -> Dict[str, Any]:
        """Apply a transformation rule to data."""
        # Simple transformation - can be enhanced with more sophisticated logic
        transformed_data = data.copy()
        
        # Extract current value
        current_value = self._extract_value_by_path(transformed_data, rule.schema_path)
        
        # Apply transformation based on rule expression
        if rule.rule_expression.startswith("uppercase"):
            if isinstance(current_value, str):
                self._set_value_by_path(transformed_data, rule.schema_path, current_value.upper())
        elif rule.rule_expression.startswith("lowercase"):
            if isinstance(current_value, str):
                self._set_value_by_path(transformed_data, rule.schema_path, current_value.lower())
        elif rule.rule_expression.startswith("default"):
            # Set default value if current value is None or empty
            if not current_value:
                default_value = rule.rule_expression.split("default:")[1].strip()
                self._set_value_by_path(transformed_data, rule.schema_path, default_value)
        
        return transformed_data
    
    def _apply_filtering_rule(self, data: Dict[str, Any], rule: SchemaRule) -> Dict[str, Any]:
        """Apply a filtering rule to data."""
        filtered_data = data.copy()
        
        # Simple filtering based on rule expression
        if rule.rule_expression == "remove_empty":
            value = self._extract_value_by_path(filtered_data, rule.schema_path)
            if not value:
                self._remove_value_by_path(filtered_data, rule.schema_path)
        elif rule.rule_expression.startswith("remove_if"):
            condition = rule.rule_expression.split("remove_if:")[1].strip()
            value = self._extract_value_by_path(filtered_data, rule.schema_path)
            if self._evaluate_rule_expression(value, condition):
                self._remove_value_by_path(filtered_data, rule.schema_path)
        
        return filtered_data
    
    def _extract_value_by_path(self, data: Dict[str, Any], path: str) -> Any:
        """Extract value from data using a path notation."""
        if not path or path == "$":
            return data
        
        # Simple dot notation support
        parts = path.replace("$.", "").split(".")
        current = data
        
        for part in parts:
            if isinstance(current, dict) and part in current:
                current = current[part]
            elif isinstance(current, list) and part.isdigit():
                index = int(part)
                if 0 <= index < len(current):
                    current = current[index]
                else:
                    return None
            else:
                return None
        
        return current
    
    def _set_value_by_path(self, data: Dict[str, Any], path: str, value: Any) -> None:
        """Set value in data using a path notation."""
        if not path or path == "$":
            return
        
        parts = path.replace("$.", "").split(".")
        current = data
        
        for i, part in enumerate(parts[:-1]):
            if isinstance(current, dict):
                if part not in current:
                    current[part] = {}
                current = current[part]
            else:
                return
        
        # Set the final value
        final_key = parts[-1]
        if isinstance(current, dict):
            current[final_key] = value
    
    def _remove_value_by_path(self, data: Dict[str, Any], path: str) -> None:
        """Remove value from data using a path notation."""
        if not path or path == "$":
            return
        
        parts = path.replace("$.", "").split(".")
        current = data
        
        for part in parts[:-1]:
            if isinstance(current, dict) and part in current:
                current = current[part]
            else:
                return
        
        # Remove the final value
        final_key = parts[-1]
        if isinstance(current, dict) and final_key in current:
            del current[final_key]
    
    def _evaluate_rule_expression(self, value: Any, expression: str) -> bool:
        """Evaluate a rule expression against a value."""
        # Simple expression evaluation - enhance with proper expression engine
        if expression == "not_empty":
            return value is not None and value != ""
        elif expression == "is_string":
            return isinstance(value, str)
        elif expression == "is_number":
            return isinstance(value, (int, float))
        elif expression.startswith("equals:"):
            expected = expression.split("equals:")[1].strip()
            return str(value) == expected
        elif expression.startswith("contains:"):
            search_term = expression.split("contains:")[1].strip()
            return search_term in str(value)
        
        return True  # Default to True for unknown expressions


class DomainContextManager:
    """
    Manager for domain context creation and management with schema-driven rules and entity filtering.
    
    Provides comprehensive context injection capabilities with configurable entity filtering
    and schema-driven rule application.
    """
    
    def __init__(self):
        """Initialize the domain context manager."""
        self.contexts: Dict[str, DomainContext] = {}
        self.injector = DomainContextInjector()
        self.context_cache: Dict[str, Dict[str, Any]] = {}
    
    def create_context(self, domain_name: str, **kwargs) -> DomainContext:
        """Create a new domain context."""
        context = DomainContext(domain_name=domain_name, **kwargs)
        self.contexts[domain_name] = context
        return context
    
    def get_context(self, domain_name: str) -> Optional[DomainContext]:
        """Get domain context by name."""
        return self.contexts.get(domain_name)
    
    def register_injection_config(self, config: ContextInjectionConfig) -> None:
        """Register a context injection configuration."""
        self.injector.register_injection_config(config)
    
    def register_entity_schema(self, entity_type: str, schema: Dict[str, Any]) -> None:
        """Register an entity schema for validation and filtering."""
        self.injector.register_entity_schema(entity_type, schema)
    
    async def inject_context_into_workflow(self, workflow_data: Dict[str, Any], domain_name: str) -> Dict[str, Any]:
        """Inject domain context into workflow data."""
        context = self.get_context(domain_name)
        if not context:
            return workflow_data
        
        return await self.injector.inject_context(workflow_data, domain_name, context)
    
    def create_entity_filter(self, entity_type: str, filter_expression: str, **kwargs) -> EntityFilter:
        """Create an entity filter configuration."""
        return EntityFilter(
            entity_type=entity_type,
            filter_expression=filter_expression,
            **kwargs
        )
    
    def create_schema_rule(self, schema_path: str, rule_type: str, rule_expression: str, **kwargs) -> SchemaRule:
        """Create a schema rule configuration."""
        return SchemaRule(
            schema_path=schema_path,
            rule_type=rule_type,
            rule_expression=rule_expression,
            **kwargs
        )
    
    def create_injection_config(self, domain_name: str, entity_filters: Optional[List[EntityFilter]] = None, schema_rules: Optional[List[SchemaRule]] = None, **kwargs) -> ContextInjectionConfig:
        """Create a context injection configuration."""
        return ContextInjectionConfig(
            domain_name=domain_name,
            entity_filters=entity_filters or [],
            schema_rules=schema_rules or [],
            **kwargs
        )
    
    async def validate_context_integrity(self, domain_name: str) -> Dict[str, Any]:
        """Validate the integrity of a domain context."""
        context = self.get_context(domain_name)
        if not context:
            return {"valid": False, "error": "Context not found"}
        
        validation_result = {
            "valid": True,
            "domain_name": domain_name,
            "rules_count": len(context.applicable_rules),
            "constraints_count": len(context.schema_constraints),
            "policies_count": len(context.organizational_policies),
            "requirements_count": len(context.regulatory_requirements),
            "issues": []
        }
        
        # Check for rule conflicts
        rule_names = [rule.name for rule in context.applicable_rules]
        duplicate_names = [name for name in rule_names if rule_names.count(name) > 1]
        if duplicate_names:
            validation_result["issues"].append(f"Duplicate rule names: {duplicate_names}")
        
        # Check for constraint conflicts
        constraint_paths = [constraint.schema_path for constraint in context.schema_constraints]
        duplicate_paths = [path for path in constraint_paths if constraint_paths.count(path) > 1]
        if duplicate_paths:
            validation_result["issues"].append(f"Duplicate constraint paths: {duplicate_paths}")
        
        # Check for policy conflicts
        mandatory_policies = [p for p in context.organizational_policies if p.enforcement_level == "mandatory"]
        optional_policies = [p for p in context.organizational_policies if p.enforcement_level == "optional"]
        
        for mandatory in mandatory_policies:
            for optional in optional_policies:
                if mandatory.name == optional.name:
                    validation_result["issues"].append(f"Policy enforcement conflict: {mandatory.name}")
        
        validation_result["valid"] = len(validation_result["issues"]) == 0
        return validation_result


================================================
FILE: dataqa/orchestration/domain/knowledge.py
================================================
"""
Domain knowledge management for multi-agent orchestration.
"""

import json
import yaml
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Union
from uuid import uuid4

from pydantic import BaseModel, Field

from ..models import (
    BusinessRule, 
    DomainContext, 
    Policy, 
    RegulatoryRequirement, 
    SchemaConstraint
)


class KnowledgeSource(BaseModel):
    """Configuration for a knowledge source."""
    source_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    source_type: str  # "file", "database", "api", "memory"
    location: str  # Path, URL, or connection string
    format: str = "yaml"  # "yaml", "json", "xml", "database"
    refresh_interval_minutes: int = 60
    enabled: bool = True
    metadata: Dict[str, Any] = Field(default_factory=dict)


class RuleMapping(BaseModel):
    """Mapping configuration for rules to domains."""
    mapping_id: str = Field(default_factory=lambda: str(uuid4()))
    source_rule_id: str
    target_domain: str
    priority: int = 1
    conditions: Dict[str, Any] = Field(default_factory=dict)
    transformations: Dict[str, Any] = Field(default_factory=dict)
    enabled: bool = True


class KnowledgeVersion(BaseModel):
    """Version information for knowledge artifacts."""
    version_id: str = Field(default_factory=lambda: str(uuid4()))
    domain_name: str
    version_number: str
    created_at: datetime = Field(default_factory=datetime.utcnow)
    created_by: str
    description: str
    changes: List[str] = Field(default_factory=list)
    is_active: bool = False
    metadata: Dict[str, Any] = Field(default_factory=dict)


class DomainKnowledgeManager:
    """
    Manager for domain-specific knowledge and business rules with configurable sources.
    
    Supports multiple knowledge sources, rule mappings, and versioning for evolving
    business rules and compliance requirements.
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialize the domain knowledge manager."""
        self.knowledge_sources: Dict[str, KnowledgeSource] = {}
        self.rule_mappings: Dict[str, List[RuleMapping]] = {}
        self.knowledge_versions: Dict[str, List[KnowledgeVersion]] = {}
        self.domain_contexts: Dict[str, DomainContext] = {}
        self.cached_knowledge: Dict[str, Dict[str, Any]] = {}
        
        if config_path:
            self._load_configuration(config_path)
    
    def _load_configuration(self, config_path: str) -> None:
        """Load knowledge manager configuration from file."""
        config_file = Path(config_path)
        if not config_file.exists():
            raise FileNotFoundError(f"Configuration file not found: {config_path}")
        
        with open(config_file, 'r') as f:
            if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                config = yaml.safe_load(f)
            else:
                config = json.load(f)
        
        # Load knowledge sources
        for source_config in config.get('knowledge_sources', []):
            source = KnowledgeSource(**source_config)
            self.knowledge_sources[source.source_id] = source
        
        # Load rule mappings
        for mapping_config in config.get('rule_mappings', []):
            mapping = RuleMapping(**mapping_config)
            domain = mapping.target_domain
            if domain not in self.rule_mappings:
                self.rule_mappings[domain] = []
            self.rule_mappings[domain].append(mapping)
    
    async def register_knowledge_source(self, source: KnowledgeSource) -> None:
        """Register a new knowledge source."""
        self.knowledge_sources[source.source_id] = source
        await self._refresh_knowledge_source(source.source_id)
    
    async def _refresh_knowledge_source(self, source_id: str) -> None:
        """Refresh knowledge from a specific source."""
        source = self.knowledge_sources.get(source_id)
        if not source or not source.enabled:
            return
        
        try:
            if source.source_type == "file":
                await self._load_from_file(source)
            elif source.source_type == "memory":
                # Memory sources are managed directly
                pass
            # Add other source types as needed
        except Exception as e:
            # Log error but don't fail completely
            print(f"Error refreshing knowledge source {source_id}: {e}")
    
    async def _load_from_file(self, source: KnowledgeSource) -> None:
        """Load knowledge from a file source."""
        file_path = Path(source.location)
        if not file_path.exists():
            return
        
        try:
            with open(file_path, 'r') as f:
                if source.format == "yaml":
                    data = yaml.safe_load(f)
                elif source.format == "json":
                    data = json.load(f)
                else:
                    raise ValueError(f"Unsupported format: {source.format}")
            
            # Cache the loaded knowledge
            self.cached_knowledge[source.source_id] = data
        except Exception as e:
            # Log error but don't fail completely
            print(f"Error loading from file {source.location}: {e}")
            if source.format not in ["yaml", "json"]:
                raise ValueError(f"Unsupported format: {source.format}")
    
    async def load_domain_context(self, domain_name: str) -> Optional[DomainContext]:
        """Load domain context by name with all applicable rules and constraints."""
        if domain_name in self.domain_contexts:
            return self.domain_contexts[domain_name]
        
        # Collect rules from all sources
        business_rules = await self._collect_business_rules(domain_name)
        schema_constraints = await self._collect_schema_constraints(domain_name)
        regulatory_requirements = await self._collect_regulatory_requirements(domain_name)
        organizational_policies = await self._collect_organizational_policies(domain_name)
        
        context = DomainContext(
            domain_name=domain_name,
            applicable_rules=business_rules,
            schema_constraints=schema_constraints,
            regulatory_requirements=regulatory_requirements,
            organizational_policies=organizational_policies
        )
        
        self.domain_contexts[domain_name] = context
        return context
    
    async def _collect_business_rules(self, domain_name: str) -> List[BusinessRule]:
        """Collect business rules applicable to a domain."""
        rules = []
        
        # Check rule mappings for this domain
        mappings = self.rule_mappings.get(domain_name, [])
        
        for source_id, source in self.knowledge_sources.items():
            if not source.enabled:
                continue
            
            knowledge = self.cached_knowledge.get(source_id, {})
            source_rules = knowledge.get('business_rules', [])
            
            for rule_data in source_rules:
                # Apply domain filtering
                if self._rule_applies_to_domain(rule_data, domain_name, mappings):
                    rule = BusinessRule(**rule_data)
                    rules.append(rule)
        
        return rules
    
    async def _collect_schema_constraints(self, domain_name: str) -> List[SchemaConstraint]:
        """Collect schema constraints applicable to a domain."""
        constraints = []
        
        for source_id, source in self.knowledge_sources.items():
            if not source.enabled:
                continue
            
            knowledge = self.cached_knowledge.get(source_id, {})
            source_constraints = knowledge.get('schema_constraints', [])
            
            for constraint_data in source_constraints:
                if self._constraint_applies_to_domain(constraint_data, domain_name):
                    constraint = SchemaConstraint(**constraint_data)
                    constraints.append(constraint)
        
        return constraints
    
    async def _collect_regulatory_requirements(self, domain_name: str) -> List[RegulatoryRequirement]:
        """Collect regulatory requirements applicable to a domain."""
        requirements = []
        
        for source_id, source in self.knowledge_sources.items():
            if not source.enabled:
                continue
            
            knowledge = self.cached_knowledge.get(source_id, {})
            source_requirements = knowledge.get('regulatory_requirements', [])
            
            for req_data in source_requirements:
                if domain_name in req_data.get('applicable_domains', []):
                    requirement = RegulatoryRequirement(**req_data)
                    requirements.append(requirement)
        
        return requirements
    
    async def _collect_organizational_policies(self, domain_name: str) -> List[Policy]:
        """Collect organizational policies applicable to a domain."""
        policies = []
        
        for source_id, source in self.knowledge_sources.items():
            if not source.enabled:
                continue
            
            knowledge = self.cached_knowledge.get(source_id, {})
            source_policies = knowledge.get('organizational_policies', [])
            
            for policy_data in source_policies:
                # Policies may apply to all domains or specific ones
                policy = Policy(**policy_data)
                policies.append(policy)
        
        return policies
    
    def _rule_applies_to_domain(self, rule_data: Dict[str, Any], domain_name: str, mappings: List[RuleMapping]) -> bool:
        """Check if a rule applies to a specific domain."""
        # Check direct domain specification
        if rule_data.get('domain') == domain_name:
            return True
        
        # Check rule mappings
        rule_id = rule_data.get('rule_id')
        for mapping in mappings:
            if mapping.source_rule_id == rule_id and mapping.enabled and mapping.target_domain == domain_name:
                # Apply mapping conditions if any
                if self._evaluate_mapping_conditions(rule_data, mapping.conditions):
                    return True
        
        return False
    
    def _constraint_applies_to_domain(self, constraint_data: Dict[str, Any], domain_name: str) -> bool:
        """Check if a constraint applies to a specific domain."""
        # Simple domain matching - can be enhanced with more complex logic
        applicable_domains = constraint_data.get('applicable_domains', [])
        return not applicable_domains or domain_name in applicable_domains
    
    def _evaluate_mapping_conditions(self, rule_data: Dict[str, Any], conditions: Dict[str, Any]) -> bool:
        """Evaluate mapping conditions for rule applicability."""
        if not conditions:
            return True
        
        # Simple condition evaluation - can be enhanced with expression engine
        for key, expected_value in conditions.items():
            if rule_data.get(key) != expected_value:
                return False
        
        return True
    
    async def update_knowledge(self, domain_name: str, knowledge: Dict[str, Any], version_info: Optional[Dict[str, Any]] = None) -> str:
        """Update domain knowledge and create a new version."""
        # Create version record
        version = KnowledgeVersion(
            domain_name=domain_name,
            version_number=version_info.get('version_number', '1.0.0') if version_info else '1.0.0',
            created_by=version_info.get('created_by', 'system') if version_info else 'system',
            description=version_info.get('description', 'Knowledge update') if version_info else 'Knowledge update',
            changes=version_info.get('changes', []) if version_info else []
        )
        
        # Store version
        if domain_name not in self.knowledge_versions:
            self.knowledge_versions[domain_name] = []
        
        # Deactivate previous versions
        for prev_version in self.knowledge_versions[domain_name]:
            prev_version.is_active = False
        
        version.is_active = True
        self.knowledge_versions[domain_name].append(version)
        
        # Update cached knowledge
        memory_source_id = f"memory_{domain_name}"
        if memory_source_id not in self.knowledge_sources:
            source = KnowledgeSource(
                source_id=memory_source_id,
                name=f"Memory source for {domain_name}",
                source_type="memory",
                location="memory",
                format="dict"
            )
            self.knowledge_sources[memory_source_id] = source
        
        self.cached_knowledge[memory_source_id] = knowledge
        
        # Invalidate cached domain context
        if domain_name in self.domain_contexts:
            del self.domain_contexts[domain_name]
        
        return version.version_id
    
    async def get_knowledge_versions(self, domain_name: str) -> List[KnowledgeVersion]:
        """Get all versions for a domain."""
        return self.knowledge_versions.get(domain_name, [])
    
    async def get_active_version(self, domain_name: str) -> Optional[KnowledgeVersion]:
        """Get the active version for a domain."""
        versions = self.knowledge_versions.get(domain_name, [])
        for version in versions:
            if version.is_active:
                return version
        return None
    
    async def refresh_all_sources(self) -> None:
        """Refresh knowledge from all enabled sources."""
        for source_id in self.knowledge_sources:
            await self._refresh_knowledge_source(source_id)
        
        # Clear cached domain contexts to force reload
        self.domain_contexts.clear()
    
    async def get_domain_names(self) -> Set[str]:
        """Get all available domain names."""
        domains = set()
        
        # From rule mappings
        domains.update(self.rule_mappings.keys())
        
        # From cached knowledge
        for knowledge in self.cached_knowledge.values():
            for rule in knowledge.get('business_rules', []):
                if rule.get('domain'):
                    domains.add(rule['domain'])
        
        return domains


================================================
FILE: dataqa/orchestration/domain/rules.py
================================================
"""
Business rules engine for domain-specific validation and compliance.
"""

import re
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Union
from uuid import uuid4

from pydantic import BaseModel, Field

from ..models import BusinessRule, DomainContext, Policy, RegulatoryRequirement, SchemaConstraint


class ValidationResult(BaseModel):
    """Result of business rule validation."""
    is_valid: bool
    violations: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    applied_rules: List[str] = Field(default_factory=list)
    compliance_score: float = 1.0


class ComplianceReport(BaseModel):
    """Comprehensive compliance report."""
    report_id: str = Field(default_factory=lambda: str(uuid4()))
    domain_name: str
    generated_at: datetime = Field(default_factory=datetime.utcnow)
    overall_compliance: bool
    compliance_score: float
    rule_results: List[ValidationResult] = Field(default_factory=list)
    recommendations: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class RuleValidator(ABC):
    """Abstract base class for rule validators."""
    
    @abstractmethod
    async def validate(self, action: Dict[str, Any], rule: BusinessRule, context: DomainContext) -> ValidationResult:
        """Validate an action against a specific rule."""
        pass
    
    @abstractmethod
    def supports_rule_type(self, rule_type: str) -> bool:
        """Check if this validator supports a specific rule type."""
        pass


class ExpressionRuleValidator(RuleValidator):
    """Validator for expression-based rules."""
    
    def supports_rule_type(self, rule_type: str) -> bool:
        """Support expression and condition rule types."""
        return rule_type in ["expression", "condition", "constraint"]
    
    async def validate(self, action: Dict[str, Any], rule: BusinessRule, context: DomainContext) -> ValidationResult:
        """Validate using expression evaluation."""
        try:
            # Simple expression evaluation - can be enhanced with safer evaluation
            result = self._evaluate_expression(rule.condition, action, context)
            
            if result:
                return ValidationResult(
                    is_valid=True,
                    applied_rules=[rule.rule_id],
                    metadata={"rule_type": rule.rule_type, "expression": rule.condition}
                )
            else:
                return ValidationResult(
                    is_valid=False,
                    violations=[f"Rule violation: {rule.name} - {rule.description}"],
                    applied_rules=[rule.rule_id],
                    compliance_score=0.0,
                    metadata={"rule_type": rule.rule_type, "expression": rule.condition}
                )
        except Exception as e:
            return ValidationResult(
                is_valid=False,
                violations=[f"Rule evaluation error: {rule.name} - {str(e)}"],
                applied_rules=[rule.rule_id],
                compliance_score=0.0,
                metadata={"rule_type": rule.rule_type, "error": str(e)}
            )
    
    def _evaluate_expression(self, expression: str, action: Dict[str, Any], context: DomainContext) -> bool:
        """Evaluate a rule expression safely."""
        # Simple pattern matching for common expressions
        # In production, use a proper expression engine
        
        # Check for field existence
        if "has_field" in expression:
            field_match = re.search(r"has_field\('([^']+)'\)", expression)
            if field_match:
                field_name = field_match.group(1)
                return field_name in action
        
        # Check for field values
        if "field_equals" in expression:
            equals_match = re.search(r"field_equals\('([^']+)',\s*'([^']+)'\)", expression)
            if equals_match:
                field_name, expected_value = equals_match.groups()
                return action.get(field_name) == expected_value
        
        # Check for numeric comparisons
        if "field_greater_than" in expression:
            gt_match = re.search(r"field_greater_than\('([^']+)',\s*(\d+(?:\.\d+)?)\)", expression)
            if gt_match:
                field_name, threshold = gt_match.groups()
                field_value = action.get(field_name)
                if isinstance(field_value, (int, float)):
                    return field_value > float(threshold)
        
        # Default to True for unknown expressions (safe default)
        return True


class PolicyRuleValidator(RuleValidator):
    """Validator for organizational policy rules."""
    
    def supports_rule_type(self, rule_type: str) -> bool:
        """Support policy rule types."""
        return rule_type in ["policy", "organizational_policy"]
    
    async def validate(self, action: Dict[str, Any], rule: BusinessRule, context: DomainContext) -> ValidationResult:
        """Validate against organizational policies."""
        # Find matching policy in context
        matching_policy = None
        for policy in context.organizational_policies:
            if policy.name == rule.name or policy.policy_id == rule.rule_id:
                matching_policy = policy
                break
        
        if not matching_policy:
            return ValidationResult(
                is_valid=True,
                warnings=[f"Policy not found: {rule.name}"],
                applied_rules=[rule.rule_id]
            )
        
        # Check enforcement level
        if matching_policy.enforcement_level == "optional":
            return ValidationResult(
                is_valid=True,
                applied_rules=[rule.rule_id],
                metadata={"enforcement_level": "optional"}
            )
        
        # Apply policy validation logic
        is_valid = self._validate_policy_compliance(action, matching_policy)
        
        if is_valid:
            return ValidationResult(
                is_valid=True,
                applied_rules=[rule.rule_id],
                metadata={"enforcement_level": matching_policy.enforcement_level}
            )
        else:
            # Handle policy violations based on enforcement level
            if matching_policy.enforcement_level == "mandatory":
                return ValidationResult(
                    is_valid=False,
                    violations=[f"Policy violation: {matching_policy.name}"],
                    applied_rules=[rule.rule_id],
                    compliance_score=0.0,
                    metadata={"enforcement_level": matching_policy.enforcement_level}
                )
            elif matching_policy.enforcement_level == "recommended":
                return ValidationResult(
                    is_valid=True,  # Recommended policies don't fail validation
                    warnings=[f"Policy recommendation: {matching_policy.name}"],
                    applied_rules=[rule.rule_id],
                    compliance_score=0.5,
                    metadata={"enforcement_level": matching_policy.enforcement_level}
                )
            else:  # optional
                return ValidationResult(
                    is_valid=True,
                    applied_rules=[rule.rule_id],
                    compliance_score=1.0,
                    metadata={"enforcement_level": matching_policy.enforcement_level}
                )
    
    def _validate_policy_compliance(self, action: Dict[str, Any], policy: Policy) -> bool:
        """Validate compliance with a specific policy."""
        # Simple policy validation - can be enhanced with more sophisticated logic
        action_type = action.get("type", "")
        
        # Check if action type is in exceptions
        if action_type in policy.exceptions:
            return True
        
        # Apply basic policy checks based on policy text
        policy_text_lower = policy.policy_text.lower()
        
        if "approval_required" in policy_text_lower or "approval" in policy_text_lower:
            return action.get("has_approval", False)
        
        if "permission" in policy_text_lower and "data access" in policy_text_lower:
            return action.get("has_approval", False)
        
        if "data_access" in policy_text_lower:
            return action.get("data_access_level", "none") in ["read", "limited"]
        
        # Default to compliant
        return True


class ConstraintValidator:
    """Validator for schema constraints."""
    
    async def validate_constraints(self, action: Dict[str, Any], constraints: List[SchemaConstraint]) -> ValidationResult:
        """Validate action against schema constraints."""
        violations = []
        warnings = []
        applied_constraints = []
        
        for constraint in constraints:
            applied_constraints.append(constraint.constraint_id)
            
            try:
                is_valid = self._validate_constraint(action, constraint)
                if not is_valid:
                    if constraint.severity == "error":
                        violations.append(constraint.error_message)
                    elif constraint.severity == "warning":
                        warnings.append(constraint.error_message)
            except Exception as e:
                violations.append(f"Constraint validation error: {constraint.name} - {str(e)}")
        
        return ValidationResult(
            is_valid=len(violations) == 0,
            violations=violations,
            warnings=warnings,
            applied_rules=applied_constraints,
            compliance_score=1.0 if len(violations) == 0 else max(0.0, 1.0 - len(violations) / len(constraints))
        )
    
    def _validate_constraint(self, action: Dict[str, Any], constraint: SchemaConstraint) -> bool:
        """Validate a single constraint."""
        # Extract value using schema path (simplified JSONPath)
        value = self._extract_value(action, constraint.schema_path)
        
        if constraint.constraint_type == "required":
            return value is not None
        elif constraint.constraint_type == "type":
            expected_type = constraint.constraint_value
            if expected_type == "string":
                return isinstance(value, str)
            elif expected_type == "number":
                return isinstance(value, (int, float))
            elif expected_type == "boolean":
                return isinstance(value, bool)
            elif expected_type == "array":
                return isinstance(value, list)
            elif expected_type == "object":
                return isinstance(value, dict)
        elif constraint.constraint_type == "range":
            if isinstance(value, (int, float)) and isinstance(constraint.constraint_value, dict):
                min_val = constraint.constraint_value.get("min")
                max_val = constraint.constraint_value.get("max")
                if min_val is not None and value < min_val:
                    return False
                if max_val is not None and value > max_val:
                    return False
                return True
        elif constraint.constraint_type == "pattern":
            if isinstance(value, str):
                pattern = constraint.constraint_value
                return bool(re.match(pattern, value))
        
        return True
    
    def _extract_value(self, data: Dict[str, Any], path: str) -> Any:
        """Extract value from data using a simple path notation."""
        if not path or path == "$":
            return data
        
        # Simple dot notation support
        parts = path.replace("$.", "").split(".")
        current = data
        
        for part in parts:
            if isinstance(current, dict) and part in current:
                current = current[part]
            else:
                return None
        
        return current


class BusinessRulesEngine:
    """
    Engine for applying business rules and domain constraints with pluggable rule sets.
    
    Supports multiple rule validators, constraint validation, and comprehensive
    compliance reporting.
    """
    
    def __init__(self):
        """Initialize the business rules engine."""
        self.rule_validators: List[RuleValidator] = []
        self.constraint_validator = ConstraintValidator()
        self.rule_cache: Dict[str, List[BusinessRule]] = {}
        
        # Register default validators
        self._register_default_validators()
    
    def _register_default_validators(self) -> None:
        """Register default rule validators."""
        self.rule_validators.extend([
            ExpressionRuleValidator(),
            PolicyRuleValidator()
        ])
    
    def register_validator(self, validator: RuleValidator) -> None:
        """Register a custom rule validator."""
        self.rule_validators.append(validator)
    
    async def validate_action(self, action: Dict[str, Any], context: DomainContext) -> ValidationResult:
        """Validate an action against all applicable business rules."""
        all_violations = []
        all_warnings = []
        all_applied_rules = []
        total_score = 0.0
        rule_count = 0
        
        # Validate business rules
        for rule in context.applicable_rules:
            if not rule.enabled:
                continue
            
            validator = self._find_validator(rule.rule_type)
            if validator:
                result = await validator.validate(action, rule, context)
                all_violations.extend(result.violations)
                all_warnings.extend(result.warnings)
                all_applied_rules.extend(result.applied_rules)
                total_score += result.compliance_score
                rule_count += 1
        
        # Validate schema constraints
        if context.schema_constraints:
            constraint_result = await self.constraint_validator.validate_constraints(
                action, context.schema_constraints
            )
            all_violations.extend(constraint_result.violations)
            all_warnings.extend(constraint_result.warnings)
            all_applied_rules.extend(constraint_result.applied_rules)
            total_score += constraint_result.compliance_score
            rule_count += 1
        
        # Calculate overall compliance score
        overall_score = total_score / rule_count if rule_count > 0 else 1.0
        
        return ValidationResult(
            is_valid=len(all_violations) == 0,
            violations=all_violations,
            warnings=all_warnings,
            applied_rules=all_applied_rules,
            compliance_score=overall_score
        )
    
    def _find_validator(self, rule_type: str) -> Optional[RuleValidator]:
        """Find a validator that supports the given rule type."""
        for validator in self.rule_validators:
            if validator.supports_rule_type(rule_type):
                return validator
        return None
    
    async def inject_constraints(self, plan: Dict[str, Any], context: DomainContext) -> Dict[str, Any]:
        """Inject domain constraints into an execution plan."""
        constrained_plan = plan.copy()
        
        # Add constraint metadata
        constrained_plan["domain_constraints"] = {
            "domain_name": context.domain_name,
            "business_rules_count": len(context.applicable_rules),
            "schema_constraints_count": len(context.schema_constraints),
            "regulatory_requirements_count": len(context.regulatory_requirements),
            "organizational_policies_count": len(context.organizational_policies)
        }
        
        # Add validation checkpoints
        if "steps" in constrained_plan:
            for step in constrained_plan["steps"]:
                step["validation_required"] = True
                step["domain_context"] = context.domain_name
        
        # Add compliance requirements
        constrained_plan["compliance_requirements"] = [
            {
                "requirement_id": req.requirement_id,
                "regulation_name": req.regulation_name,
                "severity": req.severity
            }
            for req in context.regulatory_requirements
        ]
        
        return constrained_plan
    
    async def check_compliance(self, execution_result: Dict[str, Any], context: DomainContext) -> ComplianceReport:
        """Generate comprehensive compliance report for execution results."""
        report = ComplianceReport(
            domain_name=context.domain_name,
            overall_compliance=True,
            compliance_score=1.0
        )
        
        # Validate each action in the execution result
        actions = execution_result.get("actions", [execution_result])
        total_score = 0.0
        
        for action in actions:
            validation_result = await self.validate_action(action, context)
            report.rule_results.append(validation_result)
            total_score += validation_result.compliance_score
            
            if not validation_result.is_valid:
                report.overall_compliance = False
        
        # Calculate overall compliance score
        report.compliance_score = total_score / len(actions) if actions else 1.0
        
        # Generate recommendations
        report.recommendations = self._generate_recommendations(report.rule_results, context)
        
        return report
    
    def _generate_recommendations(self, results: List[ValidationResult], context: DomainContext) -> List[str]:
        """Generate recommendations based on validation results."""
        recommendations = []
        
        # Analyze common violation patterns
        violation_patterns = {}
        for result in results:
            for violation in result.violations:
                violation_patterns[violation] = violation_patterns.get(violation, 0) + 1
        
        # Generate recommendations for frequent violations
        for violation, count in violation_patterns.items():
            if count > 1:
                recommendations.append(f"Address recurring violation: {violation} (occurred {count} times)")
        
        # Check for missing approvals
        approval_violations = [v for v in violation_patterns.keys() if "approval" in v.lower() or "policy violation" in v.lower()]
        if approval_violations:
            recommendations.append("Consider implementing automated approval workflows for sensitive operations")
        
        # Check for data access violations
        data_violations = [v for v in violation_patterns.keys() if "data" in v.lower()]
        if data_violations:
            recommendations.append("Review data access policies and implement stricter access controls")
        
        return recommendations
    
    async def load_rules(self, domain_name: str, rules: List[BusinessRule]) -> None:
        """Load business rules for a domain."""
        self.rule_cache[domain_name] = rules
    
    async def get_applicable_rules(self, domain_name: str, action_type: Optional[str] = None) -> List[BusinessRule]:
        """Get applicable rules for a domain and optional action type."""
        domain_rules = self.rule_cache.get(domain_name, [])
        
        # Filter out disabled rules first
        enabled_rules = [rule for rule in domain_rules if rule.enabled]
        
        if not action_type:
            return enabled_rules
        
        # Filter by action type if specified
        applicable_rules = []
        for rule in enabled_rules:
            if not rule.metadata.get("action_types") or action_type in rule.metadata.get("action_types", []):
                applicable_rules.append(rule)
        
        return applicable_rules


================================================
FILE: dataqa/orchestration/evaluation/__init__.py
================================================
"""
Comprehensive evaluation and benchmarking components for multi-agent systems.
"""

from .benchmark import (
    BenchmarkFramework,
    BenchmarkSuite,
    TestCase,
    GroundTruth,
    EvaluationCriterion,
    PerformanceBaseline,
    QualityThreshold,
    BenchmarkResult,
    BenchmarkReport,
    BenchmarkConfiguration
)
from .judge import (
    LLMJudgeEvaluator,
    ScoringRubric,
    EvaluationResult,
    AgentResponse,
    LLMModel
)
from .analytics import (
    PerformanceAnalytics,
    PerformanceMetrics,
    MetricsSnapshot,
    TrendAnalysis,
    Optimization,
    PerformanceData,
    MetricsCollector,
    TrendAnalyzer,
    OptimizationRecommender
)

__all__ = [
    # Benchmark Framework
    "BenchmarkFramework",
    "BenchmarkSuite",
    "TestCase",
    "GroundTruth",
    "EvaluationCriterion",
    "PerformanceBaseline",
    "QualityThreshold",
    "BenchmarkResult",
    "BenchmarkReport",
    "BenchmarkConfiguration",
    
    # LLM Judge Evaluator
    "LLMJudgeEvaluator",
    "ScoringRubric",
    "EvaluationResult",
    "AgentResponse",
    "LLMModel",
    
    # Performance Analytics
    "PerformanceAnalytics",
    "PerformanceMetrics",
    "MetricsSnapshot",
    "TrendAnalysis",
    "Optimization",
    "PerformanceData",
    "MetricsCollector",
    "TrendAnalyzer",
    "OptimizationRecommender",
]


================================================
FILE: dataqa/orchestration/evaluation/analytics.py
================================================
"""
Advanced performance analytics for multi-agent system monitoring and optimization.
"""

import statistics
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Callable, Union
from uuid import uuid4
from collections import defaultdict, deque

from pydantic import BaseModel, Field

from ...logging_config import get_primitive_logger
from ...exceptions import DataQAError
from ..models import ExecutionSession, ExecutionMetrics, AgentConfiguration


class MetricsCollector(BaseModel):
    """Configuration for metrics collection."""
    collector_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    metric_type: str  # "performance", "quality", "resource", "business"
    collection_interval_seconds: int = 30
    aggregation_method: str = "average"  # average, sum, max, min, percentile
    query_pattern: Optional[str] = None
    enabled: bool = True
    metadata: Dict[str, Any] = Field(default_factory=dict)


class TrendAnalyzer(BaseModel):
    """Configuration for trend analysis."""
    analyzer_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    metric_names: List[str] = Field(default_factory=list)
    analysis_window_hours: int = 24
    trend_detection_method: str = "linear_regression"  # linear_regression, moving_average, seasonal
    threshold_config: Dict[str, float] = Field(default_factory=dict)
    alert_conditions: List[str] = Field(default_factory=list)
    enabled: bool = True
    metadata: Dict[str, Any] = Field(default_factory=dict)


class OptimizationRecommender(BaseModel):
    """Configuration for optimization recommendations."""
    recommender_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    target_metrics: List[str] = Field(default_factory=list)
    optimization_strategy: str = "performance"  # performance, cost, quality, balanced
    recommendation_rules: List[str] = Field(default_factory=list)
    confidence_threshold: float = 0.7
    enabled: bool = True
    metadata: Dict[str, Any] = Field(default_factory=dict)


class PerformanceMetrics(BaseModel):
    """Comprehensive performance metrics for agent execution."""
    metrics_id: str = Field(default_factory=lambda: str(uuid4()))
    session_id: str
    agent_configuration_id: str
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    
    # Core performance metrics
    execution_time_seconds: float
    success_rate: float
    throughput: float
    error_rate: float = 0.0
    
    # Resource utilization metrics
    resource_utilization: Dict[str, float] = Field(default_factory=dict)
    
    # Quality metrics
    quality_scores: Dict[str, float] = Field(default_factory=dict)
    
    # Business metrics
    business_metrics: Dict[str, float] = Field(default_factory=dict)
    
    # Custom metrics
    custom_metrics: Dict[str, Any] = Field(default_factory=dict)
    
    # Metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)


class MetricsSnapshot(BaseModel):
    """Snapshot of metrics at a specific point in time."""
    snapshot_id: str = Field(default_factory=lambda: str(uuid4()))
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    metrics: List[PerformanceMetrics] = Field(default_factory=list)
    aggregated_metrics: Dict[str, float] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class TrendAnalysis(BaseModel):
    """Result of trend analysis."""
    analysis_id: str = Field(default_factory=lambda: str(uuid4()))
    analyzer_name: str
    metric_name: str
    analysis_period: str
    trend_direction: str  # "increasing", "decreasing", "stable", "volatile"
    trend_strength: float  # 0.0 to 1.0
    statistical_significance: float
    key_insights: List[str] = Field(default_factory=list)
    anomalies_detected: List[Dict[str, Any]] = Field(default_factory=list)
    predictions: Dict[str, float] = Field(default_factory=dict)
    confidence_level: float
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class Optimization(BaseModel):
    """Optimization recommendation."""
    optimization_id: str = Field(default_factory=lambda: str(uuid4()))
    recommender_name: str
    target_metric: str
    current_value: float
    target_value: float
    improvement_percentage: float
    recommendation_type: str  # "configuration", "resource", "algorithm", "workflow"
    specific_actions: List[str] = Field(default_factory=list)
    estimated_impact: Dict[str, float] = Field(default_factory=dict)
    implementation_complexity: str = "medium"  # low, medium, high
    confidence_score: float
    priority: str = "medium"  # low, medium, high, critical
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class PerformanceData(BaseModel):
    """Comprehensive performance data for analysis."""
    data_id: str = Field(default_factory=lambda: str(uuid4()))
    collection_period: str
    agent_configurations: List[str] = Field(default_factory=list)
    metrics_snapshots: List[MetricsSnapshot] = Field(default_factory=list)
    trend_analyses: List[TrendAnalysis] = Field(default_factory=list)
    benchmark_comparisons: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class PerformanceAnalytics:
    """
    Advanced analytics engine for performance monitoring and optimization.
    
    Supports configurable query patterns, metrics collection, trend analysis,
    and optimization recommendations for multi-agent systems.
    """
    
    def __init__(self):
        """Initialize the performance analytics engine."""
        self.logger = get_primitive_logger("analytics", "performance")
        
        # Core data storage
        self.metrics_history: List[PerformanceMetrics] = []
        self.snapshots_history: List[MetricsSnapshot] = []
        self.trend_analyses: List[TrendAnalysis] = []
        self.optimizations: List[Optimization] = []
        
        # Configuration
        self.metrics_collectors: List[MetricsCollector] = []
        self.trend_analyzers: List[TrendAnalyzer] = []
        self.optimization_recommenders: List[OptimizationRecommender] = []
        
        # Real-time data structures
        self.real_time_metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        self.alert_thresholds: Dict[str, Dict[str, float]] = {}
        
        # Initialize default configurations
        self._initialize_default_configurations()
    
    def _initialize_default_configurations(self) -> None:
        """Initialize default metrics collectors, analyzers, and recommenders."""
        
        # Default performance metrics collector
        perf_collector = MetricsCollector(
            name="Core Performance Metrics",
            description="Collects basic performance metrics for all agents",
            metric_type="performance",
            collection_interval_seconds=30,
            aggregation_method="average",
            query_pattern="execution_time_seconds,success_rate,throughput,error_rate"
        )
        self.add_metrics_collector(perf_collector)
        
        # Default quality metrics collector
        quality_collector = MetricsCollector(
            name="Quality Metrics",
            description="Collects quality scores and evaluation metrics",
            metric_type="quality",
            collection_interval_seconds=60,
            aggregation_method="average",
            query_pattern="quality_scores.*"
        )
        self.add_metrics_collector(quality_collector)
        
        # Default resource utilization collector
        resource_collector = MetricsCollector(
            name="Resource Utilization",
            description="Collects resource usage metrics",
            metric_type="resource",
            collection_interval_seconds=15,
            aggregation_method="max",
            query_pattern="resource_utilization.*"
        )
        self.add_metrics_collector(resource_collector)
        
        # Default trend analyzer
        trend_analyzer = TrendAnalyzer(
            name="Performance Trend Analysis",
            description="Analyzes performance trends over time",
            metric_names=["execution_time_seconds", "success_rate", "throughput"],
            analysis_window_hours=24,
            trend_detection_method="linear_regression",
            threshold_config={
                "significant_change": 0.1,
                "volatility_threshold": 0.2
            },
            alert_conditions=["success_rate < 0.8", "execution_time_seconds > 30.0"]
        )
        self.add_trend_analyzer(trend_analyzer)
        
        # Default optimization recommender
        optimizer = OptimizationRecommender(
            name="Performance Optimizer",
            description="Provides performance optimization recommendations",
            target_metrics=["execution_time_seconds", "success_rate", "throughput"],
            optimization_strategy="balanced",
            recommendation_rules=[
                "IF execution_time_seconds > 30 THEN recommend timeout optimization",
                "IF success_rate < 0.8 THEN recommend error handling improvement",
                "IF throughput < 5 THEN recommend parallelization"
            ],
            confidence_threshold=0.7
        )
        self.add_optimization_recommender(optimizer)
    
    def add_metrics_collector(self, collector: MetricsCollector) -> None:
        """Add a metrics collector configuration.
        
        Args:
            collector: Metrics collector to add
        """
        self.metrics_collectors.append(collector)
        self.logger.info(f"Added metrics collector: {collector.name}")
    
    def add_trend_analyzer(self, analyzer: TrendAnalyzer) -> None:
        """Add a trend analyzer configuration.
        
        Args:
            analyzer: Trend analyzer to add
        """
        self.trend_analyzers.append(analyzer)
        self.logger.info(f"Added trend analyzer: {analyzer.name}")
    
    def add_optimization_recommender(self, recommender: OptimizationRecommender) -> None:
        """Add an optimization recommender configuration.
        
        Args:
            recommender: Optimization recommender to add
        """
        self.optimization_recommenders.append(recommender)
        self.logger.info(f"Added optimization recommender: {recommender.name}")
    
    async def collect_metrics(self, execution_session: ExecutionSession) -> MetricsSnapshot:
        """Collect comprehensive performance metrics from an execution session.
        
        Args:
            execution_session: Execution session to collect metrics from
            
        Returns:
            Metrics snapshot containing collected metrics
        """
        try:
            collected_metrics: List[PerformanceMetrics] = []
            
            # Extract base metrics from execution session
            base_metrics = self._extract_base_metrics(execution_session)
            collected_metrics.append(base_metrics)
            
            # Apply configured collectors
            for collector in self.metrics_collectors:
                if not collector.enabled:
                    continue
                
                try:
                    additional_metrics = await self._apply_collector(
                        execution_session, collector
                    )
                    if additional_metrics:
                        collected_metrics.extend(additional_metrics)
                except Exception as e:
                    self.logger.warning(f"Collector {collector.name} failed: {e}")
            
            # Create metrics snapshot
            snapshot = MetricsSnapshot(
                metrics=collected_metrics,
                aggregated_metrics=self._aggregate_metrics(collected_metrics)
            )
            
            # Store in history
            self.metrics_history.extend(collected_metrics)
            self.snapshots_history.append(snapshot)
            
            # Update real-time metrics
            self._update_real_time_metrics(collected_metrics)
            
            self.logger.info(f"Collected {len(collected_metrics)} metrics from session {execution_session.session_id}")
            return snapshot
            
        except Exception as e:
            self.logger.error(f"Failed to collect metrics: {e}")
            raise DataQAError(f"Metrics collection failed: {e}")
    
    def _extract_base_metrics(self, execution_session: ExecutionSession) -> PerformanceMetrics:
        """Extract base performance metrics from execution session.
        
        Args:
            execution_session: Execution session to extract from
            
        Returns:
            Base performance metrics
        """
        exec_metrics = execution_session.execution_state.execution_metrics
        
        # Calculate derived metrics
        total_steps = exec_metrics.total_steps or 1
        success_rate = (exec_metrics.completed_steps / total_steps) if total_steps > 0 else 0.0
        error_rate = (exec_metrics.failed_steps / total_steps) if total_steps > 0 else 0.0
        throughput = (exec_metrics.completed_steps / exec_metrics.total_execution_time_seconds) if exec_metrics.total_execution_time_seconds > 0 else 0.0
        
        return PerformanceMetrics(
            session_id=execution_session.session_id,
            agent_configuration_id=execution_session.workflow_id,  # Using workflow_id as proxy
            execution_time_seconds=exec_metrics.total_execution_time_seconds,
            success_rate=success_rate,
            throughput=throughput,
            error_rate=error_rate,
            resource_utilization=exec_metrics.resource_utilization.copy(),
            quality_scores=exec_metrics.quality_scores.copy()
        )
    
    async def _apply_collector(
        self,
        execution_session: ExecutionSession,
        collector: MetricsCollector
    ) -> List[PerformanceMetrics]:
        """Apply a specific metrics collector.
        
        Args:
            execution_session: Execution session
            collector: Metrics collector to apply
            
        Returns:
            List of collected metrics
        """
        # This is a simplified implementation
        # In a real system, this would apply the collector's query pattern
        # and aggregation method to extract specific metrics
        
        if collector.metric_type == "business":
            # Collect business-specific metrics
            business_metrics = {
                "user_satisfaction": 0.85,
                "cost_per_query": 0.05,
                "time_to_insight": 15.0
            }
            
            return [PerformanceMetrics(
                session_id=execution_session.session_id,
                agent_configuration_id=execution_session.workflow_id,
                execution_time_seconds=0.0,
                success_rate=1.0,
                throughput=0.0,
                business_metrics=business_metrics
            )]
        
        return []
    
    def _aggregate_metrics(self, metrics: List[PerformanceMetrics]) -> Dict[str, float]:
        """Aggregate metrics using configured methods.
        
        Args:
            metrics: List of metrics to aggregate
            
        Returns:
            Dictionary of aggregated metrics
        """
        if not metrics:
            return {}
        
        aggregated = {}
        
        # Aggregate core metrics
        execution_times = [m.execution_time_seconds for m in metrics if m.execution_time_seconds > 0]
        success_rates = [m.success_rate for m in metrics]
        throughputs = [m.throughput for m in metrics if m.throughput > 0]
        error_rates = [m.error_rate for m in metrics]
        
        if execution_times:
            aggregated["avg_execution_time"] = statistics.mean(execution_times)
            aggregated["max_execution_time"] = max(execution_times)
            aggregated["min_execution_time"] = min(execution_times)
        
        if success_rates:
            aggregated["avg_success_rate"] = statistics.mean(success_rates)
        
        if throughputs:
            aggregated["avg_throughput"] = statistics.mean(throughputs)
        
        if error_rates:
            aggregated["avg_error_rate"] = statistics.mean(error_rates)
        
        return aggregated
    
    def _update_real_time_metrics(self, metrics: List[PerformanceMetrics]) -> None:
        """Update real-time metrics storage.
        
        Args:
            metrics: Metrics to add to real-time storage
        """
        for metric in metrics:
            # Store key metrics in real-time deques
            self.real_time_metrics["execution_time"].append(
                (datetime.utcnow(), metric.execution_time_seconds)
            )
            self.real_time_metrics["success_rate"].append(
                (datetime.utcnow(), metric.success_rate)
            )
            self.real_time_metrics["throughput"].append(
                (datetime.utcnow(), metric.throughput)
            )
    
    async def analyze_trends(self, historical_data: List[MetricsSnapshot]) -> TrendAnalysis:
        """Analyze performance trends using configured analyzers.
        
        Args:
            historical_data: Historical metrics snapshots
            
        Returns:
            Comprehensive trend analysis
        """
        try:
            # Use the first enabled trend analyzer
            analyzer = next((a for a in self.trend_analyzers if a.enabled), None)
            if not analyzer:
                self.logger.warning("No enabled trend analyzers found")
                return self._create_default_trend_analysis()
            
            # Prepare data for analysis
            trend_data = self._prepare_trend_data(historical_data, analyzer)
            
            # Perform trend analysis
            analysis = await self._perform_trend_analysis(trend_data, analyzer)
            
            # Store analysis
            self.trend_analyses.append(analysis)
            
            self.logger.info(f"Completed trend analysis: {analysis.trend_direction}")
            return analysis
            
        except Exception as e:
            self.logger.error(f"Trend analysis failed: {e}")
            return self._create_default_trend_analysis()
    
    def _prepare_trend_data(
        self,
        historical_data: List[MetricsSnapshot],
        analyzer: TrendAnalyzer
    ) -> Dict[str, List[float]]:
        """Prepare data for trend analysis.
        
        Args:
            historical_data: Historical metrics data
            analyzer: Trend analyzer configuration
            
        Returns:
            Dictionary of metric time series data
        """
        trend_data = defaultdict(list)
        
        # Extract time series for each metric
        for snapshot in historical_data:
            for metric_name in analyzer.metric_names:
                value = snapshot.aggregated_metrics.get(f"avg_{metric_name}")
                if value is not None:
                    trend_data[metric_name].append(value)
        
        return dict(trend_data)
    
    async def _perform_trend_analysis(
        self,
        trend_data: Dict[str, List[float]],
        analyzer: TrendAnalyzer
    ) -> TrendAnalysis:
        """Perform actual trend analysis.
        
        Args:
            trend_data: Time series data
            analyzer: Analyzer configuration
            
        Returns:
            Trend analysis result
        """
        # Simple trend analysis implementation
        # In a real system, this would use more sophisticated statistical methods
        
        primary_metric = analyzer.metric_names[0] if analyzer.metric_names else "execution_time_seconds"
        data_points = trend_data.get(primary_metric, [])
        
        if len(data_points) < 2:
            return TrendAnalysis(
                analyzer_name=analyzer.name,
                metric_name=primary_metric,
                analysis_period=f"{analyzer.analysis_window_hours}h",
                trend_direction="insufficient_data",
                trend_strength=0.0,
                statistical_significance=0.0,
                confidence_level=0.0
            )
        
        # Calculate simple trend
        first_half = data_points[:len(data_points)//2]
        second_half = data_points[len(data_points)//2:]
        
        first_avg = statistics.mean(first_half)
        second_avg = statistics.mean(second_half)
        
        change_ratio = (second_avg - first_avg) / first_avg if first_avg != 0 else 0
        
        if abs(change_ratio) < 0.05:
            trend_direction = "stable"
        elif change_ratio > 0:
            trend_direction = "increasing"
        else:
            trend_direction = "decreasing"
        
        trend_strength = min(abs(change_ratio), 1.0)
        
        # Generate insights
        insights = []
        if trend_direction == "increasing" and primary_metric == "execution_time_seconds":
            insights.append("Execution times are increasing - consider performance optimization")
        elif trend_direction == "decreasing" and primary_metric == "success_rate":
            insights.append("Success rates are declining - review error handling and agent logic")
        elif trend_direction == "stable":
            insights.append("Performance metrics are stable")
        
        return TrendAnalysis(
            analyzer_name=analyzer.name,
            metric_name=primary_metric,
            analysis_period=f"{analyzer.analysis_window_hours}h",
            trend_direction=trend_direction,
            trend_strength=trend_strength,
            statistical_significance=0.8,  # Simplified
            key_insights=insights,
            confidence_level=0.7
        )
    
    def _create_default_trend_analysis(self) -> TrendAnalysis:
        """Create a default trend analysis when analysis fails.
        
        Returns:
            Default trend analysis
        """
        return TrendAnalysis(
            analyzer_name="default",
            metric_name="unknown",
            analysis_period="24h",
            trend_direction="unknown",
            trend_strength=0.0,
            statistical_significance=0.0,
            key_insights=["Trend analysis unavailable"],
            confidence_level=0.0
        )
    
    async def recommend_optimizations(self, performance_data: PerformanceData) -> List[Optimization]:
        """Generate optimization recommendations based on performance data.
        
        Args:
            performance_data: Performance data to analyze
            
        Returns:
            List of optimization recommendations
        """
        try:
            recommendations: List[Optimization] = []
            
            # Apply each enabled recommender
            for recommender in self.optimization_recommenders:
                if not recommender.enabled:
                    continue
                
                try:
                    recs = await self._apply_recommender(performance_data, recommender)
                    recommendations.extend(recs)
                except Exception as e:
                    self.logger.warning(f"Recommender {recommender.name} failed: {e}")
            
            # Sort by priority and confidence
            recommendations.sort(
                key=lambda x: (
                    {"critical": 4, "high": 3, "medium": 2, "low": 1}[x.priority],
                    x.confidence_score
                ),
                reverse=True
            )
            
            # Store recommendations
            self.optimizations.extend(recommendations)
            
            self.logger.info(f"Generated {len(recommendations)} optimization recommendations")
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Optimization recommendation failed: {e}")
            return []
    
    async def _apply_recommender(
        self,
        performance_data: PerformanceData,
        recommender: OptimizationRecommender
    ) -> List[Optimization]:
        """Apply a specific optimization recommender.
        
        Args:
            performance_data: Performance data
            recommender: Recommender configuration
            
        Returns:
            List of optimization recommendations
        """
        recommendations = []
        
        # Get latest metrics
        if not performance_data.metrics_snapshots:
            return recommendations
        
        latest_snapshot = performance_data.metrics_snapshots[-1]
        
        # Apply recommendation rules
        for target_metric in recommender.target_metrics:
            current_value = latest_snapshot.aggregated_metrics.get(f"avg_{target_metric}")
            if current_value is None:
                continue
            
            # Generate recommendations based on simple rules
            if target_metric == "execution_time_seconds" and current_value > 30.0:
                recommendations.append(Optimization(
                    recommender_name=recommender.name,
                    target_metric=target_metric,
                    current_value=current_value,
                    target_value=15.0,
                    improvement_percentage=50.0,
                    recommendation_type="configuration",
                    specific_actions=[
                        "Increase timeout settings",
                        "Enable parallel processing",
                        "Optimize query complexity"
                    ],
                    estimated_impact={"execution_time_reduction": 0.5},
                    confidence_score=0.8,
                    priority="high"
                ))
            
            elif target_metric == "success_rate" and current_value < 0.8:
                recommendations.append(Optimization(
                    recommender_name=recommender.name,
                    target_metric=target_metric,
                    current_value=current_value,
                    target_value=0.95,
                    improvement_percentage=18.75,
                    recommendation_type="algorithm",
                    specific_actions=[
                        "Improve error handling",
                        "Add input validation",
                        "Implement retry mechanisms"
                    ],
                    estimated_impact={"success_rate_improvement": 0.15},
                    confidence_score=0.9,
                    priority="critical"
                ))
        
        return recommendations
    
    def get_real_time_metrics(self, metric_name: str, window_minutes: int = 60) -> List[tuple]:
        """Get real-time metrics for a specific metric.
        
        Args:
            metric_name: Name of metric to retrieve
            window_minutes: Time window in minutes
            
        Returns:
            List of (timestamp, value) tuples
        """
        if metric_name not in self.real_time_metrics:
            return []
        
        cutoff_time = datetime.utcnow() - timedelta(minutes=window_minutes)
        
        return [
            (timestamp, value) 
            for timestamp, value in self.real_time_metrics[metric_name]
            if timestamp >= cutoff_time
        ]
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get a summary of overall performance.
        
        Returns:
            Dictionary containing performance summary
        """
        if not self.metrics_history:
            return {"status": "no_data"}
        
        recent_metrics = self.metrics_history[-10:]  # Last 10 metrics
        
        avg_execution_time = statistics.mean([m.execution_time_seconds for m in recent_metrics])
        avg_success_rate = statistics.mean([m.success_rate for m in recent_metrics])
        avg_throughput = statistics.mean([m.throughput for m in recent_metrics if m.throughput > 0])
        
        return {
            "status": "active",
            "total_metrics_collected": len(self.metrics_history),
            "recent_performance": {
                "avg_execution_time": avg_execution_time,
                "avg_success_rate": avg_success_rate,
                "avg_throughput": avg_throughput or 0.0
            },
            "trend_analyses_count": len(self.trend_analyses),
            "optimizations_generated": len(self.optimizations),
            "last_updated": datetime.utcnow().isoformat()
        }


================================================
FILE: dataqa/orchestration/evaluation/benchmark.py
================================================
"""
Comprehensive benchmarking framework for multi-agent system evaluation.
"""

import asyncio
from datetime import datetime
from typing import Any, Dict, List, Optional, Callable, Union
from uuid import uuid4
from pathlib import Path
import json

from pydantic import BaseModel, Field, validator

from ...config.models import LLMConfig
from ...primitives.llm import LLMInterface, create_llm_interface
from ...logging_config import get_primitive_logger
from ...exceptions import DataQAError
from ..models import AgentConfiguration, ExecutionSession, ExecutionMetrics


class GroundTruth(BaseModel):
    """Ground truth data for evaluation."""
    truth_id: str = Field(default_factory=lambda: str(uuid4()))
    expected_output: Any
    acceptable_variations: List[Any] = Field(default_factory=list)
    evaluation_criteria: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class TestCase(BaseModel):
    """Individual test case for benchmarking."""
    test_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    category: str = "general"
    difficulty_level: str = "medium"  # easy, medium, hard
    inputs: Dict[str, Any] = Field(default_factory=dict)
    ground_truth: GroundTruth
    timeout_seconds: int = 300
    retry_count: int = 0
    tags: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class EvaluationCriterion(BaseModel):
    """Evaluation criterion for scoring responses."""
    criterion_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    weight: float = 1.0
    scoring_method: str = "llm_judge"  # llm_judge, exact_match, similarity, custom
    parameters: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class PerformanceBaseline(BaseModel):
    """Performance baseline for comparison."""
    baseline_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    metrics: Dict[str, float] = Field(default_factory=dict)
    agent_configuration: Optional[str] = None
    established_date: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class QualityThreshold(BaseModel):
    """Quality threshold for pass/fail determination."""
    threshold_id: str = Field(default_factory=lambda: str(uuid4()))
    metric_name: str
    minimum_value: float
    maximum_value: Optional[float] = None
    severity: str = "error"  # error, warning, info
    description: str = ""
    metadata: Dict[str, Any] = Field(default_factory=dict)


class BenchmarkSuite(BaseModel):
    """Collection of test cases for benchmarking."""
    suite_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    version: str = "1.0.0"
    test_cases: List[TestCase] = Field(default_factory=list)
    evaluation_criteria: List[EvaluationCriterion] = Field(default_factory=list)
    performance_baselines: Dict[str, PerformanceBaseline] = Field(default_factory=dict)
    quality_thresholds: Dict[str, QualityThreshold] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class BenchmarkResult(BaseModel):
    """Result of running a single test case."""
    result_id: str = Field(default_factory=lambda: str(uuid4()))
    test_case_id: str
    agent_configuration_id: str
    execution_time_seconds: float
    success: bool
    agent_response: Any
    evaluation_scores: Dict[str, float] = Field(default_factory=dict)
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    timestamp: datetime = Field(default_factory=datetime.utcnow)


class BenchmarkReport(BaseModel):
    """Comprehensive report of benchmark execution."""
    report_id: str = Field(default_factory=lambda: str(uuid4()))
    suite_id: str
    agent_configuration_id: str
    execution_start: datetime
    execution_end: datetime
    total_test_cases: int
    passed_test_cases: int
    failed_test_cases: int
    average_execution_time: float
    overall_score: float
    individual_results: List[BenchmarkResult] = Field(default_factory=list)
    performance_metrics: Dict[str, float] = Field(default_factory=dict)
    quality_violations: List[str] = Field(default_factory=list)
    recommendations: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class BenchmarkConfiguration(BaseModel):
    """Configuration for benchmark execution."""
    config_id: str = Field(default_factory=lambda: str(uuid4()))
    suite_ids: List[str]
    agent_configurations: List[AgentConfiguration]
    llm_judge_config: Optional[LLMConfig] = None
    parallel_execution: bool = False
    max_concurrent_tests: int = 5
    timeout_seconds: int = 600
    retry_failed_tests: bool = True
    generate_detailed_reports: bool = True
    output_directory: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)


class BenchmarkFramework:
    """
    Comprehensive framework for managing and executing benchmark suites.
    
    Supports configurable test cases, ground truth data sources, LLM judge evaluation,
    and automated benchmark execution with business rules integration.
    """
    
    def __init__(self, llm_config: Optional[LLMConfig] = None):
        """Initialize the benchmark framework.
        
        Args:
            llm_config: Optional LLM configuration for judge evaluation
        """
        self.logger = get_primitive_logger("benchmark", "framework")
        self.suites: Dict[str, BenchmarkSuite] = {}
        self.configurations: Dict[str, BenchmarkConfiguration] = {}
        self.results_history: List[BenchmarkReport] = []
        
        # Initialize LLM judge if configuration provided
        self.llm_judge: Optional[LLMInterface] = None
        if llm_config:
            try:
                self.llm_judge = create_llm_interface(llm_config)
                self.logger.info("LLM judge initialized for evaluation")
            except Exception as e:
                self.logger.warning(f"Failed to initialize LLM judge: {e}")
    
    def add_suite(self, suite: BenchmarkSuite) -> None:
        """Add a benchmark suite to the framework.
        
        Args:
            suite: Benchmark suite to add
        """
        self.suites[suite.suite_id] = suite
        self.logger.info(f"Added benchmark suite: {suite.name} ({suite.suite_id})")
    
    def remove_suite(self, suite_id: str) -> bool:
        """Remove a benchmark suite.
        
        Args:
            suite_id: ID of suite to remove
            
        Returns:
            True if suite was removed, False if not found
        """
        if suite_id in self.suites:
            del self.suites[suite_id]
            self.logger.info(f"Removed benchmark suite: {suite_id}")
            return True
        return False
    
    def get_suite(self, suite_id: str) -> Optional[BenchmarkSuite]:
        """Get a benchmark suite by ID.
        
        Args:
            suite_id: ID of suite to retrieve
            
        Returns:
            Benchmark suite or None if not found
        """
        return self.suites.get(suite_id)
    
    def list_suites(self) -> List[BenchmarkSuite]:
        """List all available benchmark suites.
        
        Returns:
            List of all benchmark suites
        """
        return list(self.suites.values())
    
    def add_configuration(self, config: BenchmarkConfiguration) -> None:
        """Add a benchmark configuration.
        
        Args:
            config: Benchmark configuration to add
        """
        self.configurations[config.config_id] = config
        self.logger.info(f"Added benchmark configuration: {config.config_id}")
    
    async def run_benchmark_suite(
        self,
        suite_id: str,
        agent_config: AgentConfiguration,
        agent_executor: Optional[Callable] = None
    ) -> BenchmarkReport:
        """Run a complete benchmark suite against an agent configuration.
        
        Args:
            suite_id: ID of benchmark suite to run
            agent_config: Agent configuration to test
            agent_executor: Optional custom agent executor function
            
        Returns:
            Comprehensive benchmark report
            
        Raises:
            DataQAError: If suite not found or execution fails
        """
        suite = self.suites.get(suite_id)
        if not suite:
            raise DataQAError(f"Benchmark suite not found: {suite_id}")
        
        self.logger.info(f"Starting benchmark suite: {suite.name} with agent: {agent_config.name}")
        
        start_time = datetime.utcnow()
        results: List[BenchmarkResult] = []
        
        # Execute each test case
        for test_case in suite.test_cases:
            try:
                result = await self._run_test_case(
                    test_case, agent_config, suite.evaluation_criteria, agent_executor
                )
                results.append(result)
                self.logger.debug(f"Completed test case: {test_case.name}")
            except Exception as e:
                self.logger.error(f"Failed test case {test_case.name}: {e}")
                # Create failed result
                failed_result = BenchmarkResult(
                    test_case_id=test_case.test_id,
                    agent_configuration_id=agent_config.agent_id,
                    execution_time_seconds=0.0,
                    success=False,
                    agent_response=None,
                    error_message=str(e)
                )
                results.append(failed_result)
        
        end_time = datetime.utcnow()
        
        # Generate comprehensive report
        report = self._generate_report(
            suite, agent_config, results, start_time, end_time
        )
        
        # Check quality thresholds and add violations
        violations = self._check_quality_thresholds(report, suite.quality_thresholds)
        report.quality_violations = violations
        
        # Generate recommendations
        recommendations = self._generate_recommendations(report, suite)
        report.recommendations = recommendations
        
        self.results_history.append(report)
        self.logger.info(f"Completed benchmark suite: {suite.name}")
        
        return report
    
    async def run_benchmark_configuration(
        self,
        config_id: str,
        agent_executor: Optional[Callable] = None
    ) -> List[BenchmarkReport]:
        """Run a benchmark configuration across multiple suites and agents.
        
        Args:
            config_id: ID of benchmark configuration to run
            agent_executor: Optional custom agent executor function
            
        Returns:
            List of benchmark reports for each suite/agent combination
            
        Raises:
            DataQAError: If configuration not found
        """
        config = self.configurations.get(config_id)
        if not config:
            raise DataQAError(f"Benchmark configuration not found: {config_id}")
        
        self.logger.info(f"Starting benchmark configuration: {config_id}")
        
        reports: List[BenchmarkReport] = []
        
        # Run each suite against each agent configuration
        for suite_id in config.suite_ids:
            for agent_config in config.agent_configurations:
                try:
                    report = await self.run_benchmark_suite(
                        suite_id, agent_config, agent_executor
                    )
                    reports.append(report)
                except Exception as e:
                    self.logger.error(f"Failed benchmark {suite_id} with {agent_config.name}: {e}")
        
        # Save reports if output directory specified
        if config.output_directory and config.generate_detailed_reports:
            await self._save_reports(reports, config.output_directory)
        
        self.logger.info(f"Completed benchmark configuration: {config_id}")
        return reports
    
    async def _run_test_case(
        self,
        test_case: TestCase,
        agent_config: AgentConfiguration,
        evaluation_criteria: List[EvaluationCriterion],
        agent_executor: Optional[Callable] = None
    ) -> BenchmarkResult:
        """Run a single test case.
        
        Args:
            test_case: Test case to run
            agent_config: Agent configuration to test
            evaluation_criteria: Criteria for evaluation
            agent_executor: Optional custom agent executor
            
        Returns:
            Benchmark result for the test case
        """
        start_time = datetime.utcnow()
        
        try:
            # Execute agent with test case inputs
            if agent_executor:
                agent_response = await agent_executor(test_case.inputs, agent_config)
            else:
                # Default mock execution for testing
                agent_response = {"mock_response": "test_output"}
            
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            
            # Evaluate response against ground truth
            evaluation_scores = await self._evaluate_response(
                agent_response, test_case.ground_truth, evaluation_criteria
            )
            
            # Determine success based on evaluation scores
            success = all(score >= 0.5 for score in evaluation_scores.values())
            
            return BenchmarkResult(
                test_case_id=test_case.test_id,
                agent_configuration_id=agent_config.agent_id,
                execution_time_seconds=execution_time,
                success=success,
                agent_response=agent_response,
                evaluation_scores=evaluation_scores
            )
            
        except Exception as e:
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            return BenchmarkResult(
                test_case_id=test_case.test_id,
                agent_configuration_id=agent_config.agent_id,
                execution_time_seconds=execution_time,
                success=False,
                agent_response=None,
                error_message=str(e)
            )
    
    async def _evaluate_response(
        self,
        agent_response: Any,
        ground_truth: GroundTruth,
        evaluation_criteria: List[EvaluationCriterion]
    ) -> Dict[str, float]:
        """Evaluate agent response against ground truth using specified criteria.
        
        Args:
            agent_response: Response from agent
            ground_truth: Expected ground truth
            evaluation_criteria: Evaluation criteria to apply
            
        Returns:
            Dictionary of evaluation scores by criterion
        """
        scores: Dict[str, float] = {}
        
        for criterion in evaluation_criteria:
            try:
                if criterion.scoring_method == "llm_judge" and self.llm_judge:
                    score = await self._llm_judge_evaluate(
                        agent_response, ground_truth, criterion
                    )
                elif criterion.scoring_method == "exact_match":
                    score = self._exact_match_evaluate(
                        agent_response, ground_truth, criterion
                    )
                elif criterion.scoring_method == "similarity":
                    score = self._similarity_evaluate(
                        agent_response, ground_truth, criterion
                    )
                else:
                    # Default scoring
                    score = 0.5
                
                scores[criterion.name] = score
                
            except Exception as e:
                self.logger.warning(f"Failed to evaluate criterion {criterion.name}: {e}")
                scores[criterion.name] = 0.0
        
        return scores
    
    async def _llm_judge_evaluate(
        self,
        agent_response: Any,
        ground_truth: GroundTruth,
        criterion: EvaluationCriterion
    ) -> float:
        """Evaluate using LLM judge.
        
        Args:
            agent_response: Response from agent
            ground_truth: Expected ground truth
            criterion: Evaluation criterion
            
        Returns:
            Evaluation score between 0.0 and 1.0
        """
        if not self.llm_judge:
            return 0.5
        
        # Create evaluation prompt
        prompt = f"""
        Evaluate the following agent response against the ground truth based on the criterion: {criterion.name}
        
        Criterion Description: {criterion.description}
        
        Agent Response:
        {json.dumps(agent_response, indent=2, default=str)}
        
        Ground Truth:
        {json.dumps(ground_truth.expected_output, indent=2, default=str)}
        
        Provide a score between 0.0 and 1.0, where:
        - 1.0 = Perfect match/excellent quality
        - 0.8-0.9 = Very good with minor issues
        - 0.6-0.7 = Good with some issues
        - 0.4-0.5 = Acceptable but needs improvement
        - 0.2-0.3 = Poor quality
        - 0.0-0.1 = Completely incorrect/unusable
        
        Return only the numeric score.
        """
        
        try:
            response = await self.llm_judge.format_response(
                {"evaluation_prompt": prompt}, 
                "Evaluate the response quality"
            )
            
            # Extract numeric score from response
            score_str = response.strip()
            score = float(score_str)
            return max(0.0, min(1.0, score))  # Clamp to [0, 1]
            
        except Exception as e:
            self.logger.warning(f"LLM judge evaluation failed: {e}")
            return 0.5
    
    def _exact_match_evaluate(
        self,
        agent_response: Any,
        ground_truth: GroundTruth,
        criterion: EvaluationCriterion
    ) -> float:
        """Evaluate using exact match comparison.
        
        Args:
            agent_response: Response from agent
            ground_truth: Expected ground truth
            criterion: Evaluation criterion
            
        Returns:
            1.0 if exact match, 0.0 otherwise
        """
        if agent_response == ground_truth.expected_output:
            return 1.0
        
        # Check acceptable variations
        for variation in ground_truth.acceptable_variations:
            if agent_response == variation:
                return 1.0
        
        return 0.0
    
    def _similarity_evaluate(
        self,
        agent_response: Any,
        ground_truth: GroundTruth,
        criterion: EvaluationCriterion
    ) -> float:
        """Evaluate using similarity comparison.
        
        Args:
            agent_response: Response from agent
            ground_truth: Expected ground truth
            criterion: Evaluation criterion
            
        Returns:
            Similarity score between 0.0 and 1.0
        """
        # Simple string similarity for now
        if isinstance(agent_response, str) and isinstance(ground_truth.expected_output, str):
            response_words = set(agent_response.lower().split())
            truth_words = set(ground_truth.expected_output.lower().split())
            
            if not truth_words:
                return 1.0 if not response_words else 0.0
            
            intersection = response_words.intersection(truth_words)
            union = response_words.union(truth_words)
            
            return len(intersection) / len(union) if union else 1.0
        
        # Default similarity for non-string types
        return 1.0 if agent_response == ground_truth.expected_output else 0.0
    
    def _generate_report(
        self,
        suite: BenchmarkSuite,
        agent_config: AgentConfiguration,
        results: List[BenchmarkResult],
        start_time: datetime,
        end_time: datetime
    ) -> BenchmarkReport:
        """Generate comprehensive benchmark report.
        
        Args:
            suite: Benchmark suite that was run
            agent_config: Agent configuration tested
            results: Individual test results
            start_time: Execution start time
            end_time: Execution end time
            
        Returns:
            Comprehensive benchmark report
        """
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.success)
        failed_tests = total_tests - passed_tests
        
        avg_execution_time = (
            sum(r.execution_time_seconds for r in results) / total_tests
            if total_tests > 0 else 0.0
        )
        
        # Calculate overall score as weighted average of evaluation scores
        total_score = 0.0
        total_weight = 0.0
        
        for result in results:
            for criterion_name, score in result.evaluation_scores.items():
                # Find criterion weight
                weight = 1.0
                for criterion in suite.evaluation_criteria:
                    if criterion.name == criterion_name:
                        weight = criterion.weight
                        break
                
                total_score += score * weight
                total_weight += weight
        
        overall_score = total_score / total_weight if total_weight > 0 else 0.0
        
        # Calculate performance metrics
        performance_metrics = {
            "success_rate": passed_tests / total_tests if total_tests > 0 else 0.0,
            "average_execution_time": avg_execution_time,
            "throughput": total_tests / (end_time - start_time).total_seconds(),
            "error_rate": failed_tests / total_tests if total_tests > 0 else 0.0
        }
        
        return BenchmarkReport(
            suite_id=suite.suite_id,
            agent_configuration_id=agent_config.agent_id,
            execution_start=start_time,
            execution_end=end_time,
            total_test_cases=total_tests,
            passed_test_cases=passed_tests,
            failed_test_cases=failed_tests,
            average_execution_time=avg_execution_time,
            overall_score=overall_score,
            individual_results=results,
            performance_metrics=performance_metrics
        )
    
    def _check_quality_thresholds(
        self,
        report: BenchmarkReport,
        thresholds: Dict[str, QualityThreshold]
    ) -> List[str]:
        """Check quality thresholds and return violations.
        
        Args:
            report: Benchmark report to check
            thresholds: Quality thresholds to check against
            
        Returns:
            List of quality violation messages
        """
        violations: List[str] = []
        
        for threshold_name, threshold in thresholds.items():
            metric_value = report.performance_metrics.get(threshold.metric_name)
            
            if metric_value is None:
                continue
            
            if metric_value < threshold.minimum_value:
                violations.append(
                    f"{threshold.metric_name} ({metric_value:.3f}) below minimum threshold "
                    f"({threshold.minimum_value:.3f}): {threshold.description}"
                )
            
            if threshold.maximum_value and metric_value > threshold.maximum_value:
                violations.append(
                    f"{threshold.metric_name} ({metric_value:.3f}) above maximum threshold "
                    f"({threshold.maximum_value:.3f}): {threshold.description}"
                )
        
        return violations
    
    def _generate_recommendations(
        self,
        report: BenchmarkReport,
        suite: BenchmarkSuite
    ) -> List[str]:
        """Generate optimization recommendations based on benchmark results.
        
        Args:
            report: Benchmark report to analyze
            suite: Benchmark suite that was run
            
        Returns:
            List of optimization recommendations
        """
        recommendations: List[str] = []
        
        # Performance-based recommendations
        if report.performance_metrics["success_rate"] < 0.8:
            recommendations.append(
                "Success rate is below 80%. Consider reviewing agent logic and error handling."
            )
        
        if report.performance_metrics["average_execution_time"] > 30.0:
            recommendations.append(
                "Average execution time is high. Consider optimizing agent performance or increasing timeout."
            )
        
        if report.overall_score < 0.7:
            recommendations.append(
                "Overall quality score is below 70%. Review evaluation criteria and agent responses."
            )
        
        # Failure pattern analysis
        failed_categories = {}
        for result in report.individual_results:
            if not result.success:
                test_case = next(
                    (tc for tc in suite.test_cases if tc.test_id == result.test_case_id),
                    None
                )
                if test_case:
                    category = test_case.category
                    failed_categories[category] = failed_categories.get(category, 0) + 1
        
        if failed_categories:
            worst_category = max(failed_categories.items(), key=lambda x: x[1])
            recommendations.append(
                f"High failure rate in '{worst_category[0]}' category ({worst_category[1]} failures). "
                f"Focus improvement efforts on this area."
            )
        
        return recommendations
    
    async def _save_reports(
        self,
        reports: List[BenchmarkReport],
        output_directory: str
    ) -> None:
        """Save benchmark reports to files.
        
        Args:
            reports: Reports to save
            output_directory: Directory to save reports in
        """
        try:
            output_path = Path(output_directory)
            output_path.mkdir(parents=True, exist_ok=True)
            
            for report in reports:
                filename = f"benchmark_report_{report.report_id}.json"
                filepath = output_path / filename
                
                with open(filepath, 'w') as f:
                    json.dump(report.dict(), f, indent=2, default=str)
                
                self.logger.info(f"Saved benchmark report: {filepath}")
                
        except Exception as e:
            self.logger.error(f"Failed to save reports: {e}")
    
    def get_results_history(self) -> List[BenchmarkReport]:
        """Get historical benchmark results.
        
        Returns:
            List of all benchmark reports
        """
        return self.results_history.copy()
    
    def get_performance_trends(self, agent_id: str, metric_name: str) -> List[float]:
        """Get performance trends for a specific agent and metric.
        
        Args:
            agent_id: Agent configuration ID
            metric_name: Name of metric to track
            
        Returns:
            List of metric values over time
        """
        trends = []
        for report in self.results_history:
            if report.agent_configuration_id == agent_id:
                value = report.performance_metrics.get(metric_name)
                if value is not None:
                    trends.append(value)
        return trends


================================================
FILE: dataqa/orchestration/evaluation/judge.py
================================================
"""
LLM judge evaluator for multi-agent system assessment with pluggable scoring rubrics.
"""

import json
from datetime import datetime
from typing import Any, Dict, List, Optional, Callable
from uuid import uuid4

from pydantic import BaseModel, Field

from ...config.models import LLMConfig
from ...primitives.llm import LLMInterface, create_llm_interface
from ...logging_config import get_primitive_logger
from ...exceptions import DataQAError
from ..models import AgentConfiguration


class ScoringRubric(BaseModel):
    """Scoring rubric for domain-specific evaluation."""
    rubric_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    domain: str = "general"
    criteria: List[str] = Field(default_factory=list)
    scoring_scale: Dict[str, str] = Field(default_factory=dict)
    prompt_template: str = ""
    weight_distribution: Dict[str, float] = Field(default_factory=dict)
    examples: List[Dict[str, Any]] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class EvaluationCriterion(BaseModel):
    """Individual evaluation criterion."""
    criterion_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    weight: float = 1.0
    scoring_method: str = "llm_judge"
    parameters: Dict[str, Any] = Field(default_factory=dict)


class AgentResponse(BaseModel):
    """Agent response for evaluation."""
    response_id: str = Field(default_factory=lambda: str(uuid4()))
    agent_id: str
    content: Any
    execution_time_seconds: float = 0.0
    metadata: Dict[str, Any] = Field(default_factory=dict)
    timestamp: datetime = Field(default_factory=datetime.utcnow)


class GroundTruth(BaseModel):
    """Ground truth for evaluation comparison."""
    truth_id: str = Field(default_factory=lambda: str(uuid4()))
    expected_output: Any
    acceptable_variations: List[Any] = Field(default_factory=list)
    evaluation_notes: str = ""
    metadata: Dict[str, Any] = Field(default_factory=dict)


class EvaluationResult(BaseModel):
    """Comprehensive result of LLM judge evaluation."""
    evaluation_id: str = Field(default_factory=lambda: str(uuid4()))
    test_case_id: str
    agent_configuration_id: str
    overall_score: float
    criterion_scores: Dict[str, float] = Field(default_factory=dict)
    explanation: str
    confidence_level: float
    rubric_used: Optional[str] = None
    evaluation_time_seconds: float = 0.0
    improvement_suggestions: List[str] = Field(default_factory=list)
    strengths: List[str] = Field(default_factory=list)
    weaknesses: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    timestamp: datetime = Field(default_factory=datetime.utcnow)


class LLMModel(BaseModel):
    """LLM model configuration for judging."""
    model_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    provider: str
    model_name: str
    temperature: float = 0.1
    max_tokens: int = 2000
    specialized_domains: List[str] = Field(default_factory=list)
    capabilities: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class LLMJudgeEvaluator:
    """
    Advanced LLM-based evaluator with pluggable domain-specific scoring rubrics.
    
    Supports multiple judge models, configurable evaluation criteria, and
    structured scoring with reasoning and confidence measures.
    """
    
    def __init__(self, llm_configs: List[LLMConfig]):
        """Initialize the LLM judge evaluator.
        
        Args:
            llm_configs: List of LLM configurations for judge models
        """
        self.logger = get_primitive_logger("judge", "evaluator")
        self.judge_models: List[LLMInterface] = []
        self.scoring_rubrics: Dict[str, ScoringRubric] = {}
        self.evaluation_history: List[EvaluationResult] = []
        
        # Initialize judge models
        for config in llm_configs:
            try:
                judge_model = create_llm_interface(config)
                self.judge_models.append(judge_model)
                self.logger.info(f"Initialized judge model: {config.model}")
            except Exception as e:
                self.logger.warning(f"Failed to initialize judge model {config.model}: {e}")
        
        if not self.judge_models:
            self.logger.warning("No judge models initialized - evaluation will use fallback scoring")
        
        # Initialize default rubrics
        self._initialize_default_rubrics()
    
    def _initialize_default_rubrics(self) -> None:
        """Initialize default scoring rubrics for common domains."""
        
        # General purpose rubric
        general_rubric = ScoringRubric(
            name="General Quality Assessment",
            description="General purpose evaluation rubric for agent responses",
            domain="general",
            criteria=[
                "Correctness and accuracy",
                "Completeness of response",
                "Clarity and coherence",
                "Relevance to query"
            ],
            scoring_scale={
                "excellent": "90-100: Exceptional quality, exceeds expectations",
                "good": "70-89: Good quality with minor issues",
                "acceptable": "50-69: Acceptable but needs improvement",
                "poor": "30-49: Poor quality with significant issues",
                "unacceptable": "0-29: Unacceptable, major problems"
            },
            weight_distribution={
                "correctness": 0.4,
                "completeness": 0.3,
                "clarity": 0.2,
                "relevance": 0.1
            },
            prompt_template="""
            Evaluate the agent response based on the following criteria:
            
            1. Correctness and Accuracy (40%): Is the response factually correct and accurate?
            2. Completeness (30%): Does the response fully address the query?
            3. Clarity and Coherence (20%): Is the response clear and well-structured?
            4. Relevance (10%): Is the response relevant to the original query?
            
            Agent Response: {agent_response}
            Ground Truth: {ground_truth}
            Query Context: {query_context}
            
            Provide your evaluation in the following JSON format:
            {{
                "overall_score": <0-100>,
                "criterion_scores": {{
                    "correctness": <0-100>,
                    "completeness": <0-100>,
                    "clarity": <0-100>,
                    "relevance": <0-100>
                }},
                "explanation": "<detailed explanation>",
                "confidence": <0-100>,
                "strengths": ["<strength1>", "<strength2>"],
                "weaknesses": ["<weakness1>", "<weakness2>"],
                "suggestions": ["<suggestion1>", "<suggestion2>"]
            }}
            """
        )
        self.add_scoring_rubric(general_rubric)
        
        # Data analysis specific rubric
        data_analysis_rubric = ScoringRubric(
            name="Data Analysis Quality",
            description="Specialized rubric for data analysis and SQL query evaluation",
            domain="data_analysis",
            criteria=[
                "Query correctness",
                "Data handling accuracy",
                "Performance efficiency",
                "Business logic alignment",
                "Error handling"
            ],
            scoring_scale={
                "excellent": "90-100: Perfect analysis with optimal approach",
                "good": "70-89: Correct analysis with minor inefficiencies",
                "acceptable": "50-69: Functional but suboptimal approach",
                "poor": "30-49: Incorrect analysis or major issues",
                "unacceptable": "0-29: Completely wrong or dangerous"
            },
            weight_distribution={
                "correctness": 0.35,
                "accuracy": 0.25,
                "efficiency": 0.15,
                "business_logic": 0.15,
                "error_handling": 0.1
            },
            prompt_template="""
            Evaluate this data analysis response focusing on technical accuracy and business value:
            
            1. Query Correctness (35%): Is the SQL/analysis logic correct?
            2. Data Handling Accuracy (25%): Are data types and transformations handled properly?
            3. Performance Efficiency (15%): Is the approach efficient and scalable?
            4. Business Logic Alignment (15%): Does it align with business requirements?
            5. Error Handling (10%): Are edge cases and errors handled appropriately?
            
            Agent Response: {agent_response}
            Expected Output: {ground_truth}
            Business Context: {query_context}
            
            Return evaluation as JSON with scores, explanation, and specific technical feedback.
            """
        )
        self.add_scoring_rubric(data_analysis_rubric)
        
        # Code generation rubric
        code_generation_rubric = ScoringRubric(
            name="Code Generation Quality",
            description="Rubric for evaluating generated code quality and safety",
            domain="code_generation",
            criteria=[
                "Functional correctness",
                "Code quality and style",
                "Security considerations",
                "Performance optimization",
                "Documentation and comments"
            ],
            scoring_scale={
                "excellent": "90-100: Production-ready code with best practices",
                "good": "70-89: Good code with minor style issues",
                "acceptable": "50-69: Functional but needs refinement",
                "poor": "30-49: Works but has significant issues",
                "unacceptable": "0-29: Broken or unsafe code"
            },
            weight_distribution={
                "correctness": 0.4,
                "quality": 0.25,
                "security": 0.2,
                "performance": 0.1,
                "documentation": 0.05
            }
        )
        self.add_scoring_rubric(code_generation_rubric)
    
    def add_scoring_rubric(self, rubric: ScoringRubric) -> None:
        """Add a scoring rubric to the evaluator.
        
        Args:
            rubric: Scoring rubric to add
        """
        self.scoring_rubrics[rubric.rubric_id] = rubric
        self.logger.info(f"Added scoring rubric: {rubric.name} for domain: {rubric.domain}")
    
    def get_rubric_by_domain(self, domain: str) -> Optional[ScoringRubric]:
        """Get the best scoring rubric for a domain.
        
        Args:
            domain: Domain to find rubric for
            
        Returns:
            Best matching scoring rubric or None
        """
        # First try exact domain match
        for rubric in self.scoring_rubrics.values():
            if rubric.domain == domain:
                return rubric
        
        # Fall back to general rubric
        for rubric in self.scoring_rubrics.values():
            if rubric.domain == "general":
                return rubric
        
        return None
    
    def list_rubrics(self) -> List[ScoringRubric]:
        """List all available scoring rubrics.
        
        Returns:
            List of all scoring rubrics
        """
        return list(self.scoring_rubrics.values())
    
    async def evaluate_response(
        self,
        agent_response: AgentResponse,
        ground_truth: GroundTruth,
        evaluation_criteria: List[EvaluationCriterion],
        domain: str = "general",
        query_context: Optional[str] = None
    ) -> EvaluationResult:
        """Evaluate an agent response against ground truth using domain-specific rubrics.
        
        Args:
            agent_response: Agent response to evaluate
            ground_truth: Expected ground truth
            evaluation_criteria: Specific evaluation criteria
            domain: Domain for rubric selection
            query_context: Optional context about the original query
            
        Returns:
            Comprehensive evaluation result
            
        Raises:
            DataQAError: If evaluation fails
        """
        start_time = datetime.utcnow()
        
        try:
            # Select appropriate rubric
            rubric = self.get_rubric_by_domain(domain)
            if not rubric:
                self.logger.warning(f"No rubric found for domain: {domain}, using default")
                rubric = self.get_rubric_by_domain("general")
            
            # Choose best judge model for this domain
            judge_model = self._select_judge_model(domain)
            
            if judge_model and rubric:
                # Use LLM judge evaluation
                result = await self._llm_judge_evaluate(
                    agent_response, ground_truth, rubric, judge_model, query_context
                )
            else:
                # Fallback to rule-based evaluation
                result = self._fallback_evaluate(
                    agent_response, ground_truth, evaluation_criteria
                )
            
            # Calculate evaluation time
            evaluation_time = (datetime.utcnow() - start_time).total_seconds()
            result.evaluation_time_seconds = evaluation_time
            
            # Store in history
            self.evaluation_history.append(result)
            
            self.logger.info(f"Evaluated response with score: {result.overall_score:.2f}")
            return result
            
        except Exception as e:
            self.logger.error(f"Evaluation failed: {e}")
            raise DataQAError(f"Failed to evaluate response: {e}")
    
    def _select_judge_model(self, domain: str) -> Optional[LLMInterface]:
        """Select the best judge model for a domain.
        
        Args:
            domain: Domain to select model for
            
        Returns:
            Best judge model or None
        """
        if not self.judge_models:
            return None
        
        # For now, return the first available model
        # In the future, could implement domain-specific model selection
        return self.judge_models[0]
    
    async def _llm_judge_evaluate(
        self,
        agent_response: AgentResponse,
        ground_truth: GroundTruth,
        rubric: ScoringRubric,
        judge_model: LLMInterface,
        query_context: Optional[str] = None
    ) -> EvaluationResult:
        """Perform LLM-based evaluation using a scoring rubric.
        
        Args:
            agent_response: Agent response to evaluate
            ground_truth: Expected ground truth
            rubric: Scoring rubric to use
            judge_model: LLM model for judging
            query_context: Optional query context
            
        Returns:
            Evaluation result
        """
        # Prepare evaluation prompt
        prompt = rubric.prompt_template.format(
            agent_response=json.dumps(agent_response.content, indent=2, default=str),
            ground_truth=json.dumps(ground_truth.expected_output, indent=2, default=str),
            query_context=query_context or "No additional context provided"
        )
        
        try:
            # Get evaluation from LLM judge
            response = await judge_model.format_response(
                {"evaluation_prompt": prompt},
                "Evaluate the agent response quality"
            )
            
            # Parse JSON response
            evaluation_data = self._parse_evaluation_response(response)
            
            # Create evaluation result
            result = EvaluationResult(
                test_case_id="",  # Will be set by caller
                agent_configuration_id=agent_response.agent_id,
                overall_score=evaluation_data.get("overall_score", 50.0) / 100.0,  # Convert to 0-1 scale
                criterion_scores={
                    k: v / 100.0 for k, v in evaluation_data.get("criterion_scores", {}).items()
                },
                explanation=evaluation_data.get("explanation", "No explanation provided"),
                confidence_level=evaluation_data.get("confidence", 50.0) / 100.0,
                rubric_used=rubric.name,
                improvement_suggestions=evaluation_data.get("suggestions", []),
                strengths=evaluation_data.get("strengths", []),
                weaknesses=evaluation_data.get("weaknesses", [])
            )
            
            return result
            
        except Exception as e:
            self.logger.warning(f"LLM judge evaluation failed: {e}")
            # Return fallback evaluation
            return EvaluationResult(
                test_case_id="",
                agent_configuration_id=agent_response.agent_id,
                overall_score=0.5,
                explanation=f"LLM evaluation failed: {e}",
                confidence_level=0.1,
                rubric_used=rubric.name
            )
    
    def _parse_evaluation_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM evaluation response.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed evaluation data
        """
        try:
            # Try to extract JSON from response
            json_str = response.strip()
            
            # Handle markdown code blocks
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif "```" in json_str:
                json_str = json_str.split("```")[1].split("```")[0].strip()
            
            return json.loads(json_str)
            
        except json.JSONDecodeError:
            self.logger.warning("Failed to parse JSON evaluation response")
            return {
                "overall_score": 50.0,
                "explanation": "Failed to parse evaluation response",
                "confidence": 10.0
            }
    
    def _fallback_evaluate(
        self,
        agent_response: AgentResponse,
        ground_truth: GroundTruth,
        evaluation_criteria: List[EvaluationCriterion]
    ) -> EvaluationResult:
        """Fallback rule-based evaluation when LLM judge is unavailable.
        
        Args:
            agent_response: Agent response to evaluate
            ground_truth: Expected ground truth
            evaluation_criteria: Evaluation criteria
            
        Returns:
            Basic evaluation result
        """
        # Simple rule-based evaluation
        score = 0.5  # Default neutral score
        
        # Check for exact match
        if agent_response.content == ground_truth.expected_output:
            score = 1.0
        elif agent_response.content in ground_truth.acceptable_variations:
            score = 0.8
        
        return EvaluationResult(
            test_case_id="",
            agent_configuration_id=agent_response.agent_id,
            overall_score=score,
            explanation="Rule-based evaluation (LLM judge unavailable)",
            confidence_level=0.6,
            rubric_used="fallback"
        )
    
    async def generate_explanation(self, evaluation: EvaluationResult) -> str:
        """Generate detailed explanation for an evaluation result.
        
        Args:
            evaluation: Evaluation result to explain
            
        Returns:
            Detailed explanation string
        """
        explanation = f"Evaluation Summary (Score: {evaluation.overall_score:.2f})\n\n"
        
        if evaluation.strengths:
            explanation += "Strengths:\n"
            for strength in evaluation.strengths:
                explanation += f"• {strength}\n"
            explanation += "\n"
        
        if evaluation.weaknesses:
            explanation += "Areas for Improvement:\n"
            for weakness in evaluation.weaknesses:
                explanation += f"• {weakness}\n"
            explanation += "\n"
        
        if evaluation.improvement_suggestions:
            explanation += "Recommendations:\n"
            for suggestion in evaluation.improvement_suggestions:
                explanation += f"• {suggestion}\n"
            explanation += "\n"
        
        if evaluation.criterion_scores:
            explanation += "Detailed Scores:\n"
            for criterion, score in evaluation.criterion_scores.items():
                explanation += f"• {criterion}: {score:.2f}\n"
        
        explanation += f"\nConfidence Level: {evaluation.confidence_level:.2f}"
        explanation += f"\nRubric Used: {evaluation.rubric_used or 'Default'}"
        
        return explanation
    
    async def calculate_confidence(self, evaluation: EvaluationResult) -> float:
        """Calculate confidence level for an evaluation result.
        
        Args:
            evaluation: Evaluation result
            
        Returns:
            Confidence level between 0.0 and 1.0
        """
        # Confidence is already calculated during evaluation
        return evaluation.confidence_level
    
    def get_evaluation_history(self) -> List[EvaluationResult]:
        """Get historical evaluation results.
        
        Returns:
            List of all evaluation results
        """
        return self.evaluation_history.copy()
    
    def get_performance_statistics(self) -> Dict[str, Any]:
        """Get performance statistics for the evaluator.
        
        Returns:
            Dictionary of performance statistics
        """
        if not self.evaluation_history:
            return {"total_evaluations": 0}
        
        scores = [eval_result.overall_score for eval_result in self.evaluation_history]
        confidences = [eval_result.confidence_level for eval_result in self.evaluation_history]
        
        return {
            "total_evaluations": len(self.evaluation_history),
            "average_score": sum(scores) / len(scores),
            "average_confidence": sum(confidences) / len(confidences),
            "score_distribution": {
                "excellent": len([s for s in scores if s >= 0.9]),
                "good": len([s for s in scores if 0.7 <= s < 0.9]),
                "acceptable": len([s for s in scores if 0.5 <= s < 0.7]),
                "poor": len([s for s in scores if s < 0.5])
            }
        }


================================================
FILE: dataqa/orchestration/planning/__init__.py
================================================
"""
Dynamic planning and replanning components.
"""

from .planner import AdaptivePlanner
from .replanning import ReplanningEngine
from .models import Plan, ExecutionStep, ReplanningEvent

__all__ = [
    "AdaptivePlanner",
    "ReplanningEngine",
    "Plan", 
    "ExecutionStep",
    "ReplanningEvent",
]


================================================
FILE: dataqa/orchestration/planning/execution_state.py
================================================
"""
Execution state management with intermediate result tracking.
"""

import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from ..models import ExecutionState, ExecutionStatus, ExecutionMetrics
from .models import ExecutionStep, IntermediateResult, Plan, ReplanningEvent

logger = logging.getLogger(__name__)


class ExecutionStateManager:
    """
    Manages execution state with intermediate result tracking.
    
    Provides centralized management of execution state, including step tracking,
    intermediate result storage, metrics collection, and state persistence.
    """
    
    def __init__(self):
        """Initialize the execution state manager."""
        self.logger = logging.getLogger(__name__)
    
    async def create_execution_state(
        self,
        session_id: str,
        plan: Plan
    ) -> ExecutionState:
        """
        Create a new execution state for a plan.
        
        Args:
            session_id: Unique session identifier
            plan: Execution plan to track
            
        Returns:
            New execution state
        """
        execution_state = ExecutionState(
            session_id=session_id,
            current_plan_id=plan.plan_id,
            status=ExecutionStatus.PENDING,
            started_at=datetime.utcnow()
        )
        
        # Initialize metrics based on plan
        execution_state.execution_metrics.total_steps = len(plan.steps)
        
        self.logger.info(f"Created execution state for session {session_id}")
        return execution_state
    
    async def update_step_status(
        self,
        execution_state: ExecutionState,
        step_id: str,
        status: ExecutionStatus,
        error_message: Optional[str] = None,
        outputs: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Update the status of an execution step.
        
        Args:
            execution_state: Current execution state
            step_id: ID of the step to update
            status: New status
            error_message: Error message if failed
            outputs: Step outputs if completed
        """
        # Find existing step or create new one
        step = None
        for existing_step in execution_state.completed_steps:
            if existing_step.step_id == step_id:
                step = existing_step
                break
        
        if not step:
            # Create new step record
            step = ExecutionStep(
                step_id=step_id,
                name=f"Step {step_id}",
                description=f"Execution step {step_id}",
                status=status
            )
            execution_state.completed_steps.append(step)
        
        # Update step details
        previous_status = step.status
        step.status = status
        
        if status == ExecutionStatus.RUNNING and not step.started_at:
            step.started_at = datetime.utcnow()
        elif status in [ExecutionStatus.COMPLETED, ExecutionStatus.FAILED]:
            step.completed_at = datetime.utcnow()
            if outputs:
                step.outputs.update(outputs)
            if error_message:
                step.error_message = error_message
        
        # Update execution metrics
        await self._update_execution_metrics(execution_state, step, previous_status)
        
        # Update overall execution state status
        await self._update_overall_status(execution_state)
        
        self.logger.info(f"Updated step {step_id} status: {previous_status} -> {status}")
    
    async def add_intermediate_result(
        self,
        execution_state: ExecutionState,
        step_id: str,
        result_type: str,
        data: Any,
        quality_score: Optional[float] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> IntermediateResult:
        """
        Add an intermediate result to the execution state.
        
        Args:
            execution_state: Current execution state
            step_id: ID of the step that produced the result
            result_type: Type of result
            data: Result data
            quality_score: Quality score (0.0 to 1.0)
            metadata: Additional metadata
            
        Returns:
            Created intermediate result
        """
        result = IntermediateResult(
            step_id=step_id,
            result_type=result_type,
            data=data,
            quality_score=quality_score,
            metadata=metadata or {}
        )
        
        # Store in execution state
        execution_state.intermediate_results[result.result_id] = {
            "result_id": result.result_id,
            "step_id": step_id,
            "result_type": result_type,
            "data": data,
            "quality_score": quality_score,
            "metadata": metadata or {},
            "created_at": result.created_at.isoformat(),
            "preserved": result.preserved
        }
        
        # Update quality metrics
        if quality_score is not None:
            execution_state.execution_metrics.quality_scores[result_type] = quality_score
        
        self.logger.info(f"Added intermediate result {result.result_id} for step {step_id}")
        return result
    
    async def get_intermediate_results(
        self,
        execution_state: ExecutionState,
        step_id: Optional[str] = None,
        result_type: Optional[str] = None
    ) -> List[IntermediateResult]:
        """
        Get intermediate results with optional filtering.
        
        Args:
            execution_state: Current execution state
            step_id: Filter by step ID
            result_type: Filter by result type
            
        Returns:
            List of matching intermediate results
        """
        results = []
        
        for result_data in execution_state.intermediate_results.values():
            # Apply filters
            if step_id and result_data.get("step_id") != step_id:
                continue
            if result_type and result_data.get("result_type") != result_type:
                continue
            
            # Convert back to IntermediateResult object
            result = IntermediateResult(
                result_id=result_data["result_id"],
                step_id=result_data["step_id"],
                result_type=result_data["result_type"],
                data=result_data["data"],
                quality_score=result_data.get("quality_score"),
                metadata=result_data.get("metadata", {}),
                created_at=datetime.fromisoformat(result_data["created_at"]),
                preserved=result_data.get("preserved", True)
            )
            results.append(result)
        
        return sorted(results, key=lambda r: r.created_at)
    
    async def record_replanning_event(
        self,
        execution_state: ExecutionState,
        event: ReplanningEvent
    ) -> None:
        """
        Record a replanning event in the execution state.
        
        Args:
            execution_state: Current execution state
            event: Replanning event to record
        """
        execution_state.replanning_history.append(event)
        execution_state.updated_at = datetime.utcnow()
        
        self.logger.info(f"Recorded replanning event {event.event_id}")
    
    async def get_execution_summary(
        self,
        execution_state: ExecutionState
    ) -> Dict[str, Any]:
        """
        Get a summary of the current execution state.
        
        Args:
            execution_state: Current execution state
            
        Returns:
            Execution summary
        """
        metrics = execution_state.execution_metrics
        
        # Calculate completion percentage
        completion_percentage = 0.0
        if metrics.total_steps > 0:
            completion_percentage = (metrics.completed_steps / metrics.total_steps) * 100
        
        # Calculate average quality score
        avg_quality = 0.0
        if metrics.quality_scores:
            avg_quality = sum(metrics.quality_scores.values()) / len(metrics.quality_scores)
        
        # Get recent intermediate results
        recent_results = await self.get_intermediate_results(execution_state)
        recent_results = recent_results[-5:]  # Last 5 results
        
        summary = {
            "session_id": execution_state.session_id,
            "status": execution_state.status.value,
            "completion_percentage": completion_percentage,
            "total_steps": metrics.total_steps,
            "completed_steps": metrics.completed_steps,
            "failed_steps": metrics.failed_steps,
            "average_quality_score": avg_quality,
            "total_execution_time": metrics.total_execution_time_seconds,
            "replanning_count": len(execution_state.replanning_history),
            "escalation_count": len(execution_state.escalation_points),
            "recent_results": [
                {
                    "result_type": r.result_type,
                    "quality_score": r.quality_score,
                    "created_at": r.created_at.isoformat()
                }
                for r in recent_results
            ],
            "started_at": execution_state.started_at.isoformat() if execution_state.started_at else None,
            "updated_at": execution_state.updated_at.isoformat()
        }
        
        return summary
    
    async def _update_execution_metrics(
        self,
        execution_state: ExecutionState,
        step: ExecutionStep,
        previous_status: ExecutionStatus
    ) -> None:
        """Update execution metrics based on step status change."""
        metrics = execution_state.execution_metrics
        
        # Update step counts
        if previous_status != ExecutionStatus.COMPLETED and step.status == ExecutionStatus.COMPLETED:
            metrics.completed_steps += 1
        elif previous_status != ExecutionStatus.FAILED and step.status == ExecutionStatus.FAILED:
            metrics.failed_steps += 1
        
        # Update timing metrics
        if step.started_at and step.completed_at:
            step_duration = (step.completed_at - step.started_at).total_seconds()
            metrics.total_execution_time_seconds += step_duration
            
            # Update average step time
            completed_steps = metrics.completed_steps + metrics.failed_steps
            if completed_steps > 0:
                metrics.average_step_time_seconds = (
                    metrics.total_execution_time_seconds / completed_steps
                )
        
        # Update resource utilization (placeholder - would be populated by actual monitoring)
        if step.outputs:
            if "cpu_usage" in step.outputs:
                metrics.resource_utilization["cpu"] = step.outputs["cpu_usage"]
            if "memory_usage" in step.outputs:
                metrics.resource_utilization["memory"] = step.outputs["memory_usage"]
    
    async def _update_overall_status(self, execution_state: ExecutionState) -> None:
        """Update the overall execution status based on step statuses."""
        metrics = execution_state.execution_metrics
        
        # Check if there are any running steps
        running_steps = sum(1 for step in execution_state.completed_steps 
                          if step.status == ExecutionStatus.RUNNING)
        
        # Determine overall status
        if metrics.failed_steps > 0 and metrics.completed_steps == 0 and running_steps == 0:
            execution_state.status = ExecutionStatus.FAILED
        elif metrics.completed_steps == metrics.total_steps:
            execution_state.status = ExecutionStatus.COMPLETED
        elif metrics.completed_steps > 0 or metrics.failed_steps > 0 or running_steps > 0:
            execution_state.status = ExecutionStatus.RUNNING
        else:
            execution_state.status = ExecutionStatus.PENDING
        
        execution_state.updated_at = datetime.utcnow()
    
    async def cleanup_old_results(
        self,
        execution_state: ExecutionState,
        max_results: int = 100,
        max_age_hours: int = 24
    ) -> int:
        """
        Clean up old intermediate results to manage memory usage.
        
        Args:
            execution_state: Current execution state
            max_results: Maximum number of results to keep
            max_age_hours: Maximum age of results in hours
            
        Returns:
            Number of results cleaned up
        """
        current_time = datetime.utcnow()
        results_to_remove = []
        
        # Convert to list for processing
        all_results = list(execution_state.intermediate_results.items())
        
        # Sort by creation time (oldest first)
        all_results.sort(key=lambda x: x[1].get("created_at", ""))
        
        # Remove old results
        for result_id, result_data in all_results:
            created_at_str = result_data.get("created_at")
            if created_at_str:
                created_at = datetime.fromisoformat(created_at_str)
                age_hours = (current_time - created_at).total_seconds() / 3600
                
                # Remove if too old or if we have too many results
                if (age_hours > max_age_hours or 
                    len(all_results) - len(results_to_remove) > max_results):
                    # Don't remove preserved results
                    if not result_data.get("preserved", True):
                        results_to_remove.append(result_id)
        
        # Remove identified results
        for result_id in results_to_remove:
            del execution_state.intermediate_results[result_id]
        
        if results_to_remove:
            self.logger.info(f"Cleaned up {len(results_to_remove)} old intermediate results")
        
        return len(results_to_remove)
    
    async def persist_state(
        self,
        execution_state: ExecutionState,
        storage_backend: Optional[Any] = None
    ) -> bool:
        """
        Persist execution state to storage backend.
        
        Args:
            execution_state: State to persist
            storage_backend: Storage backend (placeholder for future implementation)
            
        Returns:
            True if successful, False otherwise
        """
        # Placeholder for persistence implementation
        # In production, this would save to database, file system, etc.
        
        execution_state.updated_at = datetime.utcnow()
        self.logger.info(f"Persisted execution state for session {execution_state.session_id}")
        return True
    
    async def restore_state(
        self,
        session_id: str,
        storage_backend: Optional[Any] = None
    ) -> Optional[ExecutionState]:
        """
        Restore execution state from storage backend.
        
        Args:
            session_id: Session ID to restore
            storage_backend: Storage backend (placeholder for future implementation)
            
        Returns:
            Restored execution state or None if not found
        """
        # Placeholder for restoration implementation
        # In production, this would load from database, file system, etc.
        
        self.logger.info(f"Attempted to restore execution state for session {session_id}")
        return None


================================================
FILE: dataqa/orchestration/planning/models.py
================================================
"""
Planning-specific models and data structures.
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional
from uuid import uuid4

from pydantic import BaseModel, Field

from ..models import ExecutionStatus, CapabilityType


class ReplanningTriggerType(str, Enum):
    """Types of replanning triggers."""
    STEP_FAILURE = "step_failure"
    QUALITY_THRESHOLD = "quality_threshold"
    RESOURCE_CONSTRAINT = "resource_constraint"
    AGENT_UNAVAILABLE = "agent_unavailable"
    CONTEXT_CHANGE = "context_change"
    USER_REQUEST = "user_request"
    TIMEOUT = "timeout"
    DEPENDENCY_FAILURE = "dependency_failure"


class ContextPreservationStrategy(str, Enum):
    """Strategies for preserving context during replanning."""
    PRESERVE_ALL = "preserve_all"
    PRESERVE_SUCCESSFUL = "preserve_successful"
    PRESERVE_CRITICAL = "preserve_critical"
    MINIMAL_PRESERVATION = "minimal_preservation"


class ExecutionStep(BaseModel):
    """Individual step in an execution plan."""
    step_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    agent_id: Optional[str] = None
    capability_required: Optional[CapabilityType] = None
    inputs: Dict[str, Any] = Field(default_factory=dict)
    outputs: Dict[str, Any] = Field(default_factory=dict)
    status: ExecutionStatus = ExecutionStatus.PENDING
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    max_retries: int = 3
    success_criteria: List[str] = Field(default_factory=list)
    quality_threshold: Optional[float] = None
    timeout_seconds: Optional[int] = None
    
    class Config:
        use_enum_values = True


class IntermediateResult(BaseModel):
    """Intermediate result from an execution step."""
    result_id: str = Field(default_factory=lambda: str(uuid4()))
    step_id: str
    result_type: str
    data: Any
    quality_score: Optional[float] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    preserved: bool = True  # Whether to preserve during replanning


class Plan(BaseModel):
    """Execution plan for multi-agent workflow."""
    plan_id: str = Field(default_factory=lambda: str(uuid4()))
    name: str
    description: str
    version: int = 1
    steps: List[ExecutionStep] = Field(default_factory=list)
    dependencies: Dict[str, List[str]] = Field(default_factory=dict)  # step_id -> [prerequisite_step_ids]
    estimated_duration_minutes: Optional[int] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    parent_plan_id: Optional[str] = None  # For replanned versions
    replanning_context: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ReplanningTrigger(BaseModel):
    """Trigger condition for replanning."""
    trigger_id: str = Field(default_factory=lambda: str(uuid4()))
    trigger_type: ReplanningTriggerType
    condition: str  # Condition expression
    description: str
    priority: int = 1  # 1 = highest priority
    enabled: bool = True
    
    class Config:
        use_enum_values = True


class ReplanningEvent(BaseModel):
    """Event that triggered replanning."""
    event_id: str = Field(default_factory=lambda: str(uuid4()))
    trigger_type: ReplanningTriggerType
    trigger_description: str
    occurred_at: datetime = Field(default_factory=datetime.utcnow)
    context: Dict[str, Any] = Field(default_factory=dict)
    replanning_successful: bool = False
    new_plan_id: Optional[str] = None
    preserved_results: List[str] = Field(default_factory=list)  # result_ids
    
    class Config:
        use_enum_values = True


class ExecutionContext(BaseModel):
    """Context for plan execution and replanning."""
    context_id: str = Field(default_factory=lambda: str(uuid4()))
    user_query: str
    available_agents: List[str] = Field(default_factory=list)  # agent_ids
    agent_capabilities: Dict[str, List[CapabilityType]] = Field(default_factory=dict)
    domain_constraints: Dict[str, Any] = Field(default_factory=dict)
    quality_requirements: Dict[str, float] = Field(default_factory=dict)
    resource_limits: Dict[str, Any] = Field(default_factory=dict)
    timeout_seconds: Optional[int] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)


================================================
FILE: dataqa/orchestration/planning/planner.py
================================================
"""
Adaptive planner for dynamic multi-agent workflow planning.
"""

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set

from ..models import CapabilityType, ExecutionStatus
from .models import (
    ExecutionContext,
    ExecutionStep,
    IntermediateResult,
    Plan,
    ReplanningEvent,
    ReplanningTriggerType,
)

logger = logging.getLogger(__name__)


class PlanningStrategy:
    """Base class for planning strategies."""
    
    def estimate_step_duration(self, step: ExecutionStep, context: ExecutionContext) -> int:
        """Estimate duration for a step in minutes."""
        # Default estimation based on capability type
        duration_map = {
            CapabilityType.DATA_RETRIEVAL: 2,
            CapabilityType.DATA_ANALYSIS: 5,
            CapabilityType.VISUALIZATION: 3,
            CapabilityType.CODE_GENERATION: 4,
            CapabilityType.DOMAIN_EXPERTISE: 3,
            CapabilityType.COORDINATION: 1,
            CapabilityType.APPROVAL: 10,  # Human approval takes longer
        }
        return duration_map.get(step.capability_required, 3)
    
    def validate_dependencies(self, steps: List[ExecutionStep], dependencies: Dict[str, List[str]]) -> bool:
        """Validate that dependencies form a valid DAG."""
        step_ids = {step.step_id for step in steps}
        
        # Check all dependency references exist
        for step_id, deps in dependencies.items():
            if step_id not in step_ids:
                return False
            for dep in deps:
                if dep not in step_ids:
                    return False
        
        # Check for cycles using DFS
        visited = set()
        rec_stack = set()
        
        def has_cycle(node: str) -> bool:
            if node in rec_stack:
                return True
            if node in visited:
                return False
            
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in dependencies.get(node, []):
                if has_cycle(neighbor):
                    return True
            
            rec_stack.remove(node)
            return False
        
        for step_id in step_ids:
            if step_id not in visited:
                if has_cycle(step_id):
                    return False
        
        return True


class AdaptivePlanner:
    """
    Adaptive planner for generating and modifying execution plans based on agent capabilities.
    
    Generates initial plans by analyzing user queries, available agent capabilities,
    and domain constraints to create optimal execution workflows.
    """
    
    def __init__(self, strategy: Optional[PlanningStrategy] = None):
        """Initialize the adaptive planner."""
        self.strategy = strategy or PlanningStrategy()
        self.logger = logging.getLogger(__name__)
    
    async def generate_plan(
        self,
        query: str,
        context: ExecutionContext,
        preserve_results: Optional[List[IntermediateResult]] = None
    ) -> Plan:
        """
        Generate an initial execution plan based on query and available capabilities.
        
        Args:
            query: User query to execute
            context: Execution context with available agents and constraints
            preserve_results: Previous results to preserve during replanning
            
        Returns:
            Generated execution plan
        """
        self.logger.info(f"Generating plan for query: {query}")
        
        # Analyze query to determine required capabilities
        required_capabilities = await self._analyze_query_requirements(query, context)
        
        # Generate execution steps based on capabilities
        steps = await self._generate_execution_steps(required_capabilities, context, preserve_results)
        
        # Build dependency graph
        dependencies = await self._build_dependency_graph(steps, context)
        
        # Validate the plan
        if not self.strategy.validate_dependencies(steps, dependencies):
            raise ValueError("Generated plan contains invalid dependencies")
        
        # Calculate estimated duration
        total_duration = sum(
            self.strategy.estimate_step_duration(step, context) for step in steps
        )
        
        plan = Plan(
            name=f"Plan for: {query[:50]}...",
            description=f"Execution plan for query: {query}",
            steps=steps,
            dependencies=dependencies,
            estimated_duration_minutes=total_duration,
            metadata={
                "query": query,
                "required_capabilities": [cap.value for cap in required_capabilities],
                "agent_count": len(context.available_agents),
                "preserved_results": len(preserve_results) if preserve_results else 0,
            }
        )
        
        self.logger.info(f"Generated plan {plan.plan_id} with {len(steps)} steps")
        return plan
    
    async def _analyze_query_requirements(
        self, 
        query: str, 
        context: ExecutionContext
    ) -> List[CapabilityType]:
        """Analyze query to determine required capabilities."""
        required_capabilities = []
        query_lower = query.lower()
        
        # Simple keyword-based analysis (in production, use NLP/LLM)
        capability_keywords = {
            CapabilityType.DATA_RETRIEVAL: ["get", "fetch", "retrieve", "load", "find", "search", "data"],
            CapabilityType.DATA_ANALYSIS: ["analyze", "calculate", "compute", "aggregate", "summarize", "trends"],
            CapabilityType.VISUALIZATION: ["plot", "chart", "graph", "visualize", "show", "create", "visualization"],
            CapabilityType.CODE_GENERATION: ["generate", "write", "code"],
            CapabilityType.DOMAIN_EXPERTISE: ["explain", "interpret", "validate", "recommend"],
        }
        
        for capability, keywords in capability_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                required_capabilities.append(capability)
        
        # Ensure we have at least data retrieval for most queries
        if not required_capabilities:
            required_capabilities.append(CapabilityType.DATA_RETRIEVAL)
        
        # Add coordination if multiple capabilities needed
        if len(required_capabilities) > 1:
            required_capabilities.append(CapabilityType.COORDINATION)
        
        return required_capabilities
    
    async def _generate_execution_steps(
        self,
        required_capabilities: List[CapabilityType],
        context: ExecutionContext,
        preserve_results: Optional[List[IntermediateResult]] = None
    ) -> List[ExecutionStep]:
        """Generate execution steps for required capabilities."""
        steps = []
        
        # Add preserved results as completed steps if replanning
        if preserve_results:
            for result in preserve_results:
                if result.preserved:
                    step = ExecutionStep(
                        step_id=result.step_id,
                        name=f"Preserved: {result.result_type}",
                        description=f"Preserved result from previous execution",
                        status=ExecutionStatus.COMPLETED,
                        completed_at=result.created_at,
                        outputs={"preserved_data": result.data}
                    )
                    steps.append(step)
        
        # Generate new steps for required capabilities
        for i, capability in enumerate(required_capabilities):
            # Find suitable agent for this capability
            suitable_agent = await self._find_suitable_agent(capability, context)
            
            step = ExecutionStep(
                name=f"{capability.value.replace('_', ' ').title()} Step",
                description=f"Execute {capability.value} using {suitable_agent or 'available agent'}",
                agent_id=suitable_agent,
                capability_required=capability,
                success_criteria=await self._generate_success_criteria(capability),
                quality_threshold=context.quality_requirements.get(capability.value, 0.8),
                timeout_seconds=context.timeout_seconds or 300,
            )
            steps.append(step)
        
        return steps
    
    async def _find_suitable_agent(
        self, 
        capability: CapabilityType, 
        context: ExecutionContext
    ) -> Optional[str]:
        """Find the most suitable agent for a capability."""
        suitable_agents = []
        
        for agent_id in context.available_agents:
            agent_capabilities = context.agent_capabilities.get(agent_id, [])
            if capability in agent_capabilities:
                suitable_agents.append(agent_id)
        
        # Return first suitable agent (in production, use more sophisticated selection)
        return suitable_agents[0] if suitable_agents else None
    
    async def _generate_success_criteria(self, capability: CapabilityType) -> List[str]:
        """Generate success criteria for a capability."""
        criteria_map = {
            CapabilityType.DATA_RETRIEVAL: [
                "Data successfully retrieved",
                "No data corruption detected",
                "Response time within limits"
            ],
            CapabilityType.DATA_ANALYSIS: [
                "Analysis completed without errors",
                "Results meet quality threshold",
                "Statistical validity confirmed"
            ],
            CapabilityType.VISUALIZATION: [
                "Visualization generated successfully",
                "Chart displays correctly",
                "Data accurately represented"
            ],
            CapabilityType.CODE_GENERATION: [
                "Code generated without syntax errors",
                "Code passes basic validation",
                "Generated code is executable"
            ],
            CapabilityType.DOMAIN_EXPERTISE: [
                "Domain rules applied correctly",
                "Recommendations are valid",
                "Compliance requirements met"
            ],
            CapabilityType.COORDINATION: [
                "All sub-tasks coordinated",
                "Communication successful",
                "Dependencies resolved"
            ],
            CapabilityType.APPROVAL: [
                "Approval request submitted",
                "Response received within timeout",
                "Decision properly recorded"
            ],
        }
        
        return criteria_map.get(capability, ["Task completed successfully"])
    
    async def _build_dependency_graph(
        self,
        steps: List[ExecutionStep],
        context: ExecutionContext
    ) -> Dict[str, List[str]]:
        """Build dependency graph between execution steps."""
        dependencies = {}
        
        # Simple sequential dependencies for now
        # In production, analyze actual data flow dependencies
        for i, step in enumerate(steps):
            if i > 0:
                # Each step depends on the previous one
                dependencies[step.step_id] = [steps[i-1].step_id]
            else:
                dependencies[step.step_id] = []
        
        return dependencies
    
    async def update_plan_with_results(
        self,
        plan: Plan,
        completed_steps: List[ExecutionStep],
        intermediate_results: List[IntermediateResult]
    ) -> Plan:
        """Update plan with completed steps and intermediate results."""
        # Update step statuses
        completed_step_ids = {step.step_id for step in completed_steps}
        
        for step in plan.steps:
            if step.step_id in completed_step_ids:
                completed_step = next(s for s in completed_steps if s.step_id == step.step_id)
                step.status = completed_step.status
                step.started_at = completed_step.started_at
                step.completed_at = completed_step.completed_at
                step.error_message = completed_step.error_message
                step.outputs = completed_step.outputs
        
        # Add intermediate results to metadata
        plan.metadata["intermediate_results"] = [
            {
                "result_id": result.result_id,
                "step_id": result.step_id,
                "result_type": result.result_type,
                "quality_score": result.quality_score,
            }
            for result in intermediate_results
        ]
        
        return plan


================================================
FILE: dataqa/orchestration/planning/replanning.py
================================================
"""
Replanning engine for dynamic plan adaptation.
"""

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from ..models import ExecutionState, ExecutionStatus
from .models import (
    ContextPreservationStrategy,
    ExecutionContext,
    ExecutionStep,
    IntermediateResult,
    Plan,
    ReplanningEvent,
    ReplanningTrigger,
    ReplanningTriggerType,
)
from .planner import AdaptivePlanner

logger = logging.getLogger(__name__)


class ReplanningEngine:
    """
    Engine for detecting replanning triggers and generating revised plans.
    
    Monitors execution state, detects when replanning is needed based on configurable
    triggers, and generates revised plans while preserving successful intermediate results.
    """
    
    def __init__(
        self,
        max_replanning_iterations: int = 3,
        context_preservation_strategy: ContextPreservationStrategy = ContextPreservationStrategy.PRESERVE_SUCCESSFUL,
        planner: Optional[AdaptivePlanner] = None
    ):
        """
        Initialize the replanning engine.
        
        Args:
            max_replanning_iterations: Maximum number of replanning attempts
            context_preservation_strategy: Strategy for preserving context during replanning
            planner: Adaptive planner instance for generating revised plans
        """
        self.max_replanning_iterations = max_replanning_iterations
        self.context_preservation_strategy = context_preservation_strategy
        self.planner = planner or AdaptivePlanner()
        self.replanning_triggers: List[ReplanningTrigger] = self._initialize_default_triggers()
        self.logger = logging.getLogger(__name__)
    
    def _initialize_default_triggers(self) -> List[ReplanningTrigger]:
        """Initialize default replanning triggers."""
        return [
            ReplanningTrigger(
                trigger_type=ReplanningTriggerType.STEP_FAILURE,
                condition="step.status == 'failed' and step.retry_count >= step.max_retries",
                description="Step failed after maximum retries",
                priority=1
            ),
            ReplanningTrigger(
                trigger_type=ReplanningTriggerType.QUALITY_THRESHOLD,
                condition="result.quality_score < step.quality_threshold",
                description="Result quality below threshold",
                priority=2
            ),
            ReplanningTrigger(
                trigger_type=ReplanningTriggerType.AGENT_UNAVAILABLE,
                condition="assigned_agent.status == 'unavailable'",
                description="Assigned agent became unavailable",
                priority=1
            ),
            ReplanningTrigger(
                trigger_type=ReplanningTriggerType.TIMEOUT,
                condition="execution_time > step.timeout_seconds",
                description="Step execution timeout exceeded",
                priority=2
            ),
            ReplanningTrigger(
                trigger_type=ReplanningTriggerType.RESOURCE_CONSTRAINT,
                condition="available_resources < required_resources",
                description="Insufficient resources available",
                priority=3
            ),
        ]
    
    async def should_replan(
        self,
        execution_state: ExecutionState,
        current_plan: Plan,
        context: ExecutionContext
    ) -> Optional[ReplanningTriggerType]:
        """
        Determine if replanning is needed based on execution state and triggers.
        
        Args:
            execution_state: Current execution state
            current_plan: Current execution plan
            context: Execution context
            
        Returns:
            Trigger type if replanning needed, None otherwise
        """
        # Check if we've exceeded max replanning iterations
        if len(execution_state.replanning_history) >= self.max_replanning_iterations:
            self.logger.warning(f"Maximum replanning iterations ({self.max_replanning_iterations}) reached")
            return None
        
        # Evaluate each trigger
        for trigger in sorted(self.replanning_triggers, key=lambda t: t.priority):
            if not trigger.enabled:
                continue
            
            if await self._evaluate_trigger(trigger, execution_state, current_plan, context):
                self.logger.info(f"Replanning triggered by: {trigger.description}")
                return trigger.trigger_type
        
        return None
    
    async def _evaluate_trigger(
        self,
        trigger: ReplanningTrigger,
        execution_state: ExecutionState,
        current_plan: Plan,
        context: ExecutionContext
    ) -> bool:
        """Evaluate a specific replanning trigger."""
        try:
            # Simple condition evaluation based on trigger type
            if trigger.trigger_type == ReplanningTriggerType.STEP_FAILURE:
                return await self._check_step_failures(execution_state, current_plan)
            
            elif trigger.trigger_type == ReplanningTriggerType.QUALITY_THRESHOLD:
                return await self._check_quality_thresholds(execution_state, current_plan)
            
            elif trigger.trigger_type == ReplanningTriggerType.AGENT_UNAVAILABLE:
                return await self._check_agent_availability(execution_state, current_plan, context)
            
            elif trigger.trigger_type == ReplanningTriggerType.TIMEOUT:
                return await self._check_timeouts(execution_state, current_plan)
            
            elif trigger.trigger_type == ReplanningTriggerType.RESOURCE_CONSTRAINT:
                return await self._check_resource_constraints(execution_state, current_plan, context)
            
            elif trigger.trigger_type == ReplanningTriggerType.CONTEXT_CHANGE:
                return await self._check_context_changes(execution_state, current_plan, context)
            
            return False
            
        except Exception as e:
            self.logger.error(f"Error evaluating trigger {trigger.trigger_type}: {e}")
            return False
    
    async def _check_step_failures(self, execution_state: ExecutionState, current_plan: Plan) -> bool:
        """Check for step failures that require replanning."""
        for step in execution_state.completed_steps:
            if (step.status == ExecutionStatus.FAILED and 
                step.retry_count >= step.max_retries):
                return True
        return False
    
    async def _check_quality_thresholds(self, execution_state: ExecutionState, current_plan: Plan) -> bool:
        """Check if quality thresholds are not being met."""
        # Check intermediate results quality
        for result_data in execution_state.intermediate_results.values():
            if isinstance(result_data, dict) and "quality_score" in result_data:
                quality_score = result_data["quality_score"]
                threshold = result_data.get("quality_threshold", 0.8)
                if quality_score < threshold:
                    return True
        return False
    
    async def _check_agent_availability(
        self, 
        execution_state: ExecutionState, 
        current_plan: Plan, 
        context: ExecutionContext
    ) -> bool:
        """Check if assigned agents are still available."""
        # Check if any assigned agents are no longer available
        for step in current_plan.steps:
            if (step.agent_id and 
                step.status in [ExecutionStatus.PENDING, ExecutionStatus.RUNNING] and
                step.agent_id not in context.available_agents):
                return True
        return False
    
    async def _check_timeouts(self, execution_state: ExecutionState, current_plan: Plan) -> bool:
        """Check for step timeouts."""
        current_time = datetime.utcnow()
        
        for step in current_plan.steps:
            if (step.status == ExecutionStatus.RUNNING and 
                step.started_at):
                # Check if step has timeout_seconds attribute (from planning models)
                timeout_seconds = getattr(step, 'timeout_seconds', None)
                if timeout_seconds:
                    elapsed = (current_time - step.started_at).total_seconds()
                    if elapsed > timeout_seconds:
                        return True
        return False
    
    async def _check_resource_constraints(
        self, 
        execution_state: ExecutionState, 
        current_plan: Plan, 
        context: ExecutionContext
    ) -> bool:
        """Check for resource constraint violations."""
        # Simple check based on resource limits in context
        if context.resource_limits:
            # Check if we're approaching resource limits
            current_usage = execution_state.execution_metrics.resource_utilization
            for resource, limit in context.resource_limits.items():
                if current_usage.get(resource, 0) > limit * 0.9:  # 90% threshold
                    return True
        return False
    
    async def _check_context_changes(
        self, 
        execution_state: ExecutionState, 
        current_plan: Plan, 
        context: ExecutionContext
    ) -> bool:
        """Check for significant context changes."""
        # Check if available agents have changed significantly
        plan_agents = {step.agent_id for step in current_plan.steps if step.agent_id}
        available_agents = set(context.available_agents)
        
        # If more than 50% of assigned agents are unavailable, trigger replanning
        if plan_agents:
            unavailable_ratio = len(plan_agents - available_agents) / len(plan_agents)
            if unavailable_ratio > 0.5:
                return True
        
        return False
    
    async def generate_revised_plan(
        self,
        current_plan: Plan,
        execution_state: ExecutionState,
        context: ExecutionContext,
        trigger_type: ReplanningTriggerType
    ) -> Plan:
        """
        Generate a revised execution plan based on current state and trigger.
        
        Args:
            current_plan: Current execution plan
            execution_state: Current execution state
            context: Execution context
            trigger_type: Type of trigger that caused replanning
            
        Returns:
            Revised execution plan
        """
        self.logger.info(f"Generating revised plan due to {trigger_type}")
        
        # Preserve intermediate results based on strategy
        preserved_results = await self._preserve_intermediate_results(
            execution_state, current_plan
        )
        
        # Update context based on current state
        updated_context = await self._update_context_for_replanning(
            context, execution_state, trigger_type
        )
        
        # Generate new plan with preserved results
        revised_plan = await self.planner.generate_plan(
            query=current_plan.metadata.get("query", "Revised execution"),
            context=updated_context,
            preserve_results=preserved_results
        )
        
        # Set plan relationships
        revised_plan.parent_plan_id = current_plan.plan_id
        revised_plan.version = current_plan.version + 1
        revised_plan.replanning_context = {
            "trigger_type": trigger_type.value,
            "preserved_results_count": len(preserved_results),
            "original_plan_id": current_plan.plan_id,
            "replanning_iteration": len(execution_state.replanning_history) + 1,
        }
        
        self.logger.info(f"Generated revised plan {revised_plan.plan_id} (v{revised_plan.version})")
        return revised_plan
    
    async def _preserve_intermediate_results(
        self,
        execution_state: ExecutionState,
        current_plan: Plan
    ) -> List[IntermediateResult]:
        """Preserve intermediate results based on preservation strategy."""
        preserved_results = []
        
        # Convert execution state data to IntermediateResult objects
        for step in execution_state.completed_steps:
            if step.status == ExecutionStatus.COMPLETED:
                should_preserve = False
                
                if self.context_preservation_strategy == ContextPreservationStrategy.PRESERVE_ALL:
                    should_preserve = True
                elif self.context_preservation_strategy == ContextPreservationStrategy.PRESERVE_SUCCESSFUL:
                    should_preserve = step.status == ExecutionStatus.COMPLETED
                elif self.context_preservation_strategy == ContextPreservationStrategy.PRESERVE_CRITICAL:
                    # Preserve steps marked as critical or high-quality results
                    should_preserve = (
                        step.outputs.get("critical", False) or
                        step.outputs.get("quality_score", 0) > 0.9
                    )
                elif self.context_preservation_strategy == ContextPreservationStrategy.MINIMAL_PRESERVATION:
                    # Only preserve final outputs
                    should_preserve = step.outputs.get("final_output", False)
                
                if should_preserve:
                    result = IntermediateResult(
                        step_id=step.step_id,
                        result_type=step.name,
                        data=step.outputs,
                        quality_score=step.outputs.get("quality_score"),
                        metadata={"preserved_from": current_plan.plan_id},
                        created_at=step.completed_at or datetime.utcnow(),
                        preserved=True
                    )
                    preserved_results.append(result)
        
        self.logger.info(f"Preserved {len(preserved_results)} intermediate results")
        return preserved_results
    
    async def _update_context_for_replanning(
        self,
        original_context: ExecutionContext,
        execution_state: ExecutionState,
        trigger_type: ReplanningTriggerType
    ) -> ExecutionContext:
        """Update execution context for replanning."""
        # Create updated context with current state
        updated_context = ExecutionContext(
            context_id=original_context.context_id,
            user_query=original_context.user_query,
            available_agents=original_context.available_agents.copy(),
            agent_capabilities=original_context.agent_capabilities.copy(),
            domain_constraints=original_context.domain_constraints.copy(),
            quality_requirements=original_context.quality_requirements.copy(),
            resource_limits=original_context.resource_limits.copy(),
            timeout_seconds=original_context.timeout_seconds,
            metadata=original_context.metadata.copy()
        )
        
        # Adjust context based on trigger type
        if trigger_type == ReplanningTriggerType.AGENT_UNAVAILABLE:
            # Remove unavailable agents
            failed_agents = {
                step.agent_id for step in execution_state.completed_steps
                if step.status == ExecutionStatus.FAILED and step.agent_id
            }
            updated_context.available_agents = [
                agent_id for agent_id in updated_context.available_agents
                if agent_id not in failed_agents
            ]
        
        elif trigger_type == ReplanningTriggerType.QUALITY_THRESHOLD:
            # Increase quality requirements
            for key in updated_context.quality_requirements:
                updated_context.quality_requirements[key] = min(
                    updated_context.quality_requirements[key] + 0.1, 1.0
                )
        
        elif trigger_type == ReplanningTriggerType.TIMEOUT:
            # Increase timeout allowances
            if updated_context.timeout_seconds:
                updated_context.timeout_seconds = int(updated_context.timeout_seconds * 1.5)
        
        # Add replanning metadata
        updated_context.metadata["replanning_trigger"] = trigger_type.value
        updated_context.metadata["replanning_iteration"] = len(execution_state.replanning_history) + 1
        
        return updated_context
    
    async def record_replanning_event(
        self,
        execution_state: ExecutionState,
        trigger_type: ReplanningTriggerType,
        old_plan: Plan,
        new_plan: Plan,
        context: Dict[str, Any]
    ) -> ReplanningEvent:
        """Record a replanning event in the execution state."""
        event = ReplanningEvent(
            trigger_type=trigger_type,
            trigger_description=f"Replanning triggered by {trigger_type.value}",
            context=context,
            replanning_successful=True,
            new_plan_id=new_plan.plan_id,
            preserved_results=[
                result.get("result_id") for result in new_plan.metadata.get("intermediate_results", [])
            ]
        )
        
        execution_state.replanning_history.append(event)
        self.logger.info(f"Recorded replanning event {event.event_id}")
        return event
    
    def add_custom_trigger(self, trigger: ReplanningTrigger) -> None:
        """Add a custom replanning trigger."""
        self.replanning_triggers.append(trigger)
        self.logger.info(f"Added custom trigger: {trigger.description}")
    
    def remove_trigger(self, trigger_type: ReplanningTriggerType) -> None:
        """Remove triggers of a specific type."""
        self.replanning_triggers = [
            t for t in self.replanning_triggers if t.trigger_type != trigger_type
        ]
        self.logger.info(f"Removed triggers of type: {trigger_type}")
    
    def set_max_iterations(self, max_iterations: int) -> None:
        """Set maximum replanning iterations."""
        self.max_replanning_iterations = max_iterations
        self.logger.info(f"Set max replanning iterations to: {max_iterations}")
    
    def set_preservation_strategy(self, strategy: ContextPreservationStrategy) -> None:
        """Set context preservation strategy."""
        self.context_preservation_strategy = strategy
        self.logger.info(f"Set preservation strategy to: {strategy}")


================================================
FILE: dataqa/primitives/__init__.py
================================================
"""Core primitive interfaces for DataQA components."""

from .knowledge import KnowledgePrimitive
from .executor import ExecutorPrimitive
from .llm import LLMInterface
from .in_memory_executor import InMemoryExecutor
from .faiss_knowledge import FAISSKnowledge

__all__ = ["KnowledgePrimitive", "ExecutorPrimitive", "LLMInterface", "InMemoryExecutor", "FAISSKnowledge"]


================================================
FILE: dataqa/primitives/executor.py
================================================
"""Abstract base class for executor primitive implementations."""

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

from ..exceptions import ExecutionError
from ..models.execution import ExecutionResult


class ExecutorPrimitive(ABC):
    """Abstract base class for code execution environments.
    
    This interface defines the contract for components that execute
    generated SQL and Python code in secure, controlled environments.
    Implementations might use in-memory databases, remote APIs,
    or containerized execution environments.
    """
    
    @abstractmethod
    async def execute_sql(
        self, 
        sql: str, 
        parameters: Optional[Dict[str, Any]] = None
    ) -> ExecutionResult:
        """Execute SQL code and return results.
        
        Args:
            sql: The SQL query to execute
            parameters: Optional parameters for parameterized queries
            
        Returns:
            ExecutionResult containing the query results or error information
            
        Raises:
            ExecutionError: If execution fails due to system issues
        """
        pass
    
    @abstractmethod
    async def execute_python(
        self, 
        code: str, 
        context: Optional[Dict[str, Any]] = None
    ) -> ExecutionResult:
        """Execute Python code and return results.
        
        Args:
            code: The Python code to execute
            context: Optional context variables to make available to the code
            
        Returns:
            ExecutionResult containing the execution results or error information
            
        Raises:
            ExecutionError: If execution fails due to system issues
        """
        pass
    
    @abstractmethod
    async def get_schema(self, table_name: Optional[str] = None) -> Dict[str, Any]:
        """Get database schema information.
        
        Args:
            table_name: Optional specific table name, if None returns all tables
            
        Returns:
            Dictionary containing schema information (tables, columns, types, etc.)
            
        Raises:
            ExecutionError: If schema retrieval fails
        """
        pass
    
    @abstractmethod
    async def list_tables(self) -> List[str]:
        """List all available tables in the database.
        
        Returns:
            List of table names
            
        Raises:
            ExecutionError: If table listing fails
        """
        pass
    
    @abstractmethod
    async def get_sample_data(
        self, 
        table_name: str, 
        limit: int = 5
    ) -> ExecutionResult:
        """Get sample data from a table.
        
        Args:
            table_name: Name of the table to sample
            limit: Maximum number of rows to return
            
        Returns:
            ExecutionResult containing sample data
            
        Raises:
            ExecutionError: If sampling fails
        """
        pass
    
    @abstractmethod
    async def validate_code(self, code: str, code_type: str) -> bool:
        """Validate code before execution for security and syntax.
        
        Args:
            code: The code to validate
            code_type: Type of code ('sql' or 'python')
            
        Returns:
            True if code is valid and safe to execute
            
        Raises:
            ExecutionError: If validation fails
        """
        pass
    
    @abstractmethod
    async def generate_visualization(
        self, 
        data: Any, 
        chart_type: str = "auto",
        **kwargs
    ) -> ExecutionResult:
        """Generate a visualization based on data characteristics.
        
        Args:
            data: Data to visualize (typically a pandas DataFrame)
            chart_type: Type of chart to generate ('auto', 'bar', 'line', 'scatter', 'histogram', 'box', 'heatmap')
            **kwargs: Additional parameters for customization
            
        Returns:
            ExecutionResult containing the generated plot
            
        Raises:
            ExecutionError: If visualization generation fails
        """
        pass


================================================
FILE: dataqa/primitives/faiss_knowledge.py
================================================
"""FAISS-based knowledge primitive implementation."""

import json
import os
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

from ..exceptions import KnowledgeError
from ..models.document import Document
from .knowledge import KnowledgePrimitive


class FAISSKnowledge(KnowledgePrimitive):
    """FAISS-based knowledge retrieval system.
    
    This implementation uses FAISS for efficient vector similarity search
    and sentence-transformers for generating embeddings. It supports
    persistence to disk for loading pre-built knowledge bases.
    """
    
    def __init__(
        self,
        model_name: str = "all-MiniLM-L6-v2",
        index_path: Optional[Union[str, Path]] = None,
        embedding_dim: Optional[int] = None,
    ):
        """Initialize the FAISS knowledge primitive.
        
        Args:
            model_name: Name of the sentence-transformers model to use
            index_path: Path to save/load the FAISS index and metadata
            embedding_dim: Dimension of embeddings (auto-detected if None)
        """
        self.model_name = model_name
        self.index_path = Path(index_path) if index_path else None
        self._embedding_model: Optional[SentenceTransformer] = None
        self._index: Optional[faiss.Index] = None
        self._documents: List[Document] = []
        self._embedding_dim = embedding_dim
        
    @property
    def embedding_model(self) -> SentenceTransformer:
        """Lazy load the sentence transformer model."""
        if self._embedding_model is None:
            try:
                self._embedding_model = SentenceTransformer(self.model_name)
            except Exception as e:
                raise KnowledgeError(f"Failed to load embedding model '{self.model_name}': {e}")
        return self._embedding_model
    
    @property
    def index(self) -> faiss.Index:
        """Get or create the FAISS index."""
        if self._index is None:
            if self._embedding_dim is None:
                # Get embedding dimension from model
                self._embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            
            # Create a flat L2 index for simplicity
            self._index = faiss.IndexFlatL2(self._embedding_dim)
            
        return self._index
    
    async def ingest(self, documents: List[Document]) -> None:
        """Ingest documents into the knowledge base.
        
        Args:
            documents: List of documents to add to the knowledge base
            
        Raises:
            KnowledgeError: If ingestion fails
        """
        if not documents:
            return
            
        try:
            # Generate embeddings for documents that don't have them
            texts_to_embed = []
            doc_indices = []
            
            for i, doc in enumerate(documents):
                if doc.embedding is None:
                    texts_to_embed.append(doc.content)
                    doc_indices.append(i)
            
            if texts_to_embed:
                embeddings = self.embedding_model.encode(
                    texts_to_embed,
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
                
                # Update documents with embeddings
                for i, embedding in zip(doc_indices, embeddings):
                    documents[i].embedding = embedding.tolist()
            
            # Add embeddings to FAISS index
            embeddings_array = np.array([
                doc.embedding for doc in documents
            ], dtype=np.float32)
            
            self.index.add(embeddings_array)
            
            # Store documents
            self._documents.extend(documents)
            
        except Exception as e:
            raise KnowledgeError(f"Failed to ingest documents: {e}")
    
    async def search(
        self, 
        query: str, 
        limit: int = 5,
        filters: Optional[dict] = None
    ) -> List[Document]:
        """Search for relevant documents based on a query.
        
        Args:
            query: The search query string
            limit: Maximum number of documents to return
            filters: Optional filters to apply to the search (not implemented yet)
            
        Returns:
            List of relevant documents ordered by relevance
            
        Raises:
            KnowledgeError: If search fails
        """
        if not self._documents:
            return []
            
        try:
            # Generate embedding for query
            query_embedding = self.embedding_model.encode(
                [query],
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            
            # Search in FAISS index
            distances, indices = self.index.search(
                query_embedding.astype(np.float32), 
                min(limit, len(self._documents))
            )
            
            # Return documents ordered by relevance
            results = []
            for idx in indices[0]:
                if idx != -1:  # FAISS returns -1 for empty slots
                    results.append(self._documents[idx])
            
            # Apply filters if provided (basic implementation)
            if filters:
                filtered_results = []
                for doc in results:
                    match = True
                    for key, value in filters.items():
                        if key not in doc.metadata or doc.metadata[key] != value:
                            match = False
                            break
                    if match:
                        filtered_results.append(doc)
                results = filtered_results
            
            # Apply limit after filtering
            return results[:limit]
            
        except Exception as e:
            raise KnowledgeError(f"Failed to search documents: {e}")
    
    async def update(self, document_id: str, document: Document) -> None:
        """Update an existing document in the knowledge base.
        
        Args:
            document_id: Unique identifier for the document
            document: Updated document content
            
        Raises:
            KnowledgeError: If update fails or document not found
        """
        # Find document by ID (using source as ID for now)
        doc_index = None
        for i, doc in enumerate(self._documents):
            if doc.source == document_id:
                doc_index = i
                break
        
        if doc_index is None:
            raise KnowledgeError(f"Document with ID '{document_id}' not found")
        
        try:
            # Generate new embedding if content changed
            if document.embedding is None:
                embedding = self.embedding_model.encode(
                    [document.content],
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
                document.embedding = embedding[0].tolist()
            
            # Update document in list
            self._documents[doc_index] = document
            
            # Rebuild index (for simplicity - could be optimized)
            await self._rebuild_index()
            
        except Exception as e:
            raise KnowledgeError(f"Failed to update document '{document_id}': {e}")
    
    async def delete(self, document_id: str) -> None:
        """Delete a document from the knowledge base.
        
        Args:
            document_id: Unique identifier for the document to delete
            
        Raises:
            KnowledgeError: If deletion fails or document not found
        """
        # Find document by ID (using source as ID for now)
        doc_index = None
        for i, doc in enumerate(self._documents):
            if doc.source == document_id:
                doc_index = i
                break
        
        if doc_index is None:
            raise KnowledgeError(f"Document with ID '{document_id}' not found")
        
        try:
            # Remove document from list
            self._documents.pop(doc_index)
            
            # Rebuild index (for simplicity - could be optimized)
            await self._rebuild_index()
            
        except Exception as e:
            raise KnowledgeError(f"Failed to delete document '{document_id}': {e}")
    
    async def get_stats(self) -> dict:
        """Get statistics about the knowledge base.
        
        Returns:
            Dictionary containing stats like document count, index size, etc.
        """
        return {
            "document_count": len(self._documents),
            "index_size": self.index.ntotal if self._index else 0,
            "embedding_dimension": self._embedding_dim,
            "model_name": self.model_name,
            "index_path": str(self.index_path) if self.index_path else None,
        }
    
    async def save(self, path: Optional[Union[str, Path]] = None) -> None:
        """Save the knowledge base to disk.
        
        Args:
            path: Path to save the knowledge base (uses self.index_path if None)
            
        Raises:
            KnowledgeError: If saving fails
        """
        save_path = Path(path) if path else self.index_path
        if save_path is None:
            raise KnowledgeError("No save path specified")
        
        try:
            save_path.mkdir(parents=True, exist_ok=True)
            
            # Save FAISS index
            if self._index and self._index.ntotal > 0:
                faiss.write_index(self._index, str(save_path / "index.faiss"))
            
            # Save documents and metadata
            documents_data = [doc.model_dump() for doc in self._documents]
            with open(save_path / "documents.json", "w") as f:
                json.dump(documents_data, f, indent=2)
            
            # Save configuration
            config = {
                "model_name": self.model_name,
                "embedding_dim": self._embedding_dim,
                "document_count": len(self._documents),
            }
            with open(save_path / "config.json", "w") as f:
                json.dump(config, f, indent=2)
                
        except Exception as e:
            raise KnowledgeError(f"Failed to save knowledge base: {e}")
    
    async def load(self, path: Optional[Union[str, Path]] = None) -> None:
        """Load the knowledge base from disk.
        
        Args:
            path: Path to load the knowledge base from (uses self.index_path if None)
            
        Raises:
            KnowledgeError: If loading fails
        """
        load_path = Path(path) if path else self.index_path
        if load_path is None or not load_path.exists():
            raise KnowledgeError(f"Knowledge base path does not exist: {load_path}")
        
        try:
            # Load configuration
            config_path = load_path / "config.json"
            if config_path.exists():
                with open(config_path) as f:
                    config = json.load(f)
                self.model_name = config.get("model_name", self.model_name)
                self._embedding_dim = config.get("embedding_dim")
            
            # Load documents
            documents_path = load_path / "documents.json"
            if documents_path.exists():
                with open(documents_path) as f:
                    documents_data = json.load(f)
                self._documents = [Document(**doc_data) for doc_data in documents_data]
            
            # Load FAISS index
            index_path = load_path / "index.faiss"
            if index_path.exists():
                self._index = faiss.read_index(str(index_path))
            else:
                # Rebuild index from documents if index file doesn't exist
                await self._rebuild_index()
                
        except Exception as e:
            raise KnowledgeError(f"Failed to load knowledge base: {e}")
    
    async def _rebuild_index(self) -> None:
        """Rebuild the FAISS index from current documents."""
        if not self._documents:
            return
            
        # Reset index
        if self._embedding_dim is None:
            self._embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        
        self._index = faiss.IndexFlatL2(self._embedding_dim)
        
        # Add all embeddings
        embeddings = []
        for doc in self._documents:
            if doc.embedding is None:
                # Generate embedding if missing
                embedding = self.embedding_model.encode(
                    [doc.content],
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
                doc.embedding = embedding[0].tolist()
            embeddings.append(doc.embedding)
        
        if embeddings:
            embeddings_array = np.array(embeddings, dtype=np.float32)
            self._index.add(embeddings_array)


================================================
FILE: dataqa/primitives/in_memory_executor.py
================================================
"""In-memory executor implementation using DuckDB."""

import ast
import asyncio
import base64
import io
import sys
import time
from contextlib import redirect_stdout, redirect_stderr
from typing import Any, Dict, List, Optional, Union
import warnings

import duckdb
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from ..exceptions import ExecutionError
from ..models.execution import ExecutionResult
from .executor import ExecutorPrimitive


class InMemoryExecutor(ExecutorPrimitive):
    """In-memory executor using DuckDB for SQL and controlled Python execution.
    
    This implementation provides a secure, sandboxed environment for executing
    SQL queries and Python code. It uses DuckDB as the SQL engine with pandas
    integration and implements controlled Python execution with restricted
    built-ins and imports.
    """
    
    def __init__(self, config: Union[str, Dict[str, Any]] = ":memory:"):
        """Initialize the in-memory executor.
        
        Args:
            config: Either a database path string or configuration dictionary
        """
        if isinstance(config, str):
            # Legacy string path
            self.database_path = config
            self.max_execution_time = 30.0
            self.max_memory_mb = 512
        else:
            # Configuration dictionary
            self.database_path = config.get("database_path", ":memory:")
            self.max_execution_time = config.get("max_execution_time", 30.0)
            self.max_memory_mb = config.get("max_memory_mb", 512)
        self._connection: Optional[duckdb.DuckDBPyConnection] = None
        self._python_globals: Dict[str, Any] = {}
        self._setup_matplotlib()
        self._setup_python_environment()
    
    def _setup_matplotlib(self) -> None:
        """Set up matplotlib for non-interactive plotting."""
        # Use Agg backend for non-interactive plotting
        matplotlib.use('Agg')
        # Set seaborn style
        sns.set_style("whitegrid")
        plt.style.use('default')
    
    def _setup_python_environment(self) -> None:
        """Set up the controlled Python execution environment."""
        # Safe built-ins for Python execution
        safe_builtins = {
            'abs', 'all', 'any', 'bool', 'dict', 'enumerate', 'filter',
            'float', 'int', 'len', 'list', 'map', 'max', 'min', 'range',
            'round', 'set', 'sorted', 'str', 'sum', 'tuple', 'zip',
            'print', 'type', 'isinstance', 'hasattr', 'getattr', 'setattr'
        }
        
        # Get built-ins properly (handle both dict and module cases)
        import builtins
        safe_builtins_dict = {}
        for name in safe_builtins:
            if hasattr(builtins, name):
                safe_builtins_dict[name] = getattr(builtins, name)
        
        # Add a restricted __import__ that only allows safe modules
        def restricted_import(name, globals=None, locals=None, fromlist=(), level=0):
            # Allow only specific safe modules that pandas might need
            allowed_modules = {
                'numpy', 'pandas', 'datetime', 'decimal', 'fractions',
                'collections', 'itertools', 'functools', 'operator',
                'math', 'statistics', 'json', 're'
            }
            
            # Block dangerous imports
            if name in ['os', 'sys', 'subprocess', 'socket', 'urllib', 'requests', 'http']:
                raise ImportError(f"Import of '{name}' is not allowed")
            
            # For pandas internal imports, we'll be more permissive but still block dangerous ones
            if name.startswith('pandas') or name.startswith('numpy') or name in allowed_modules:
                return builtins.__import__(name, globals, locals, fromlist, level)
            
            raise ImportError(f"Import of '{name}' is not allowed")
        
        safe_builtins_dict['__import__'] = restricted_import
        
        # Create restricted globals for Python execution
        self._python_globals = {
            '__builtins__': safe_builtins_dict,
            'pd': pd,
            'pandas': pd,
            'plt': plt,
            'matplotlib': matplotlib,
            'sns': sns,
            'seaborn': sns,
            'np': np,
            'numpy': np,
        }
    
    @property
    def connection(self) -> duckdb.DuckDBPyConnection:
        """Get or create DuckDB connection."""
        if self._connection is None:
            self._connection = duckdb.connect(self.database_path)
            # DuckDB has built-in pandas support, no need to install extension
        return self._connection
    
    async def execute_sql(
        self, 
        sql: str, 
        parameters: Optional[Dict[str, Any]] = None
    ) -> ExecutionResult:
        """Execute SQL code using DuckDB.
        
        Args:
            sql: The SQL query to execute
            parameters: Optional parameters for parameterized queries
            
        Returns:
            ExecutionResult containing the query results or error information
        """
        start_time = time.time()
        
        try:
            # Validate SQL before execution
            if not await self.validate_code(sql, "sql"):
                return ExecutionResult(
                    success=False,
                    error="SQL validation failed: potentially unsafe query",
                    execution_time=time.time() - start_time,
                    code_executed=sql
                )
            
            # Execute the SQL query
            if parameters:
                result = self.connection.execute(sql, parameters).fetchdf()
            else:
                result = self.connection.execute(sql).fetchdf()
            
            # Convert DataFrame to dictionary for serialization
            data = {
                "dataframe": result.to_dict(orient="records"),
                "columns": result.columns.tolist(),
                "shape": list(result.shape),  # Convert tuple to list for consistency
                "dtypes": result.dtypes.astype(str).to_dict()
            }
            
            return ExecutionResult(
                success=True,
                data=data,
                execution_time=time.time() - start_time,
                code_executed=sql,
                output_type="dataframe"
            )
            
        except Exception as e:
            return ExecutionResult(
                success=False,
                error=f"SQL execution error: {str(e)}",
                execution_time=time.time() - start_time,
                code_executed=sql
            )
    
    async def execute_python(
        self, 
        code: str, 
        context: Optional[Dict[str, Any]] = None
    ) -> ExecutionResult:
        """Execute Python code in a controlled environment.
        
        Args:
            code: The Python code to execute
            context: Optional context variables to make available to the code
            
        Returns:
            ExecutionResult containing the execution results or error information
        """
        start_time = time.time()
        
        try:
            # Validate Python code before execution
            if not await self.validate_code(code, "python"):
                return ExecutionResult(
                    success=False,
                    error="Python validation failed: potentially unsafe code",
                    execution_time=time.time() - start_time,
                    code_executed=code
                )
            
            # Set up execution environment
            execution_globals = self._python_globals.copy()
            if context:
                execution_globals.update(context)
            
            # Add connection to globals for database access
            execution_globals['connection'] = self.connection
            execution_globals['conn'] = self.connection  # Common alias
            
            # Capture stdout and stderr
            stdout_capture = io.StringIO()
            stderr_capture = io.StringIO()
            
            # Clear any existing plots
            plt.clf()
            plt.close('all')
            
            # Execute the code
            with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                # Compile and execute the code
                compiled_code = compile(code, '<string>', 'exec')
                exec(compiled_code, execution_globals)
            
            # Get captured output
            stdout_output = stdout_capture.getvalue()
            stderr_output = stderr_capture.getvalue()
            
            # Check if any plots were created
            plot_data = None
            output_type = "python_execution"
            
            if plt.get_fignums():  # Check if there are any figures
                plot_data = self._capture_plot()
                output_type = "plot"
            
            # Prepare result data
            data = {
                "stdout": stdout_output,
                "stderr": stderr_output,
                "globals": {k: str(v) for k, v in execution_globals.items() 
                           if not k.startswith('__') and k not in ['pd', 'pandas', 'connection', 'conn', 'plt', 'matplotlib', 'sns', 'seaborn', 'np', 'numpy']}
            }
            
            # Add plot data if available
            if plot_data:
                data["plot"] = plot_data
            
            return ExecutionResult(
                success=True,
                data=data,
                execution_time=time.time() - start_time,
                code_executed=code,
                output_type=output_type
            )
            
        except Exception as e:
            return ExecutionResult(
                success=False,
                error=f"Python execution error: {str(e)}",
                execution_time=time.time() - start_time,
                code_executed=code
            )
    
    async def get_schema(self, table_name: Optional[str] = None) -> Dict[str, Any]:
        """Get database schema information.
        
        Args:
            table_name: Optional specific table name, if None returns all tables
            
        Returns:
            Dictionary containing schema information
        """
        try:
            if table_name:
                # Get schema for specific table
                schema_query = """
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns 
                WHERE table_name = ?
                ORDER BY ordinal_position
                """
                columns_df = self.connection.execute(schema_query, [table_name]).fetchdf()
                
                return {
                    "table_name": table_name,
                    "columns": columns_df.to_dict(orient="records")
                }
            else:
                # Get schema for all tables
                tables_query = "SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'"
                tables_df = self.connection.execute(tables_query).fetchdf()
                
                schema_info = {"tables": []}
                
                for table in tables_df['table_name']:
                    columns_query = """
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns 
                    WHERE table_name = ?
                    ORDER BY ordinal_position
                    """
                    columns_df = self.connection.execute(columns_query, [table]).fetchdf()
                    
                    schema_info["tables"].append({
                        "table_name": table,
                        "columns": columns_df.to_dict(orient="records")
                    })
                
                return schema_info
                
        except Exception as e:
            raise ExecutionError(f"Failed to get schema: {str(e)}")
    
    async def list_tables(self) -> List[str]:
        """List all available tables in the database.
        
        Returns:
            List of table names
        """
        try:
            query = "SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'"
            result = self.connection.execute(query).fetchdf()
            return result['table_name'].tolist()
        except Exception as e:
            raise ExecutionError(f"Failed to list tables: {str(e)}")
    
    async def get_sample_data(
        self, 
        table_name: str, 
        limit: int = 5
    ) -> ExecutionResult:
        """Get sample data from a table.
        
        Args:
            table_name: Name of the table to sample
            limit: Maximum number of rows to return
            
        Returns:
            ExecutionResult containing sample data
        """
        start_time = time.time()
        
        try:
            # Validate table name to prevent SQL injection
            tables = await self.list_tables()
            if table_name not in tables:
                return ExecutionResult(
                    success=False,
                    error=f"Table '{table_name}' not found",
                    execution_time=time.time() - start_time,
                    code_executed=f"SELECT * FROM {table_name} LIMIT {limit}"
                )
            
            query = f"SELECT * FROM {table_name} LIMIT {limit}"
            return await self.execute_sql(query)
            
        except Exception as e:
            return ExecutionResult(
                success=False,
                error=f"Failed to get sample data: {str(e)}",
                execution_time=time.time() - start_time,
                code_executed=f"SELECT * FROM {table_name} LIMIT {limit}"
            )
    
    async def validate_code(self, code: str, code_type: str) -> bool:
        """Validate code before execution for security and syntax.
        
        Args:
            code: The code to validate
            code_type: Type of code ('sql' or 'python')
            
        Returns:
            True if code is valid and safe to execute
        """
        try:
            if code_type.lower() == "sql":
                return self._validate_sql(code)
            elif code_type.lower() == "python":
                return self._validate_python(code)
            else:
                return False
        except Exception:
            return False
    
    def _validate_sql(self, sql: str) -> bool:
        """Validate SQL code for safety and syntax.
        
        Args:
            sql: SQL code to validate
            
        Returns:
            True if SQL is safe to execute
        """
        # Convert to lowercase for checking
        sql_lower = sql.lower().strip()
        
        # Block potentially dangerous SQL operations
        dangerous_keywords = [
            'drop', 'delete', 'truncate', 'alter', 'create', 'insert', 'update',
            'grant', 'revoke', 'exec', 'execute', 'sp_', 'xp_'
        ]
        
        for keyword in dangerous_keywords:
            if keyword in sql_lower:
                return False
        
        # Basic syntax validation by attempting to prepare the statement
        try:
            # Use DuckDB's prepare to validate syntax without execution
            # Create a temporary connection for validation to avoid side effects
            temp_conn = duckdb.connect(":memory:")
            temp_conn.execute("PREPARE stmt AS " + sql)
            temp_conn.execute("DEALLOCATE stmt")
            temp_conn.close()
            return True
        except Exception:
            # If prepare fails, try a simple syntax check by parsing
            # Allow the query if it starts with SELECT (read-only operations)
            if sql_lower.strip().startswith('select'):
                return True
            return False
    
    def _validate_python(self, code: str) -> bool:
        """Validate Python code for safety and syntax.
        
        Args:
            code: Python code to validate
            
        Returns:
            True if Python code is safe to execute
        """
        try:
            # Parse the code to check syntax
            tree = ast.parse(code)
            
            # Check for dangerous operations
            for node in ast.walk(tree):
                # Allow imports of safe modules that are already in globals
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        if alias.name not in ['matplotlib', 'matplotlib.pyplot', 'numpy', 'pandas', 'seaborn']:
                            return False
                
                if isinstance(node, ast.ImportFrom):
                    if node.module not in ['matplotlib', 'matplotlib.pyplot', 'numpy', 'pandas', 'seaborn']:
                        return False
                
                # Block access to dangerous attributes
                if isinstance(node, ast.Attribute):
                    if node.attr in ['__import__', '__builtins__', '__globals__', '__locals__']:
                        return False
                
                # Block function calls to dangerous functions
                if isinstance(node, ast.Call):
                    if isinstance(node.func, ast.Name):
                        if node.func.id in ['eval', 'exec', 'compile', 'open']:
                            return False
            
            return True
            
        except SyntaxError:
            return False
        except Exception:
            return False
    
    def _capture_plot(self) -> Dict[str, Any]:
        """Capture the current matplotlib plot as base64 encoded image data.
        
        Returns:
            Dictionary containing plot data and metadata
        """
        try:
            # Get the current figure
            fig = plt.gcf()
            
            # Save plot to bytes buffer
            img_buffer = io.BytesIO()
            fig.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
            img_buffer.seek(0)
            
            # Encode as base64
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')
            
            # Get plot metadata
            plot_data = {
                "image_data": img_base64,
                "format": "png",
                "width": fig.get_figwidth(),
                "height": fig.get_figheight(),
                "dpi": fig.dpi
            }
            
            # Clear the figure to free memory
            plt.close(fig)
            
            return plot_data
            
        except Exception as e:
            # If plot capture fails, return error info
            return {
                "error": f"Failed to capture plot: {str(e)}",
                "image_data": None
            }
    
    async def generate_visualization(
        self, 
        data: pd.DataFrame, 
        chart_type: str = "auto",
        **kwargs
    ) -> ExecutionResult:
        """Generate a visualization based on data characteristics.
        
        Args:
            data: DataFrame to visualize
            chart_type: Type of chart to generate ('auto', 'bar', 'line', 'scatter', 'histogram', 'box', 'heatmap')
            **kwargs: Additional parameters for customization
            
        Returns:
            ExecutionResult containing the generated plot
        """
        start_time = time.time()
        
        try:
            # Clear any existing plots
            plt.clf()
            plt.close('all')
            
            # Determine chart type if auto
            if chart_type == "auto":
                chart_type = self._recommend_chart_type(data)
            
            # Generate the appropriate visualization
            code = self._generate_plot_code(data, chart_type, **kwargs)
            
            # Execute the plotting code
            return await self.execute_python(code, context={"data": data})
            
        except Exception as e:
            return ExecutionResult(
                success=False,
                error=f"Visualization generation error: {str(e)}",
                execution_time=time.time() - start_time,
                code_executed=f"generate_visualization(chart_type='{chart_type}')"
            )
    
    def _recommend_chart_type(self, data: pd.DataFrame) -> str:
        """Recommend the best chart type based on data characteristics.
        
        Args:
            data: DataFrame to analyze
            
        Returns:
            Recommended chart type
        """
        if data.empty:
            return "bar"
        
        num_cols = len(data.select_dtypes(include=[np.number]).columns)
        cat_cols = len(data.select_dtypes(include=['object', 'category']).columns)
        
        # Time series detection - check for datetime columns first
        date_cols = data.select_dtypes(include=['datetime64']).columns
        if len(date_cols) > 0 and num_cols >= 1:
            return "line"
        
        # Single numeric column
        if num_cols == 1 and cat_cols == 0:
            return "histogram"
        
        # One categorical, one numeric
        if num_cols == 1 and cat_cols == 1:
            unique_cats = data.select_dtypes(include=['object', 'category']).nunique().iloc[0]
            if unique_cats <= 10:
                return "bar"
            else:
                return "box"
        
        # Two numeric columns
        if num_cols == 2 and cat_cols == 0:
            return "scatter"
        
        # Multiple numeric columns - correlation heatmap
        if num_cols > 2:
            return "heatmap"
        
        # Default fallback
        return "bar"
    
    def _generate_plot_code(
        self, 
        data: pd.DataFrame, 
        chart_type: str, 
        **kwargs
    ) -> str:
        """Generate Python code for creating the specified chart type.
        
        Args:
            data: DataFrame to plot
            chart_type: Type of chart to generate
            **kwargs: Additional parameters
            
        Returns:
            Python code string for generating the plot
        """
        # Handle empty DataFrame
        if data.empty:
            figsize = kwargs.get('figsize', (10, 6))
            title = kwargs.get('title', 'Empty Data')
            return f"""
plt.figure(figsize={figsize})
plt.text(0.5, 0.5, 'No data to display', ha='center', va='center', transform=plt.gca().transAxes, fontsize=16)
plt.title('{title}')
plt.axis('off')
plt.tight_layout()
"""
        
        # Get column information
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        date_cols = data.select_dtypes(include=['datetime64']).columns.tolist()
        
        # Extract common parameters
        title = kwargs.get('title', f'{chart_type.title()} Chart')
        figsize = kwargs.get('figsize', (10, 6))
        
        if chart_type == "bar":
            if len(categorical_cols) > 0 and len(numeric_cols) > 0:
                x_col = categorical_cols[0]
                y_col = numeric_cols[0]
                return f"""
plt.figure(figsize={figsize})
data.groupby('{x_col}')['{y_col}'].mean().plot(kind='bar')
plt.title('{title}')
plt.xlabel('{x_col}')
plt.ylabel('{y_col}')
plt.xticks(rotation=45)
plt.tight_layout()
"""
            elif len(data.columns) > 0:
                # Fallback to value counts of first column
                col = data.columns[0]
                return f"""
plt.figure(figsize={figsize})
data['{col}'].value_counts().head(10).plot(kind='bar')
plt.title('{title}')
plt.xlabel('{col}')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
"""
            else:
                # No columns available
                return f"""
plt.figure(figsize={figsize})
plt.text(0.5, 0.5, 'No data to display', ha='center', va='center', transform=plt.gca().transAxes, fontsize=16)
plt.title('{title}')
plt.axis('off')
plt.tight_layout()
"""
        
        elif chart_type == "line":
            if len(date_cols) > 0 and len(numeric_cols) > 0:
                x_col = date_cols[0]
                y_col = numeric_cols[0]
                return f"""
plt.figure(figsize={figsize})
plt.plot(data['{x_col}'], data['{y_col}'])
plt.title('{title}')
plt.xlabel('{x_col}')
plt.ylabel('{y_col}')
plt.xticks(rotation=45)
plt.tight_layout()
"""
            elif len(numeric_cols) >= 2:
                x_col = numeric_cols[0]
                y_col = numeric_cols[1]
                return f"""
plt.figure(figsize={figsize})
plt.plot(data['{x_col}'], data['{y_col}'])
plt.title('{title}')
plt.xlabel('{x_col}')
plt.ylabel('{y_col}')
plt.tight_layout()
"""
            else:
                # Line plot of single numeric column
                col = numeric_cols[0] if numeric_cols else data.columns[0]
                return f"""
plt.figure(figsize={figsize})
data['{col}'].plot(kind='line')
plt.title('{title}')
plt.ylabel('{col}')
plt.tight_layout()
"""
        
        elif chart_type == "scatter":
            if len(numeric_cols) >= 2:
                x_col = numeric_cols[0]
                y_col = numeric_cols[1]
                return f"""
plt.figure(figsize={figsize})
plt.scatter(data['{x_col}'], data['{y_col}'], alpha=0.6)
plt.title('{title}')
plt.xlabel('{x_col}')
plt.ylabel('{y_col}')
plt.tight_layout()
"""
            else:
                # Fallback to histogram
                col = numeric_cols[0] if numeric_cols else data.columns[0]
                return f"""
plt.figure(figsize={figsize})
plt.hist(data['{col}'], bins=20, alpha=0.7)
plt.title('{title}')
plt.xlabel('{col}')
plt.ylabel('Frequency')
plt.tight_layout()
"""
        
        elif chart_type == "histogram":
            col = numeric_cols[0] if numeric_cols else data.columns[0]
            bins = kwargs.get('bins', 20)
            return f"""
plt.figure(figsize={figsize})
plt.hist(data['{col}'], bins={bins}, alpha=0.7, edgecolor='black')
plt.title('{title}')
plt.xlabel('{col}')
plt.ylabel('Frequency')
plt.tight_layout()
"""
        
        elif chart_type == "box":
            if len(categorical_cols) > 0 and len(numeric_cols) > 0:
                return f"""
plt.figure(figsize={figsize})
sns.boxplot(data=data, x='{categorical_cols[0]}', y='{numeric_cols[0]}')
plt.title('{title}')
plt.xticks(rotation=45)
plt.tight_layout()
"""
            else:
                col = numeric_cols[0] if numeric_cols else data.columns[0]
                return f"""
plt.figure(figsize={figsize})
plt.boxplot(data['{col}'].dropna())
plt.title('{title}')
plt.ylabel('{col}')
plt.tight_layout()
"""
        
        elif chart_type == "heatmap":
            if len(numeric_cols) > 1:
                return f"""
plt.figure(figsize={figsize})
correlation_matrix = data[{numeric_cols}].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('{title}')
plt.tight_layout()
"""
            else:
                # Fallback to bar chart
                col = data.columns[0]
                return f"""
plt.figure(figsize={figsize})
data['{col}'].value_counts().head(10).plot(kind='bar')
plt.title('{title}')
plt.xlabel('{col}')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
"""
        
        else:
            # Default fallback
            col = data.columns[0]
            return f"""
plt.figure(figsize={figsize})
data['{col}'].value_counts().head(10).plot(kind='bar')
plt.title('{title}')
plt.xlabel('{col}')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
"""

    async def load_dataframe(self, table_name: str, df: pd.DataFrame) -> None:
        """Load a pandas DataFrame into the database as a table.
        
        Args:
            table_name: Name for the table
            df: DataFrame to load
        """
        try:
            self.connection.register(table_name, df)
        except Exception as e:
            raise ExecutionError(f"Failed to load DataFrame: {str(e)}")
    
    def close(self) -> None:
        """Close the database connection."""
        if self._connection:
            self._connection.close()
            self._connection = None
    
    def __enter__(self):
        """Context manager entry."""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.close()


================================================
FILE: dataqa/primitives/knowledge.py
================================================
"""Abstract base class for knowledge primitive implementations."""

from abc import ABC, abstractmethod
from typing import List, Optional

from ..exceptions import KnowledgeError
from ..models.document import Document


class KnowledgePrimitive(ABC):
    """Abstract base class for knowledge retrieval systems.
    
    This interface defines the contract for components that manage
    domain-specific knowledge and provide context for agent responses.
    Implementations might use vector databases, traditional search,
    or hybrid approaches.
    """
    
    @abstractmethod
    async def ingest(self, documents: List[Document]) -> None:
        """Ingest documents into the knowledge base.
        
        Args:
            documents: List of documents to add to the knowledge base
            
        Raises:
            KnowledgeError: If ingestion fails
        """
        pass
    
    @abstractmethod
    async def search(
        self, 
        query: str, 
        limit: int = 5,
        filters: Optional[dict] = None
    ) -> List[Document]:
        """Search for relevant documents based on a query.
        
        Args:
            query: The search query string
            limit: Maximum number of documents to return
            filters: Optional filters to apply to the search
            
        Returns:
            List of relevant documents ordered by relevance
            
        Raises:
            KnowledgeError: If search fails
        """
        pass
    
    @abstractmethod
    async def update(self, document_id: str, document: Document) -> None:
        """Update an existing document in the knowledge base.
        
        Args:
            document_id: Unique identifier for the document
            document: Updated document content
            
        Raises:
            KnowledgeError: If update fails or document not found
        """
        pass
    
    @abstractmethod
    async def delete(self, document_id: str) -> None:
        """Delete a document from the knowledge base.
        
        Args:
            document_id: Unique identifier for the document to delete
            
        Raises:
            KnowledgeError: If deletion fails or document not found
        """
        pass
    
    @abstractmethod
    async def get_stats(self) -> dict:
        """Get statistics about the knowledge base.
        
        Returns:
            Dictionary containing stats like document count, index size, etc.
        """
        pass


================================================
FILE: dataqa/primitives/llm.py
================================================
"""Abstract base class for LLM interface implementations."""

import asyncio
import json
from abc import ABC, abstractmethod
from typing import Any

import openai
from openai import AsyncOpenAI

from ..config.models import LLMConfig, LLMProvider
from ..exceptions import LLMError, RetryableError
from ..logging_config import get_primitive_logger
from ..models.message import Message
from ..utils.retry import retry_async, retry_on_rate_limit


class LLMInterface(ABC):
    """Abstract base class for Large Language Model interfaces.

    This interface defines the contract for components that interact
    with language models to generate code, analyze queries, and
    provide natural language responses. Implementations might use
    OpenAI, Anthropic, local models, or other LLM providers.
    """

    @abstractmethod
    async def generate_code(
        self,
        query: str,
        context: str | None = None,
        code_type: str = "sql",
        conversation_history: list[Message] | None = None
    ) -> str:
        """Generate code based on a natural language query.

        Args:
            query: The natural language query from the user
            context: Optional context information (schema, examples, etc.)
            code_type: Type of code to generate ('sql', 'python')
            conversation_history: Previous messages for context

        Returns:
            Generated code as a string

        Raises:
            LLMError: If code generation fails
        """
        pass

    @abstractmethod
    async def analyze_query(
        self,
        query: str,
        conversation_history: list[Message] | None = None
    ) -> dict[str, Any]:
        """Analyze a user query to understand intent and requirements.

        Args:
            query: The natural language query from the user
            conversation_history: Previous messages for context

        Returns:
            Dictionary containing analysis results (intent, entities, etc.)

        Raises:
            LLMError: If query analysis fails
        """
        pass

    @abstractmethod
    async def format_response(
        self,
        results: dict[str, Any],
        query: str,
        conversation_history: list[Message] | None = None
    ) -> str:
        """Format execution results into a natural language response.

        Args:
            results: The results from code execution
            query: The original user query
            conversation_history: Previous messages for context

        Returns:
            Formatted natural language response

        Raises:
            LLMError: If response formatting fails
        """
        pass

    @abstractmethod
    async def generate_clarification(
        self,
        query: str,
        ambiguities: list[str],
        conversation_history: list[Message] | None = None
    ) -> str:
        """Generate clarifying questions for ambiguous queries.

        Args:
            query: The ambiguous query from the user
            ambiguities: List of identified ambiguities
            conversation_history: Previous messages for context

        Returns:
            Clarifying question(s) as a string

        Raises:
            LLMError: If clarification generation fails
        """
        pass

    @abstractmethod
    async def validate_generated_code(
        self,
        code: str,
        code_type: str,
        context: str | None = None
    ) -> dict[str, Any]:
        """Validate generated code for correctness and safety.

        Args:
            code: The generated code to validate
            code_type: Type of code ('sql', 'python')
            context: Optional context for validation

        Returns:
            Dictionary containing validation results (is_valid, issues, etc.)

        Raises:
            LLMError: If validation fails
        """
        pass

    @abstractmethod
    async def get_model_info(self) -> dict[str, Any]:
        """Get information about the underlying model.

        Returns:
            Dictionary containing model information (name, version, capabilities)
        """
        pass


class OpenAILLM(LLMInterface):
    """OpenAI LLM implementation with structured prompts and context injection."""

    def __init__(self, config: LLMConfig):
        """Initialize OpenAI LLM with configuration.

        Args:
            config: LLM configuration containing API key, model, etc.

        Raises:
            LLMError: If configuration is invalid or API key is missing
        """
        if config.provider != LLMProvider.OPENAI:
            raise LLMError(
                f"Invalid provider for OpenAI LLM: {config.provider}",
                user_message="Configuration error: Invalid LLM provider specified.",
                error_code="INVALID_LLM_PROVIDER"
            )

        if not config.api_key:
            raise LLMError(
                "OpenAI API key is required",
                user_message="OpenAI API key is not configured. Please set your API key.",
                error_code="MISSING_API_KEY",
                recovery_suggestions=[
                    "Set the OPENAI_API_KEY environment variable",
                    "Check your configuration file for the correct API key setting"
                ]
            )

        self.config = config
        self.logger = get_primitive_logger("llm", f"openai-{config.model}")
        
        try:
            self.client = AsyncOpenAI(
                api_key=config.api_key.get_secret_value(),
                base_url=config.api_base,
                timeout=config.timeout,
                max_retries=config.max_retries
            )
            self.logger.info(f"OpenAI LLM initialized with model {config.model}")
        except Exception as e:
            raise LLMError(
                f"Failed to initialize OpenAI client: {e}",
                user_message="Failed to connect to OpenAI service. Please check your configuration.",
                error_code="CLIENT_INIT_FAILED",
                original_error=e
            )

        # Prompt templates for different operations
        self.code_generation_template = """You are a data analysis expert. Generate {code_type} code to answer the user's question.

Context Information:
{context}

Conversation History:
{history}

User Query: {query}

Requirements:
- Generate only the {code_type} code, no explanations
- Ensure the code is safe and follows best practices
- Use appropriate error handling
- For SQL: Use standard SQL syntax compatible with DuckDB
- For Python: Use pandas for data manipulation, matplotlib/seaborn for visualization

Generated {code_type_upper} Code:"""

        self.query_analysis_template = """Analyze the following user query to understand their intent and requirements.

Conversation History:
{history}

User Query: {query}

Provide analysis in JSON format with the following structure:
{{
    "intent": "description of what the user wants to accomplish",
    "query_type": "data_retrieval|analysis|visualization|modification",
    "entities": ["list", "of", "key", "entities"],
    "complexity": "simple|moderate|complex",
    "requires_clarification": true/false,
    "suggested_approach": "brief description of recommended approach"
}}

Analysis:"""

        self.response_formatting_template = """Format the execution results into a clear, natural language response for the user.

Original Query: {query}

Execution Results:
{results}

Conversation History:
{history}

Requirements:
- Provide a clear, concise summary of the results
- Highlight key insights or patterns
- Use natural language that's easy to understand
- If there are visualizations, describe what they show
- If there are errors, explain them in user-friendly terms

Response:"""

        self.clarification_template = """The user's query is ambiguous. Generate clarifying questions to better understand their needs.

User Query: {query}

Identified Ambiguities:
{ambiguities}

Conversation History:
{history}

Generate 1-3 specific clarifying questions that will help resolve the ambiguities:

Questions:"""

        self.code_validation_template = """Validate the following {code_type} code for correctness and safety.

Code to Validate:
{code}

Context:
{context}

Provide validation results in JSON format:
{{
    "is_valid": true/false,
    "issues": ["list", "of", "identified", "issues"],
    "security_concerns": ["list", "of", "security", "issues"],
    "suggestions": ["list", "of", "improvement", "suggestions"],
    "risk_level": "low|medium|high"
}}

Validation Results:"""

    def _format_conversation_history(self, history: list[Message] | None) -> str:
        """Format conversation history for prompt inclusion."""
        if not history:
            return "No previous conversation."

        formatted = []
        for msg in history[-5:]:  # Include last 5 messages for context
            role = msg.role.title()
            formatted.append(f"{role}: {msg.content}")

        return "\n".join(formatted)

    def _build_messages(self, system_prompt: str, user_prompt: str) -> list[dict[str, str]]:
        """Build message list for OpenAI API."""
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    @retry_on_rate_limit(max_attempts=3, base_delay=2.0)
    async def _make_api_call(self, messages: list[dict[str, str]], **kwargs) -> str:
        """Make API call with error handling and retries."""
        try:
            self.logger.debug(f"Making OpenAI API call with model {self.config.model}")
            
            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                **self.config.extra_params,
                **kwargs
            )

            if not response.choices:
                raise LLMError(
                    "No response choices returned from OpenAI API",
                    user_message="The AI service didn't provide a response. Please try again.",
                    error_code="NO_RESPONSE_CHOICES"
                )

            content = response.choices[0].message.content
            if not content:
                raise LLMError(
                    "Empty response content from OpenAI API",
                    user_message="The AI service provided an empty response. Please try again.",
                    error_code="EMPTY_RESPONSE"
                )

            self.logger.debug(f"Received response with {len(content)} characters")
            return content.strip()

        except openai.RateLimitError as e:
            self.logger.warning(f"Rate limit hit: {e}")
            raise RetryableError(
                f"Rate limit exceeded: {e}",
                user_message="The AI service is currently busy. Please wait a moment and try again.",
                error_code="RATE_LIMIT_EXCEEDED",
                retry_after=60.0,
                max_retries=5,
                original_error=e
            )

        except openai.APITimeoutError as e:
            self.logger.error(f"API timeout: {e}")
            raise RetryableError(
                f"API request timed out: {e}",
                user_message="The AI service took too long to respond. Please try again.",
                error_code="API_TIMEOUT",
                retry_after=5.0,
                max_retries=3,
                original_error=e
            )

        except openai.APIConnectionError as e:
            self.logger.error(f"API connection error: {e}")
            raise RetryableError(
                f"Failed to connect to OpenAI API: {e}",
                user_message="Unable to connect to the AI service. Please check your internet connection.",
                error_code="CONNECTION_ERROR",
                retry_after=10.0,
                max_retries=3,
                original_error=e
            )

        except openai.AuthenticationError as e:
            self.logger.error(f"Authentication error: {e}")
            raise LLMError(
                f"Invalid API key or authentication failed: {e}",
                user_message="Authentication failed. Please check your API key configuration.",
                error_code="AUTH_ERROR",
                recovery_suggestions=[
                    "Verify your OpenAI API key is correct",
                    "Check if your API key has sufficient permissions",
                    "Ensure your account has available credits"
                ],
                original_error=e
            )

        except openai.BadRequestError as e:
            self.logger.error(f"Bad request error: {e}")
            raise LLMError(
                f"Invalid request to OpenAI API: {e}",
                user_message="The request format is invalid. Please try rephrasing your question.",
                error_code="BAD_REQUEST",
                recovery_suggestions=[
                    "Try a shorter or simpler question",
                    "Check if your input contains any unusual characters"
                ],
                original_error=e
            )

        except Exception as e:
            self.logger.error(f"Unexpected error in OpenAI API call: {e}")
            raise LLMError(
                f"Unexpected error: {e}",
                user_message="An unexpected error occurred with the AI service. Please try again.",
                error_code="UNEXPECTED_ERROR",
                original_error=e
            )

    async def generate_code(
        self,
        query: str,
        context: str | None = None,
        code_type: str = "sql",
        conversation_history: list[Message] | None = None
    ) -> str:
        """Generate code based on a natural language query."""
        if code_type.lower() not in ["sql", "python"]:
            raise LLMError(f"Unsupported code type: {code_type}")

        history_str = self._format_conversation_history(conversation_history)
        context_str = context or "No additional context provided."

        prompt = self.code_generation_template.format(
            code_type=code_type.lower(),
            code_type_upper=code_type.upper(),
            context=context_str,
            history=history_str,
            query=query
        )

        system_prompt = f"You are an expert data analyst specializing in {code_type.upper()} code generation."
        messages = self._build_messages(system_prompt, prompt)

        try:
            response = await self._make_api_call(messages)

            # Extract code from response (remove markdown formatting if present)
            code = response
            if f"```{code_type.lower()}" in code:
                code = code.split(f"```{code_type.lower()}")[1].split("```")[0].strip()
            elif "```" in code:
                code = code.split("```")[1].split("```")[0].strip()

            if not code:
                raise LLMError(
                    "Generated code is empty",
                    user_message="The AI service didn't generate any code. Please try rephrasing your question.",
                    error_code="EMPTY_CODE_GENERATION"
                )

            self.logger.info(f"Generated {code_type} code for query: {query[:50]}...")
            return code

        except Exception as e:
            self.logger.error(f"Failed to generate {code_type} code: {e}")
            raise LLMError(
                f"Code generation failed: {e}",
                user_message="Failed to generate code for your request. Please try a different approach.",
                error_code="CODE_GENERATION_FAILED",
                original_error=e
            )

    async def analyze_query(
        self,
        query: str,
        conversation_history: list[Message] | None = None
    ) -> dict[str, Any]:
        """Analyze a user query to understand intent and requirements."""
        history_str = self._format_conversation_history(conversation_history)

        prompt = self.query_analysis_template.format(
            history=history_str,
            query=query
        )

        system_prompt = "You are an expert at analyzing data-related queries and understanding user intent."
        messages = self._build_messages(system_prompt, prompt)

        try:
            response = await self._make_api_call(messages)

            # Extract JSON from response
            json_str = response
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()

            try:
                analysis = json.loads(json_str)

                # Validate required fields
                required_fields = ["intent", "query_type", "entities", "complexity", "requires_clarification"]
                for field in required_fields:
                    if field not in analysis:
                        analysis[field] = None

                self.logger.info(f"Analyzed query: {query[:50]}... -> {analysis['query_type']}")
                return analysis

            except json.JSONDecodeError as e:
                self.logger.warning(f"Failed to parse JSON response, using fallback: {e}")
                return {
                    "intent": "Data analysis request",
                    "query_type": "analysis",
                    "entities": [],
                    "complexity": "moderate",
                    "requires_clarification": False,
                    "suggested_approach": "Generate appropriate code to answer the query"
                }

        except Exception as e:
            self.logger.error(f"Failed to analyze query: {e}")
            raise LLMError(
                f"Query analysis failed: {e}",
                user_message="Failed to analyze your question. Please try rephrasing it.",
                error_code="QUERY_ANALYSIS_FAILED",
                original_error=e
            )

    async def format_response(
        self,
        results: dict[str, Any],
        query: str,
        conversation_history: list[Message] | None = None
    ) -> str:
        """Format execution results into a natural language response."""
        history_str = self._format_conversation_history(conversation_history)

        # Format results for prompt
        results_str = json.dumps(results, indent=2, default=str)

        prompt = self.response_formatting_template.format(
            query=query,
            results=results_str,
            history=history_str
        )

        system_prompt = "You are an expert at explaining data analysis results in clear, accessible language."
        messages = self._build_messages(system_prompt, prompt)

        try:
            response = await self._make_api_call(messages)
            self.logger.info(f"Formatted response for query: {query[:50]}...")
            return response

        except Exception as e:
            self.logger.error(f"Failed to format response: {e}")
            raise LLMError(
                f"Response formatting failed: {e}",
                user_message="Failed to format the response. Please try again.",
                error_code="RESPONSE_FORMATTING_FAILED",
                original_error=e
            )

    async def generate_clarification(
        self,
        query: str,
        ambiguities: list[str],
        conversation_history: list[Message] | None = None
    ) -> str:
        """Generate clarifying questions for ambiguous queries."""
        history_str = self._format_conversation_history(conversation_history)
        ambiguities_str = "\n".join(f"- {amb}" for amb in ambiguities)

        prompt = self.clarification_template.format(
            query=query,
            ambiguities=ambiguities_str,
            history=history_str
        )

        system_prompt = "You are an expert at identifying ambiguities and asking clarifying questions."
        messages = self._build_messages(system_prompt, prompt)

        try:
            response = await self._make_api_call(messages)
            self.logger.info(f"Generated clarification for query: {query[:50]}...")
            return response

        except Exception as e:
            self.logger.error(f"Failed to generate clarification: {e}")
            raise LLMError(
                f"Clarification generation failed: {e}",
                user_message="Failed to generate clarifying questions. Please try again.",
                error_code="CLARIFICATION_FAILED",
                original_error=e
            )

    async def validate_generated_code(
        self,
        code: str,
        code_type: str,
        context: str | None = None
    ) -> dict[str, Any]:
        """Validate generated code for correctness and safety."""
        if code_type.lower() not in ["sql", "python"]:
            raise LLMError(f"Unsupported code type for validation: {code_type}")

        context_str = context or "No additional context provided."

        prompt = self.code_validation_template.format(
            code_type=code_type.lower(),
            code=code,
            context=context_str
        )

        system_prompt = f"You are an expert at validating {code_type.upper()} code for correctness and security."
        messages = self._build_messages(system_prompt, prompt)

        try:
            response = await self._make_api_call(messages)

            # Extract JSON from response
            json_str = response
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()

            try:
                validation = json.loads(json_str)

                # Ensure required fields exist
                required_fields = ["is_valid", "issues", "security_concerns", "suggestions", "risk_level"]
                for field in required_fields:
                    if field not in validation:
                        if field == "is_valid":
                            validation[field] = True
                        elif field == "risk_level":
                            validation[field] = "low"
                        else:
                            validation[field] = []

                self.logger.info(f"Validated {code_type} code: {validation['is_valid']}, risk: {validation['risk_level']}")
                return validation

            except json.JSONDecodeError as e:
                self.logger.warning(f"Failed to parse validation JSON, using fallback: {e}")
                return {
                    "is_valid": True,
                    "issues": [],
                    "security_concerns": [],
                    "suggestions": [],
                    "risk_level": "low"
                }

        except Exception as e:
            self.logger.error(f"Failed to validate code: {e}")
            raise LLMError(
                f"Code validation failed: {e}",
                user_message="Failed to validate the generated code. Please try again.",
                error_code="CODE_VALIDATION_FAILED",
                original_error=e
            )

    async def get_model_info(self) -> dict[str, Any]:
        """Get information about the underlying model."""
        return {
            "provider": "openai",
            "model": self.config.model,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "timeout": self.config.timeout,
            "max_retries": self.config.max_retries,
            "capabilities": [
                "code_generation",
                "query_analysis",
                "response_formatting",
                "clarification_generation",
                "code_validation"
            ]
        }


def create_llm_interface(config: LLMConfig) -> LLMInterface:
    """Factory function to create LLM interface based on configuration.

    Args:
        config: LLM configuration

    Returns:
        Appropriate LLM interface implementation

    Raises:
        LLMError: If provider is not supported
    """
    if config.provider == LLMProvider.OPENAI:
        return OpenAILLM(config)
    else:
        raise LLMError(f"Unsupported LLM provider: {config.provider}")



================================================
FILE: dataqa/utils/__init__.py
================================================
"""Utility modules for DataQA."""


================================================
FILE: dataqa/utils/retry.py
================================================
"""Retry mechanisms and error recovery utilities."""

import asyncio
import functools
import random
import time
from typing import Any, Callable, Optional, Type, Union

from ..exceptions import DataQAError, ErrorRecovery, RetryableError
from ..logging_config import get_logger


class RetryConfig:
    """Configuration for retry behavior."""
    
    def __init__(
        self,
        max_attempts: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        exponential_base: float = 2.0,
        jitter: bool = True,
        retryable_exceptions: Optional[tuple[Type[Exception], ...]] = None
    ):
        """Initialize retry configuration.
        
        Args:
            max_attempts: Maximum number of retry attempts
            base_delay: Base delay between retries in seconds
            max_delay: Maximum delay between retries in seconds
            exponential_base: Base for exponential backoff
            jitter: Whether to add random jitter to delays
            retryable_exceptions: Tuple of exception types that should trigger retries
        """
        self.max_attempts = max_attempts
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
        self.jitter = jitter
        self.retryable_exceptions = retryable_exceptions or (
            RetryableError,
            ConnectionError,
            TimeoutError,
        )
    
    def calculate_delay(self, attempt: int) -> float:
        """Calculate delay for given attempt number."""
        delay = min(
            self.base_delay * (self.exponential_base ** (attempt - 1)),
            self.max_delay
        )
        
        if self.jitter:
            # Add up to 25% jitter
            jitter_amount = delay * 0.25 * random.random()
            delay += jitter_amount
        
        return delay
    
    def should_retry(self, exception: Exception, attempt: int) -> bool:
        """Determine if an exception should trigger a retry."""
        if attempt >= self.max_attempts:
            return False
        
        # Check if it's a retryable exception type
        if isinstance(exception, self.retryable_exceptions):
            return True
        
        # Use ErrorRecovery utility for additional logic
        return ErrorRecovery.should_retry(exception, attempt)


def retry_sync(
    config: Optional[RetryConfig] = None,
    *,
    max_attempts: int = 3,
    base_delay: float = 1.0,
    logger_name: Optional[str] = None
) -> Callable:
    """Decorator for synchronous function retry logic.
    
    Args:
        config: Retry configuration object
        max_attempts: Maximum retry attempts (used if config not provided)
        base_delay: Base delay between retries (used if config not provided)
        logger_name: Logger name for retry logging
    
    Returns:
        Decorated function with retry logic
    """
    if config is None:
        config = RetryConfig(max_attempts=max_attempts, base_delay=base_delay)
    
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            logger = get_logger(logger_name or func.__module__)
            
            last_exception = None
            
            for attempt in range(1, config.max_attempts + 1):
                try:
                    if attempt > 1:
                        logger.info(f"Retry attempt {attempt}/{config.max_attempts} for {func.__name__}")
                    
                    result = func(*args, **kwargs)
                    
                    if attempt > 1:
                        logger.info(f"Function {func.__name__} succeeded on attempt {attempt}")
                    
                    return result
                
                except Exception as e:
                    last_exception = e
                    
                    if not config.should_retry(e, attempt):
                        logger.error(f"Function {func.__name__} failed permanently: {e}")
                        raise
                    
                    if attempt < config.max_attempts:
                        delay = config.calculate_delay(attempt)
                        logger.warning(
                            f"Function {func.__name__} failed on attempt {attempt}, "
                            f"retrying in {delay:.2f}s: {e}"
                        )
                        time.sleep(delay)
                    else:
                        logger.error(f"Function {func.__name__} failed after {attempt} attempts: {e}")
            
            # If we get here, all retries failed
            raise last_exception
        
        return wrapper
    return decorator


def retry_async(
    config: Optional[RetryConfig] = None,
    *,
    max_attempts: int = 3,
    base_delay: float = 1.0,
    logger_name: Optional[str] = None
) -> Callable:
    """Decorator for asynchronous function retry logic.
    
    Args:
        config: Retry configuration object
        max_attempts: Maximum retry attempts (used if config not provided)
        base_delay: Base delay between retries (used if config not provided)
        logger_name: Logger name for retry logging
    
    Returns:
        Decorated async function with retry logic
    """
    if config is None:
        config = RetryConfig(max_attempts=max_attempts, base_delay=base_delay)
    
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            logger = get_logger(logger_name or func.__module__)
            
            last_exception = None
            
            for attempt in range(1, config.max_attempts + 1):
                try:
                    if attempt > 1:
                        logger.info(f"Retry attempt {attempt}/{config.max_attempts} for {func.__name__}")
                    
                    result = await func(*args, **kwargs)
                    
                    if attempt > 1:
                        logger.info(f"Function {func.__name__} succeeded on attempt {attempt}")
                    
                    return result
                
                except Exception as e:
                    last_exception = e
                    
                    if not config.should_retry(e, attempt):
                        logger.error(f"Function {func.__name__} failed permanently: {e}")
                        raise
                    
                    if attempt < config.max_attempts:
                        delay = config.calculate_delay(attempt)
                        logger.warning(
                            f"Function {func.__name__} failed on attempt {attempt}, "
                            f"retrying in {delay:.2f}s: {e}"
                        )
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"Function {func.__name__} failed after {attempt} attempts: {e}")
            
            # If we get here, all retries failed
            raise last_exception
        
        return wrapper
    return decorator


class CircuitBreaker:
    """Circuit breaker pattern implementation for fault tolerance."""
    
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: float = 60.0,
        expected_exception: Type[Exception] = Exception
    ):
        """Initialize circuit breaker.
        
        Args:
            failure_threshold: Number of failures before opening circuit
            recovery_timeout: Time to wait before attempting recovery
            expected_exception: Exception type that triggers circuit breaker
        """
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half-open
        
        self.logger = get_logger(__name__, component="circuit_breaker")
    
    def __call__(self, func: Callable) -> Callable:
        """Decorator to apply circuit breaker to a function."""
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return self._call_with_circuit_breaker(func, *args, **kwargs)
        return wrapper
    
    def _call_with_circuit_breaker(self, func: Callable, *args, **kwargs):
        """Execute function with circuit breaker logic."""
        if self.state == "open":
            if self._should_attempt_reset():
                self.state = "half-open"
                self.logger.info("Circuit breaker moving to half-open state")
            else:
                raise DataQAError(
                    "Circuit breaker is open",
                    user_message="Service is temporarily unavailable. Please try again later.",
                    error_code="CIRCUIT_BREAKER_OPEN"
                )
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        
        except self.expected_exception as e:
            self._on_failure()
            raise
    
    def _should_attempt_reset(self) -> bool:
        """Check if enough time has passed to attempt reset."""
        if self.last_failure_time is None:
            return True
        
        return time.time() - self.last_failure_time >= self.recovery_timeout
    
    def _on_success(self):
        """Handle successful function execution."""
        if self.state == "half-open":
            self.state = "closed"
            self.failure_count = 0
            self.logger.info("Circuit breaker reset to closed state")
    
    def _on_failure(self):
        """Handle failed function execution."""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = "open"
            self.logger.warning(
                f"Circuit breaker opened after {self.failure_count} failures"
            )


class GracefulDegradation:
    """Utilities for graceful degradation when services fail."""
    
    @staticmethod
    def with_fallback(
        primary_func: Callable,
        fallback_func: Callable,
        fallback_exceptions: tuple[Type[Exception], ...] = (Exception,),
        logger_name: Optional[str] = None
    ) -> Callable:
        """Execute primary function with fallback on failure.
        
        Args:
            primary_func: Primary function to execute
            fallback_func: Fallback function to execute on failure
            fallback_exceptions: Exception types that trigger fallback
            logger_name: Logger name for fallback logging
        
        Returns:
            Function that executes primary with fallback
        """
        def wrapper(*args, **kwargs):
            logger = get_logger(logger_name or primary_func.__module__)
            
            try:
                return primary_func(*args, **kwargs)
            except fallback_exceptions as e:
                logger.warning(
                    f"Primary function {primary_func.__name__} failed, using fallback: {e}"
                )
                try:
                    return fallback_func(*args, **kwargs)
                except Exception as fallback_error:
                    logger.error(
                        f"Fallback function {fallback_func.__name__} also failed: {fallback_error}"
                    )
                    # Re-raise original error
                    raise e
        
        return wrapper
    
    @staticmethod
    async def with_fallback_async(
        primary_func: Callable,
        fallback_func: Callable,
        fallback_exceptions: tuple[Type[Exception], ...] = (Exception,),
        logger_name: Optional[str] = None
    ) -> Callable:
        """Async version of with_fallback."""
        async def wrapper(*args, **kwargs):
            logger = get_logger(logger_name or primary_func.__module__)
            
            try:
                if asyncio.iscoroutinefunction(primary_func):
                    return await primary_func(*args, **kwargs)
                else:
                    return primary_func(*args, **kwargs)
            except fallback_exceptions as e:
                logger.warning(
                    f"Primary function {primary_func.__name__} failed, using fallback: {e}"
                )
                try:
                    if asyncio.iscoroutinefunction(fallback_func):
                        return await fallback_func(*args, **kwargs)
                    else:
                        return fallback_func(*args, **kwargs)
                except Exception as fallback_error:
                    logger.error(
                        f"Fallback function {fallback_func.__name__} also failed: {fallback_error}"
                    )
                    # Re-raise original error
                    raise e
        
        return wrapper


# Convenience functions for common retry patterns
def retry_on_connection_error(max_attempts: int = 3, base_delay: float = 1.0):
    """Retry decorator specifically for connection errors."""
    config = RetryConfig(
        max_attempts=max_attempts,
        base_delay=base_delay,
        retryable_exceptions=(ConnectionError, TimeoutError, OSError)
    )
    return retry_sync(config)


def retry_on_rate_limit(max_attempts: int = 5, base_delay: float = 2.0):
    """Retry decorator specifically for rate limit errors."""
    config = RetryConfig(
        max_attempts=max_attempts,
        base_delay=base_delay,
        max_delay=120.0,  # Longer max delay for rate limits
        retryable_exceptions=(RetryableError,)
    )
    return retry_async(config)

