# Evaluate Your Agent

This guide shows you how to evaluate your agent's performance using the benchmarking suite. This is crucial for tracking improvements and catching regressions as you refine your knowledge assets.

---

## Why Evaluate?

- **Measure Accuracy:** Objectively determine if your agent is answering questions correctly
- **Track Progress:** See if changes to your `schema.yml`, `rules.yml`, or `examples.yml` are making the agent smarter
- **Prevent Regressions:** Ensure that fixing one type of query doesn't break another

---

## Preparing Test Data

Create a YAML file containing a list of questions you expect your agent to answer correctly.

**Example: `test_questions.yml`**
```yaml
# A friendly name for your test suite
use_case: "Sales Agent Evaluation"

# A list of test items
data:
  - id: "sales_total_revenue"
    question: "What is the total revenue of all sales?"
    # The expected SQL query the agent should generate
    solution:
      - function_arguments:
          sql: "SELECT SUM(revenue) FROM sales_report;"
    # Optional: A ground truth text answer for LLM-based judging
    ground_truth_output: "The total revenue is $27,650."
    active: true

  - id: "sales_by_region"
    question: "Show me the total revenue per region."
    solution:
      - function_arguments:
          sql: "SELECT region, SUM(revenue) FROM sales_report GROUP BY region;"
    active: true

  - id: "sales_last_month"
    question: "What was the revenue for last month?"
    solution:
      - function_arguments:
          sql: |
            SELECT SUM(revenue) 
            FROM sales_report 
            WHERE sales_date >= DATE('now', 'start of month', '-1 month')
              AND sales_date < DATE('now', 'start of month');
    active: true
```

### Test Data Fields

- **`id`**: A unique identifier for the test case
- **`question`**: The user query to test
- **`solution`**: The ground truth, typically the exact SQL you expect the agent to generate
- **`ground_truth_output`**: Optional final text answer, used for LLM-based judging
- **`active`**: Whether this test case should be run (set to `false` to skip)

---

## Running the Benchmark

The benchmarking suite is run from the command line. You point it to your agent's configuration and your test data file.

### Basic Usage

```bash
# Set your LLM environment variables first!
export AZURE_OPENAI_API_KEY="..."
export OPENAI_API_BASE="..."

# Run the test
python -m dataqa.core.components.knowledge_extraction.rule_inference_batch_test \
    --config /path/to/your/agent.yaml \
    --test-data /path/to/your/test_questions.yml
```

### Command Options

- **`--config`**: Path to your `agent.yaml` configuration file
- **`--test-data`**: Path to your test questions YAML file
- **`--output-dir`**: Optional. Directory to save results (defaults to `temp/`)

---

## Interpreting Results

The benchmark script will output:

### Console Output
- **Execution Logs:** Shows each test question and the agent's generated SQL
- **Summary:** Overall pass/fail statistics

### Result Files

Results are saved in Excel (`.xlsx`) and Pickle (`.pkl`) files in the output directory, containing:

- **`question`**: The original user question
- **`expected_sql`**: The ground truth SQL
- **`generated_sql`**: The SQL generated by the agent
- **`llm_label`**: Correct/Wrong (if `ground_truth_output` was provided and evaluated)
- **`match_score`**: Similarity score between expected and generated SQL

### Analyzing Results

1. **Compare SQL:** Look at the `expected_sql` vs `generated_sql` columns in the Excel file
2. **Identify Patterns:** Group failures by type (wrong table, wrong column, missing filter, etc.)
3. **Root Cause Analysis:**
   - Wrong table/column → Improve `schema.yml` descriptions
   - Missing business logic → Add `rules.yml`
   - Complex query pattern → Add `examples.yml`

---

## Iterative Improvement Workflow

The typical workflow is:

1. **Run the benchmark** and identify failing test cases
2. **Analyze why it failed:**
   - Is a column description unclear?
   - Is a business rule missing?
   - Is there a complex query pattern that needs an example?
3. **Update your assets:**
   - Fix `schema.yml` descriptions
   - Add rules to `rules.yml`
   - Add examples to `examples.yml`
4. **Re-run the benchmark** to confirm the fix
5. **Check for regressions** - ensure no other tests broke
6. **Repeat**

---

## Example: Fixing a Failing Test

### Initial Test Result

**Question:** "What is the total revenue for active accounts?"

**Expected SQL:**
```sql
SELECT SUM(revenue) 
FROM accounts 
WHERE account_status = 'ACTIVE' 
  AND last_transaction_date >= DATE('now', '-90 days');
```

**Generated SQL:**
```sql
SELECT SUM(revenue) 
FROM accounts 
WHERE account_status = 'ACTIVE';
```

**Issue:** Missing the "last 90 days" filter for active accounts.

### Fix: Add a Rule

Add to `rules.yml`:
```yaml
rules:
  - module_name: "retrieval_worker"
    rules:
      - rule_name: "active_account_definition"
        instructions: |
          - An active account is defined as: account_status = 'ACTIVE' AND last_transaction_date >= DATE('now', '-90 days')
          - When filtering for active accounts, always use this exact WHERE clause.
```

### Re-run and Verify

After adding the rule, re-run the benchmark. The test should now pass.

---

## Best Practices

1. **Start Small:** Begin with 5-10 high-quality test cases covering your most important queries
2. **Cover Edge Cases:** Include ambiguous queries, complex calculations, and edge cases
3. **Maintain Test Data:** Keep your test data file up to date as your schema evolves
4. **Automate:** Integrate benchmarking into your development workflow
5. **Document:** Add comments to test cases explaining why they're important

---

## Advanced: LLM-Based Judging

If you provide `ground_truth_output` in your test data, the benchmark can use an LLM to judge whether the agent's final answer matches the expected answer, even if the SQL differs slightly.

This is useful for:
- Queries where multiple SQL approaches are valid
- Testing the agent's ability to format and explain results
- Evaluating end-to-end performance

---

## Next Steps

- **[Create Knowledge Assets Manually](creating_knowledge_assets.md)**: Improve your assets based on evaluation results
- **[Configure Your Agent](configuring_your_agent.md)**: Configure your agent for evaluation
- **[API Reference](../reference/index.md)**: Detailed configuration reference

