I am trying to solve the problem of natural language querying against databases. This usually involves 3 steps at a high level
* Data Querying
* Data Analysis
* Data Visualization
Data Querying: Databases sometimes can be queried using SQL (Snowflake, Databricks, Redshift, etc..). Sometimes we don't have the access to the direct database and instead we will do an API call or a function call to retrive the data.
Data Analytics: We either generate the code for analytics or we could use the set of pre-built functions we have.
Data Visualization: We either generate the code for visualization or we could use the set of pre-built functions we have


dataqa - A config driven python library for natural language querying of databases
The users should have the ability to build Agents or Workflows (Workflows are systems where LLMs and tools are orchestrated through predefined code paths. Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks) depending on their needs. Everything should be config driven.

We want to build this library in a way that its easy to switch/add support to new libraries in the future. Currently we want to support Langgraph to begin with; (meaning the workflows and agents we create will be langgraph based). Its important to keep in mind that if we want to add CrewAI or Autogen or Pydantic AI, we should be able to add it without changing a lot of the existing code. So, do not fully couple the library with langgraph.

We also don't want to be in the business to provide multiple connectors to different databases, instead we will ask the user to provide a code execution api (data api) to take the code generated by the dataqa library and execute it their apis and send the results back. I am not exactly sure how do we handle the back and forth here; meaning if we get error, how do we go back and regenerate the code. Is MCP servers a better idea here than a traditional API? I really don't know. Need ideas here

In order to provide the option of both Agents and Workflows, we need to build a bunch of components which we should be able to use to build a workflow graph as well as use the component as a tool for any Agent. There should be a base component and the rest of the components should derive from it. We can have some hierarchy here. for example. code_executor/base can be derived from the base component and in_memory code executor and api based code executor can be derived from code_executor/base. We should also have a generic LLM generation component which we can use to build any LLM related components. These are just some ideas.

When it comes to Agent, I am thinking it will be multi agent system where there will be a high level agent which can call other agents like data querying agent, data analysis agent, data visualization agent. These agents can inturn use the components built abvove as tools.


Data Querying is the most important step in this. This usually requires the assets below
    * business rules
    * examples (query-code pairs with an optional reasoning)
    * schema of the databases (table name, table description, column name, column description, column type, column values)
The users will provide these assets via yaml/txt files and we should be able to ingest them in a knowledge base  like opensearch (for example or a simple local store for testing) and use them to retrieve.
For retrieval we will do multiple ways: dense retrival (embedding vector based), sparse retrieval (bm35), hybrid (combination of sparse and dense), tag based where the users can have tags assigned to each business rule, example and schema. we can then do tag based retrival along with other types too.

Usually for data querying,
    * we rewrite the user question,
    * retrieve the schema, examples, business rules that is relevant to the current context
    * compose the final prompt with the retrieved assets which we send to LLM for code generation steps
    * we then send the code generated to the code execution api to get the result bad


Guidelines:
* For python environment management use poetry
* Sphinx style documentation
* Readme with a section to use the library and a section for developers
* Use pytest for testing
* Use ruff for formatting and linting
* Use YAML based config
* The system support real-time interaction or only batch processes (e.g., for agents and workflows)?
* To handle iterative code regeneration on execution failure â€” depending on the type of error we may want to retry logic with error analysis, or add human-in-the-loop mechanisms
* For LangGraph, lets use Graph API style when building workflows or Agents
* Lets start with AzureOpenAI for LLM


Can you create me a detailed breakdown of steps which i can feed to an LLM so i can make it generate the entire library. As many details as possible. The entire library should be config driven.
