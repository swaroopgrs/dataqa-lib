Directory Structure:
dataqa/
├── __init__.py
├── errors.py
├── state.py
├── components/
│   ├── __init__.py
│   ├── base_component.py
│   ├── base_utils.py
│   ├── gather.py
│   ├── code_executor/
│   │   ├── __init__.py
│   │   ├── base_code_executor.py
│   │   └── in_memory_code_executor.py
│   ├── langgraph_conditional_edge/
│   │   ├── __init__.py
│   │   ├── base_conditional_edge.py
│   │   └── categorical_variable_condition.py
│   ├── llm/
│   │   ├── __init__.py
│   │   ├── base_llm_component.py
│   │   └── base_prompt_llm_chain.py
│   ├── prompt/
│   │   ├── __init__.py
│   │   └── base_prompt.py
│   └── retriever/
│       ├── __init__.py
│       ├── base_retriever.py
│       └── tag_retriever.py
├── data_models/
│   ├── __init__.py
│   └── asset_models.py
├── llm/
│   ├── __init__.py
│   ├── base_llm.py
│   └── openai.py
├── pipelines/
│   ├── __init__.py
│   ├── constants.py
│   ├── pipeline.py
│   └── schema.py
└── utils/
    ├── __init__.py
    ├── component_utils.py
    ├── data_model_util.py
    ├── ingest_knowledge.py
    └── utils.py


File: dataqa/__init__.py
=======================================


File: dataqa/errors.py
=======================================
from typing import Optional


class PipelineConfigError(Exception):
    def __init__(self, message: Optional[str] = None):
        super().__init__()
        self.message = message

    def __str__(self):
        return self.message

    def __repr__(self):
        return str(self)


File: dataqa/state.py
=======================================
from pydantic import BaseModel, Field
from typing import List, Union, Any
from datetime import date, datetime
from dataqa.components.code_executor.base_code_executor import CodeExecutorOutput


class PipelineError(BaseModel):
    error_code: int
    error_type: str
    error_message: str


class PipelineInput(BaseModel):
    query: str = Field(description="the input query.")
    context: List[str] = (
        Field(
            default_factory=list, description="the conversation History"
        )
    )
    previous_rewritten_query: str = Field(
        default="",
        description="the `rewritten_query` field from the last state in the same conversation.",
    )
    datetime: str = Field(default=str(datetime.today()), description="current datetime")


class PipelineOutput(BaseModel):
    rewritten_query: str = Field(
        default="None",
        description="""
        The newly generated rewritten query for the input query. 
        Any rewriter components should always save rewritten query to this field.
        """,
    )
    code: str = Field(default="", description="the final generated code to be returned")
    execution_output: CodeExecutorOutput = Field(
        default=None,
        description="execution output, containing dataframes, texts, images etc",  # TODO list?
    )
    text: str = Field(
        default="", description="any textual output generated from LLM pipeline"
    )


class BasePipelineState(BaseModel):
    # static input fields
    input: PipelineInput = Field(description="the input to a pipeline")
    return_output: PipelineOutput = Field(
        default=None, description="The output that may be displayed to users."
    )

    # metadata
    total_time: float = Field(default=0, description="Pipeline running time")
    error: Union[PipelineError, None] = Field(
        default=None, description="Save the exception occured during pipeline execution"
    )
    full_state: Any = Field(
        default=None,
        description="Return full pipeline state for debugging and logging purpose",
    )


File: dataqa/components/__init__.py
=======================================


File: dataqa/components/base_component.py
=======================================
from pydantic import BaseModel, Field
from typing import Any, Dict, Optional, Union, Type
from abc import ABC, abstractmethod
import logging
from langchain_core.runnables.config import RunnableConfig
from pydantic import BaseModel, Field

from dataqa.components.base_utils import import get_field

logger = logging.getLogger(__name__)


class Variable(BaseModel):
    """Define a variable, can be used as the input or output for a tool."""

    name: str
    type: str
    description: Optional[str] = None
    optional: Optional[bool] = Field(
        default=False, description="If this variable is optional in the input"
    )
    default: Optional[Any] = Field(
        default=None, description="If the variable has a default value."
    )


class OutputVariable(Variable):
    display: Optional[bool] = Field(
        default=True,
        description="If this variable appears in the output message to the orchestrator",
    )


class ComponentInput(BaseModel):
    """Base input for all components"""

    # Actual input models for the components are defined in the component classes
    # DISCUSS: component_name, component_type will be used for logging. we could also think about if we can use them as part of .run() method
    component_name: str = Field(description="Name of the target component")
    component_type: str = Field(description="Type of the target component")
    metadata: Dict[str, Any] = Field(
        default_factory=dict, description="Metadata about the input"
    )
    # run_mode: langgraph


class ComponentOutput(BaseModel):
    """Base output for all components."""

    output_data: Any = Field(description="Output data of the component")
    # DISCUSS: component_name, component_type will be used for logging. we could also think about if we can use them as part of .run() method
    component_name: str = Field(
        description="Name of the component that produced this output"
    )
    component_type: str = Field(description="Type of the component")
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Metadata about the output (e.g., processing time, tokens)",
    )


class ComponentConfig(BaseModel):
    """Base configuration for all components."""

    name: str = Field(description="Name of the component instance")


class Component(ABC):
    """Abstract base class for all components"""

    is_component = True

    def __init__(self, config: Union[ComponentConfig, Dict] = None, **kwargs) -> None:
        self.config = config
        if isinstance(config, Dict):
            self.config = self.config_base_model(**config)
        if not config:
            self.config = self.config_base_model(**kwargs)
        self.input_mapping: Dict[str, str] = None

    @property
    @abstractmethod
    def config_base_model(self) -> Type[BaseModel]:
        raise NotImplementedError

    @property
    @abstractmethod
    def component_type(self) -> str:
        raise NotImplementedError

    @property
    @abstractmethod
    def input_base_model(self) -> Type[BaseModel]:
        raise NotImplementedError

    @property
    @abstractmethod
    def output_base_model(self) -> Type[BaseModel]:
        raise NotImplementedError

    @abstractmethod
    async def run(
        self, input_data: ComponentInput, config: RunnableConfig
    ) -> ComponentOutput:
        """Abstract method to execute the component's logic"""
        pass

    @abstractmethod
    def display(self):
        pass

    def set_input_mapping(self, mapping):
        # validate
        for field in mapping:
            assert field in self.input_base_model.__fields__  # TODO field is optional

        self.input_mapping = mapping

    async def __call__(self, state, config: Optional[RunnableConfig] = {}):
        # build input data from state
        input_data = {}
        for field, mapped_field in self.input_mapping.items():
            input_data[field] = get_field(state, mapped_field)

        input_data = self.input_base_model(**input_data)

        # run
        response = await self.run(input_data=input_data, config=config)

        # validate output and update state
        assert isinstance(response, self.output_base_model)

        return {f"{self.config.name}_output": response}


File: dataqa/components/base_utils.py
=======================================
from pydantic import BaseModel


def get_field(model: BaseModel, field: str):
    try:
        fields = field.split(".")
        for field in fields:
            model = getattr(model, field)
        return model
    except AttributeError as e:
        raise e


File: dataqa/components/gather.py
=======================================
import logging
from dataqa.components.base_component import Component, ComponentConfig
from dataqa.state import PipelineOutput

logger = logging.getLogger(__name__)


class GatherOutput(Component):
    config_base_model = ComponentConfig
    input_base_model = PipelineOutput
    output_base_model = PipelineOutput
    component_type = "GatherOutput"

    def display(self):
        logger.info("Gather PipelineOutput")

    async def run(self, input_data, config):
        return input_data


File: dataqa/components/code_executor/__init__.py
=======================================


File: dataqa/components/code_executor/base_code_executor.py
=======================================
from abc import ABC, abstractmethod
from typing import Any, List

from pydantic import Field, BaseModel
from dataqa.components.base_component import (
    Component,
    ComponentConfig,
    ComponentInput,
    ComponentOutput,
)


class CodeExecutorOutput(BaseModel):
    code: str = ""
    dataframe: List[str] = Field(default_factory=list)
    image_byte_str: List[str] = Field(default_factory=list)
    html: str = ""
    markdown: str = ""
    running_log: str = ""
    error: str = ""


class CodeExecutorConfig(ComponentConfig):
    component_type: str = Field(
        description="Type of the component (e.g., InMemoryExecutor)"
    )


class CodeExecutor(Component, ABC):
    config: CodeExecutorConfig

    def __init__(self, config: CodeExecutorConfig):
        super().__init__(config)

    @abstractmethod
    def run(self, input_data: Any) -> CodeExecutorOutput:
        pass


File: dataqa/components/code_executor/in_memory_code_executor.py
=======================================
from dataqa.components.base_component import (
    Component,
    ComponentConfig,
    OutputVariable,
    Variable,
)
from dataqa.components.component_utils import build_base_model_from_parameters
from dataqa.utils.data_model_util import create_base_model
from pydantic import Field
from typing import Union, List, Dict, Any
from dataqa.components.code_executor.base_code_executor import (
    CodeExecutorOutput,
)
import duckdb
from pathlib import Path
import pandas as pd
import logging

logger = logging.getLogger(__name__)


class InMemoryCodeExecutorConfig(ComponentConfig):
    data_files: Any = Field(
        description="List of dictionaries containing 'path' to the CSV file and 'table_name' for the DuckDB table"
    )
    input: List[Variable] = Field(
        description="the schema of input parameters", default=[]
    )
    output: List[OutputVariable] = Field(
        description="the schema of output parameters", default=[]
    )


class InMemoryCodeExecutor(Component):
    component_type = "InMemoryCodeExecutor"
    config_base_model = InMemoryCodeExecutorConfig
    input_base_model = "dynamically built"
    output_base_model = CodeExecutorOutput

    def __init__(
        self, config: Union[InMemoryCodeExecutorConfig, Dict], **kwargs
    ) -> None:
        super().__init__(config=config, **kwargs)
        self.input_base_model = build_base_model_from_parameters(
            base_model_name=f"{self.config.name}_input", parameters=self.config.input
        )
        self.connection = duckdb.connect(database=":memory:")
        self.load_data_into_duckdb()

    def display(self):
        logger.info(f"Component Name: {self.config.name}")
        logger.info(f"Component Type: {self.component_type}")
        logger.info(f"Input BaseModel: {self.input_base_model.__fields__}")
        logger.info(f"Output BaseModel: {self.output_base_model.__fields__}")
    
    def load_dataframe(self, path):
        if path.endswith(".csv"):
            return pd.read_csv(path)
        elif path.endswith(".xlsx"):
            return pd.read_excel(path)
        else:
            raise NotImplementedError

    def load_data_into_duckdb(self):
        for data_file in self.config.data_files:
            path = data_file["path"]
            table_name = data_file["table_name"]
            dataframe = self.load_dataframe(path)
            self.connection.register("data", dataframe)
            self.connection.execute(f"CREATE TABLE {table_name} AS SELECT * FROM data")

    def run(self, input_data, config: Dict) -> CodeExecutorOutput:
        try:
            result_df = self.connection.execute(input_data.code).fetchdf()
            response = CodeExecutorOutput(
                code=input_data.code,
                dataframe=result_df.to_json(index=False),
            )
        except Exception as e:
            response = CodeExecutorOutput(code=input_data.code, error=str(e))
        return response


File: dataqa/components/langgraph_conditional_edge/__init__.py
=======================================


File: dataqa/components/langgraph_conditional_edge/base_conditional_edge.py
=======================================
from abc import ABC, abstractmethod
from pydantic import BaseModel, Field
from typing import Coroutine, List, Optional, Union, Dict, Literal
from langchain_core.runnables.config import RunnableConfig
from langgraph.constants import START, END
from dataqa.components.base_component import Component, ComponentConfig
from dataqa.components.base_utils import import get_field
from dataqa.pipelines.constants import PIPELINE_START, PIPELINE_END


class Condition(BaseModel):
    output: str = Field(description="the name of target node")


class BaseConditionalEdgeConfig(ComponentConfig):
    condition: List[Condition] = Field(
        description="the config of every condition"
    )
    default_output: Optional[str] = Field(
        default="__end__",
        description="the output if failed to meet any conditions",
    )


class BaseConditionalEdge(Component, ABC):
    is_conditional_edge = True
    config_base_model = BaseConditionalEdgeConfig
    output_base_model = str
    config: BaseConditionalEdgeConfig

    def __init__(
        self, config: Union[ComponentConfig, Dict] = None, **kwargs
    ) -> None:
        super().__init__(config=config, **kwargs)
        for condition in self.config.condition:
            if condition.output == PIPELINE_START:
                condition.output = START
            if condition.output == PIPELINE_END:
                condition.output = END

    @abstractmethod
    def check_condition(self, condition, input_data, **kwargs) -> bool:
        """
        Return True if the condition is met, False otherwise.
        """
        raise NotImplementedError

    def get_function(self) -> Coroutine:
        """
        Return a function pointer as the callable of the conditional edge.
        Add annotated types.
        """
        valid_args = []
        for condition in self.config.condition:
            valid_args.append(condition.output)
        valid_args.append(self.config.default_output)
        valid_args = list(set(valid_args))

        literal_type_str = f"Literal[{', '.join(repr(s) for s in valid_args)}]"

        async def func(state, config) -> eval(literal_type_str):
            return await self.__call__(state, config)

        return func

    async def __call__(self, state, config: Optional[RunnableConfig] = {}):
        # build input data from state
        input_data = {}
        for field, mapped_field in self.input_mapping.items():
            input_data[field] = get_field(state, mapped_field)

        input_data = self.input_base_model(**input_data)

        # run
        response = await self.run(input_data=input_data, config=config.get('configurable', {}))

        # validate output and update state
        assert isinstance(response, self.output_base_model)

        return response


File: dataqa/components/langgraph_conditional_edge/categorical_variable_condition.py
=======================================
import logging
from typing import List, Dict, Any, Union
from pydantic import Field, BaseModel
from dataqa.components.langgraph_conditional_edge.base_conditional_edge import (
    BaseConditionalEdge,
    BaseConditionalEdgeConfig,
    Condition,
)

logger = logging.getLogger(__name__)


class CategoricalVariableCondition(Condition):
    values: List[Any] = Field(description="allowed values")


class CategoricalVariableConditionEdgeConfig(BaseConditionalEdgeConfig):
    condition: List[CategoricalVariableCondition]


class CategoricalVariableConditionInput(BaseModel):
    variable: Any = Field(description="the variable to check in conditions")


class CategoricalVariableConditionEdge(BaseConditionalEdge):
    component_type = "CategoricalVariableConditionEdge"
    config_base_model = CategoricalVariableConditionEdgeConfig
    input_base_model = CategoricalVariableConditionInput
    config: CategoricalVariableConditionEdgeConfig

    def __init__(
        self, config: Union[CategoricalVariableConditionEdgeConfig, Dict] = None, **kwargs
    ) -> None:
        super().__init__(config=config, **kwargs)

    def check_condition(self, condition: CategoricalVariableCondition, input_data: CategoricalVariableConditionInput) -> bool:
        """
        Return True if the condition is met, False otherwise.
        """
        if input_data.variable in condition.values:
            return True
        return False

    def display(self):
        logger.info(f"Component Name: {self.config.name}")
        logger.info(f"Component Type: {self.component_type}")
        logger.info(f"Input BaseModel: {self.input_base_model.__fields__}")
        logger.info(f"Output BaseModel: {self.output_base_model.__fields__}")

    async def run(self, input_data: CategoricalVariableConditionInput, config: Dict):
        for condition in self.config.condition:
            if self.check_condition(condition, input_data):
                logger.debug(
                    f"'{input_data.variable}' matches condition {condition.values}.\nNext node is {condition.output}"
                )
                return condition.output
        logger.debug(
            f"No condition is matched by value {input_data.variable}.\nNext node is {self.config.default_output}"
        )
        return self.config.default_output


File: dataqa/components/llm/__init__.py
=======================================


File: dataqa/components/llm/base_llm_component.py
=======================================
from typing import List, Literal, Union, Dict, Tuple
from pydantic import Field, BaseModel
import logging
from dataqa.llm.base_llm import BaseLLM
from dataqa.components.base_component import (
    OutputVariable,
    RunnableConfig,
    Component,
    ComponentConfig,
)
from dataqa.utils.component_utils import build_base_model_from_parameters, extract


logger = logging.getLogger(__name__)


class BaseLLMComponentConfig(ComponentConfig):
    output: List[OutputVariable] = Field(
        description="the schema of output parameters", default=[]
    )
    output_parser: Literal["basemodel", "xml"] = Field(
        default="basemodel",
        description="""How to parse the llm generation to output_base_model.
- Default is 'basemodel', use llm.with_structured_output(output_base_model)
- If use 'xml', manually parse every field of 'output_base_model' as text between <field> </field>
""",
    )


class BaseLLMComponentInput(BaseModel):
    messages: List[Tuple[str, str]] = Field(description="The input messages")


class BaseLLMComponent(Component):
    component_type = "BaseLLMComponent"
    config_base_model = BaseLLMComponentConfig
    input_base_model = BaseLLMComponentInput
    output_base_model = "build dynamically from config.output"
    # prompt: ChatPromptTemplate # TODO should prompt be a str or a list of messages
    config: BaseLLMComponentConfig

    def __init__(
        self,
        llm: BaseLLM,
        config: Union[ComponentConfig, Dict] = None,
        **kwargs,
    ):
        super().__init__(config=config, **kwargs)
        self.llm = llm
        # self.prompt = prompt
        self.output_base_model = build_base_model_from_parameters(
            base_model_name=f"{self.config.name}_output", parameters=self.config.output
        )
        if self.config.output_parser == "basemodel":
            self.llm.with_structured_output = self.output_base_model

    def display(self):
        logger.info(f"Component Name: {self.config.name}")
        logger.info(f"Component Type: {self.component_type}")
        logger.info(f"Input BaseModel: {self.input_base_model.__fields__}")
        logger.info(f"Output BaseModel: {self.output_base_model.__fields__}")

    async def run(self, input_data, config: RunnableConfig = {}):
        # logger.info(f"Run {self.config.component_name} with input: {input_data.model_dump_json(indent=4)}")

        assert isinstance(input_data, self.input_base_model)

        api_key = config.get("configurable", {}).get("api_key", "")
        base_url = config.get("configurable", {}).get("base_url", "")

        if self.config.output_parser == "basemodel":
            with_structured_output = self.output_base_model
        else:
            with_structured_output = None

        response = await self.llm.ainvoke(
            messages=input_data.messages, # TODO validation
            api_key=api_key,
            base_url=base_url,
            from_component=self.config.name,
            with_structured_output=with_structured_output,
        )

        if self.config.output_parser == "xml":
            assert isinstance(response.generation, str)
            response.generation = self.output_base_model( # TODO validation
                **{
                    field: extract(response.generation, f"<{field}>", f"</{field}>")
                    for field in self.output_base_model.__fields__
                }
            )

        assert isinstance(response.generation, self.output_base_model)

        # logger.info(
        #     f"{self.config.name} gets response {response.generation.model_dump_json(indent=4)}"
        # )
        return response.generation # TODO return raw llm response to a list


File: dataqa/components/llm/base_prompt_llm_chain.py
=======================================
from typing import List, Literal, Union, Dict, Tuple
from pydantic import Field, BaseModel
import logging
from langchain_core.prompts import ChatPromptTemplate
from dataqa.llm.base_llm import BaseLLM
from dataqa.components.base_component import (
    OutputVariable,
    RunnableConfig,
    Component,
    ComponentConfig,
    Variable,
)
from dataqa.utils.component_utils import build_base_model_from_parameters, extract


logger = logging.getLogger(__name__)


class BasePromptLLMChainConfig(ComponentConfig):
    prompt: str = Field()
    input: List[Variable] = Field(
        description="the schema of input parameters", default=[]
    )
    output: List[OutputVariable] = Field(
        description="the schema of output parameters", default=[]
    )
    output_parser: Literal["basemodel", "xml"] = Field(
        default="basemodel",
        description="""How to parse the llm generation to output_base_model.
- Default is 'basemodel', use llm.with_structured_output(output_base_model)
- If use 'xml', manually parse every field of 'output_base_model' as text between <field> </field>
""",
    )


class BasePromptLLMChain(Component):
    component_type = "BasePromptLLMChain"
    config_base_model = BasePromptLLMChainConfig
    input_base_model = "build dynamically from config.input"
    output_base_model = "build dynamically from config.output"
    # prompt: ChatPromptTemplate # TODO should prompt be a str or a list of messages
    config: BasePromptLLMChainConfig

    def __init__(
        self,
        llm: BaseLLM,
        config: Union[BasePromptLLMChainConfig, Dict] = None,
        **kwargs,
    ):
        super().__init__(config=config, **kwargs)
        self.llm = llm
        self.prompt = ChatPromptTemplate.from_messages( # TODO assume that the prompt is a str
            [("system", self.config.prompt)]
        )
        self.input_base_model = build_base_model_from_parameters(
            base_model_name=f"{self.config.name}_input", parameters=self.config.input
        )
        self.output_base_model = build_base_model_from_parameters(
            base_model_name=f"{self.config.name}_output", parameters=self.config.output
        )
        # add structured output
        self.llm.config.with_structured_output = self.output_base_model
        self.validate_llm_input()

    def validate_llm_input(self):
        for field in self.prompt.input_schema.__annotations__:
            assert field in self.input_base_model.__annotations__, (
                f"The prompt of {self.config.name} requires '{field}' as input, but it is not defined the input BaseModel"
            )

    def display(self):
        logger.info(f"Component Name: {self.config.name}")
        logger.info(f"Component Type: {self.component_type}")
        logger.info(f"Input BaseModel: {self.input_base_model.__fields__}")
        logger.info(f"Output BaseModel: {self.output_base_model.__fields__}")

    async def run(self, input_data, config: RunnableConfig = {}):
        logger.info(f"Run {self.config.name} with input: {input_data.model_dump_json(indent=4)}")

        assert isinstance(input_data, self.input_base_model)
        messages = self.prompt.invoke(input_data.model_dump())

        api_key = config.get("configurable", {}).get("api_key", "")
        base_url = config.get("configurable", {}).get("base_url", "")

        if self.config.output_parser == "basemodel":
            with_structured_output = self.output_base_model
        else:
            with_structured_output = None

        response = await self.llm.ainvoke(
            messages=messages, # TODO validation
            api_key=api_key,
            base_url=base_url,
            from_component=self.config.name,
            with_structured_output=with_structured_output,
        )

        if self.config.output_parser == "xml":
            assert isinstance(response.generation, str)
            response.generation = self.output_base_model( # TODO validation
                **{
                    field: extract(response.generation, f"<{field}>", f"</{field}>")
                    for field in self.output_base_model.__fields__
                }
            )

        assert isinstance(response.generation, self.output_base_model)

        # logger.info(
        #     f"{self.config.name} gets response {response.generation.model_dump_json(indent=4)}"
        # )
        return response.generation # TODO return raw llm response to a list

File: dataqa/components/prompt/__init__.py
=======================================


File: dataqa/components/prompt/base_prompt.py
=======================================
import logging
from typing import List, Union, Dict, Tuple
from pydantic import Field, BaseModel
from langchain_core.prompts import ChatPromptTemplate
from dataqa.components.base_component import (
    Component,
    ComponentConfig,
    Variable,
    RunnableConfig,
)
from dataqa.utils.component_utils import build_base_model_from_parameters


logger = logging.getLogger(__name__)


class BasePromptConfig(ComponentConfig):
    prompt: str = Field()
    role: str = Field(default="system", description="the role of this generated prompt as a message")
    input: List[Variable] = Field(
        description="the schema of input parameters", default=[]
    )


class BasePromptOutput(BaseModel):
    messages: List[Tuple[str, str]] = Field(description="the generated prompt messages")


class BasePromptComponent(Component):
    component_type = "BasePrompt"
    config_base_model = BasePromptConfig
    input_base_model = "build dynamically from config.input"
    output_base_model = BasePromptOutput
    config: BasePromptConfig

    def __init__(self, config: Union[ComponentConfig, Dict] = None, **kwargs):
        super().__init__(config=config, **kwargs)
        self.prompt = ChatPromptTemplate.from_messages( # TODO assume that the prompt is a str
            [(self.config.role, self.config.prompt)]
        )
        self.input_base_model = build_base_model_from_parameters(
            base_model_name=f"{self.config.name}_input", parameters=self.config.input
        )
        self.validate_llm_input()

    def validate_llm_input(self):
        for field in self.prompt.input_schema.__annotations__:
            assert field in self.input_base_model.__annotations__, (
                f"The prompt of {self.config.name} requires '{field}' as input, but it is not defined the input BaseModel"
            )

    def display(self):
        logger.info(f"Component Name: {self.config.name}")
        logger.info(f"Component Type: {self.component_type}")
        logger.info(f"Input BaseModel: {self.input_base_model.__fields__}")
        logger.info(f"Output BaseModel: {self.output_base_model.__fields__}")

    async def run(self, input_data, config: RunnableConfig = {}):
        logger.info(f"Run {self.config.name} with input: {input_data.model_dump_json(indent=4)}")

        assert isinstance(input_data, self.input_base_model)
        messages = self.prompt.invoke(input_data.model_dump()).to_messages()
        return self.output_base_model(
            messages=[(message.type, message.content) for message in messages]
        )


File: dataqa/components/retriever/__init__.py
=======================================


File: dataqa/components/retriever/base_retriever.py
=======================================
from pydantic import BaseModel, Field
from typing import Any, Dict, List, Optional, Union, Type
from typing_extensions import Annotated
from abc import ABC, abstractmethod
from enum import Enum
from dataqa.components.base_component import (
    Component,
    ComponentConfig,
    ComponentInput,
    ComponentOutput,
)
from dataqa.data_models.asset_models import RetrievedAsset


class RetrievalMethod(Enum):
    TAG = "tag"
    VECTOR = "vector"
    HYBRID = "hybrid"


class RetrieverInput(ComponentInput):
    query: Any = Field(description="Query for the retrieval")


class RetrieverConfig(ComponentConfig):
    knowledge_base_index: str = Field(
        description="Identifier for the knowledge base index to query"
    )
    retrieval_method: RetrievalMethod = Field(
        description="retrieval algorithm or method"
    )
    parameters: Dict[str, Any] = Field(
        description="parameters of retriever component"
    )


class RetrieverOutput(ComponentOutput):
    retrieved_asset: List[RetrievedAsset] = Field(description="list of retrieved assets")


class Retriever(Component, ABC):
    config: RetrieverConfig
    knowledge_base_index: str
    retrieval_method: RetrievalMethod
    parameters: Dict[str, Any]

    def __init__(self, config: RetrieverConfig):
        super().__init__(config)
        self.knowledge_base_index = self.config.knowledge_base_index
        self.retrieval_method = self.config.retrieval_method
        self.parameters = self.config.parameters

    @abstractmethod
    def retrieve_assets(self, query: Any) -> List[RetrievedAsset]:
        pass

    async def run(
        self, input_data: RetrieverInput, config: Dict = {}
    ) -> RetrieverOutput:
        pass


File: dataqa/components/retriever/tag_retriever.py
=======================================
from typing import Dict, List, Any
from pydantic import BaseModel, Field
import logging
import time
import sys
import yaml
import os 
from datetime import datetime

from dataqa.components.base_component import Component, ComponentInput
from dataqa.components.retriever.base_retriever import (
    Retriever,
    RetrieverConfig,
    RetrieverInput,
    RetrieverOutput,
)
from dataqa.data_models.asset_models import RetrievedAsset
from dataqa.ingest_knowledge import KnowledgeBase
from dataqa.utils.data_model_util import create_base_model


logger = logging.getLogger(__name__)


class TagRetriever(Retriever):
    component_type = "TagRetriever"
    config_base_model = RetrieverConfig
    input_base_model = "dynamically built"
    output_base_model = "dynamically built"

    def __init__(
        self,
        config: Dict,
        knowledge_base: KnowledgeBase,
        input_config: List,
        output_config: List,
    ):
        """
        :param config: config dictionary of tag retriever component
        :param knowledge_base: knowledge base object
        """
        tag_retriever_config = RetrieverConfig.model_validate(config)
        super().__init__(tag_retriever_config)
        self.knowledge_base = knowledge_base.get_kb_by_index(
            self.knowledge_base_index
        )
        input_base_model_name = f"{self.config.name}_input"
        self.input_base_model = create_base_model(input_base_model_name, input_config)
        output_base_model_name = f"{self.config.name}_output"
        self.output_base_model = create_base_model(
            output_base_model_name, output_config, RetrieverOutput
        ) 
        self.output_field_name = output_config[0]["name"] # TODO add support for dynamic multiple output fields
        logger.info(
            f"Component {self.config.name} of type {self.component_type} created."
        )

    def display(self):
        logger.info(f"Component Name: {self.config.name}")
        logger.info(f"Component Type: {self.component_type}")
        logger.info(f"Input BaseModel: {self.input_base_model.__fields__}")
        logger.info(f"Output BaseModel: {self.output_base_model.__fields__}")

    def prepare_input(self, state: Dict[str, Any]):
        """
        temporary. to be replaced by generic component input preparation function
        :param state:
        :return:
        """
        input_data = self.input_base_model.model_validate(state)
        return input_data

    def retrieve_assets(self, query: RetrieverInput) -> List[RetrievedAsset]:
        """
        :param query: RetrieverInput for tag retrieval method
        :return: list of retrieved record
        """
        search_field = [r for r in self.input_base_model.model_fields]
        if isinstance(search_field, str):
            pass
        elif isinstance(search_field, list):
            if len(search_field) > 1:
                raise NotImplementedError(
                    f"Algorithm of multiple search fields for tag retriever is not implemented. Search field: {search_field}"
                )
            elif len(search_field) == 1:
                search_field = search_field[0]
        else:
            raise NotImplementedError(
                f"Algorithm of search fields of type {type(search_field)} for tag retriever is not implemented. Search field: {search_field}"
            )

        all_retrieved = []
        for record in self.knowledge_base["records"]:
            input_tag = getattr(query, search_field)
            record_tag = getattr(record, search_field)
            if self.validate(input_tag, record_tag):
                retrieved_record = {
                    "asset_type": self.knowledge_base["name"],
                    "content": record,
                    "relevance_score": 1,
                }
                retrieved_asset = RetrievedAsset.model_validate(retrieved_record)
                all_retrieved.append(retrieved_asset)
        logger.info(
            f"Finish {query}, retrieved {len(all_retrieved)} records of {self.knowledge_base['name']}."
        )
        return all_retrieved

    @staticmethod
    def validate(input_tag: List, asset_tag: List) -> bool:
        """
        :param input_tag: list of input tags
        :param asset_tag: list of tags of the asset record
        :return: boolean of whether the asset record should be selected
        """
        for conjunction in asset_tag:
            if not isinstance(conjunction, list):
                conjunction = [conjunction]
            f = True
            for predicate in conjunction:
                if predicate == "all":
                    return True
                if predicate[0] == "~" and predicate[1:] in input_tag:
                    f = False
                    break
                if predicate[0] != "~" and predicate not in input_tag:
                    f = False
                    break
            if f:
                return True
        return False

    async def run(self, input_data: RetrieverInput, config: Dict = {}) -> RetrieverOutput:
        """
        :param input_data: RetrieverInput for tag retrieval method
        :return: RetrieverOutput base model for retriever component output
        """
        # TODO filter fields of retrieved asset to base model of component output
        # param query: RetrieverInput for tag retrieval method
        # return output base model for retriever component
        start_time = time.time()
        retrieved_asset = self.retrieve_assets(input_data)
        retrieve_time = time.time() - start_time

        output_str_list = []
        for r in retrieved_asset:
            output_str_list.append(r.content.prompt)
        output_str = "\n\n".join(output_str_list)

        # print(f"{self.output_field_name}:\n{output_str}")

        component_output = {
            "component_name": self.config.name,
            "component_type": self.component_type,
            "output_data": retrieved_asset, 
            self.output_field_name: output_str,
            "metadata": {"time": retrieve_time},
        }
        return self.output_base_model.model_validate(component_output)


if __name__ == "__main__":
    config = yaml.safe_load(open("examples/ccb_risk/config/config_retriever.yml", "r"))
    my_kb = KnowledgeBase(config["components"][0]["parameters"]["config"])
    mock_state = {"tag": ["trade", "deposit"]}

    for component_config in config["components"][1:]:
        retriever_node_config = component_config["parameters"]
        r_config = retriever_node_config["config"]
        r_config["name"] = component_config["name"]
        r_input = retriever_node_config["input"]
        r_output = retriever_node_config["output"]
        tag_retriever = TagRetriever(r_config, my_kb, r_input, r_output)
        tag_retriever_input = tag_retriever.prepare_input(mock_state)
        my_retrieved_asset = tag_retriever.run(tag_retriever_input)
        print(
            f"Component {tag_retriever.config.name} of type {tag_retriever.component_type} created."
        )


File: dataqa/data_models/__init__.py
=======================================


File: dataqa/data_models/asset_models.py
=======================================
from pydantic import BaseModel, Field
from typing import Any, Dict, Optional


class RetrievedAsset(BaseModel):
    """
    Data model for a retrieved knowledge asset at record level.
    """

    asset_type: str = Field(
        description="Type of the asset (e.g., 'schema', 'rule', 'example')"
    )
    content: Any = Field(
        description="Content of the retrieved asset (e.g., schema definition, rule text)"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict, description="Metadata about the asset (e.g., source)"
    )
    relevance_score: Optional[float] = Field(
        default=None, description="Optional relevant score assigned to the asset"
    )
    asset_id: Optional[str] = Field(
        default=None, description="Optional unique identifier for the asset"
    )


File: dataqa/llm/__init__.py
=======================================


File: dataqa/llm/base_llm.py
=======================================
from abc import ABC, abstractmethod
from pydantic import BaseModel, Field
from typing import List, TypeVar, Dict, Type, Any, Optional, Union
from langchain_core.messages.utils import AnyMessage


_BM = TypeVar("_BM", bound="BaseModel")
_DictOrPydanticClass = Union[Dict[str, Any], Type[_BM]]
_StrOrDictOrPydantic = Union[str, Dict, _BM]


class LLMConfig(BaseModel):
    model: str = Field(
        description="The model name, such as deployment_name for oai llm, such as 'gpt-4o-2024-08-06', but NOT model_name like 'gpt-4o'"
    )
    with_structured_output: Optional[Union[None, _DictOrPydanticClass]] = Field(
        default=None,
        description="""Parse raw llm generations to structured output.
The input is a dict or a BaseModel class.
""",
    )
    # with_tools TODO


class LLMMetadata(BaseModel):
    request_id: str = Field(
        description="A unique identifier for this LLM call. Usually provided by the LLM provider."
    )
    model: str = Field(description="The LLM model.")
    num_generations: int = Field(
        description="The number of generations requested in this LLM call."
    )
    start_timestamp: str = Field(
        description="Timestamp to send request. The preferred format is %a, %d %b %Y %H:%M:%S %Z, e.g. 'Tue, 04 Mar 2025 16:00:22 GMT'"
    )
    end_timestamp: str = Field(
        description="Timestamp to receive response. In the same format as start timestamp"
    )
    latency: float = Field(
        description="The latency between start and end timestamps in milliseconds"
    )
    input_token: int = Field(description="The number of input tokens")
    output_token: int = Field(
        description="The number of LLM completion tokens summed over all responses"
    )
    reasoning_token: Optional[int] = Field(
        default=0,
        description="The number of reasoning tokens. Use for reasoning models only such as GPT-01.",
    )
    cost: Union[None, float] = Field(
        default=None, description="The cost of this LLM call in dollars."
    )
    ratelimit_tokens: int = Field(
        description="The maximum number of tokens to reach rate limit"
    )
    ratelimit_remaining_tokens: int = Field(
        description="The number of remaining tokens to reach rate limit. By default"
    )
    ratelimit_requests: int = Field(
        description="The maximum number of requests to reach rate limit"
    )
    ratelimit_remaining_requests: int = Field(
        description="The number of remaining requests to reach rate limit"
    )


class LLMError(BaseModel):
    error_code: int
    error_type: str
    error_message: str


class LLMOutput(BaseModel):
    prompt: List = Field(description="The input prompt")
    generation: _StrOrDictOrPydantic = Field(
        description="""The raw LLM generations.
Parse to Dict or Pydantic BaseModel is the structured output is required.
"""
    )
    from_component: Optional[str] = Field(
        default="",
        description="""The name of component that triggers this LLM call.
Set to empty if no component name is provided.
""",
    )
    metadata: Union[None, LLMMetadata] = Field(
        default=None, description="Token usage, cost, latency, ratelimit, ..."
    )
    error: Optional[LLMError] = None


class BaseLLM(ABC):
    def __init__(self, config: Union[LLMConfig, Dict] = None, **kwargs):
        self.config = config
        if isinstance(config, Dict):
            self.config = self.config_base_model(**config)
        if self.config is None:
            self.config = self.config_base_model(**kwargs)

    @property
    @abstractmethod
    def config_base_model(self):
        raise NotImplementedError

    def invoke(self, messages: List[AnyMessage], **kwargs) -> LLMOutput:
        raise NotImplementedError

    async def ainvoke(self, messages: List[AnyMessage], **kwargs) -> LLMOutput:
        raise NotImplementedError

    def stream(self, messages: List[AnyMessage], **kwargs):
        raise NotImplementedError

    async def astream(self, messages: List[AnyMessage], **kwargs):
        raise NotImplementedError


File: dataqa/llm/openai.py
=======================================
import time
from pydantic import Field
from typing import Dict, List, Optional, Any, Union
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from dataqa.llm.base_llm import BaseLLM, LLMConfig, LLMMetadata, LLMOutput, LLMError


class AzureOpenAIConfig(LLMConfig):
    base_url: Optional[str] = Field(
        default="",
        description="""The base URL of AzureOpenAI.
It should be provided either
- when define AzureOpenAIConfig
- call AzureOpenAI.invoke()
""",
    )
    api_key: Optional[str] = Field(
        default="",
        description="""The API KEY of AzureOpenAI.
It should be provided either
- when define AzureOpenAIConfig
- call AzureOpenAI.invoke()
""",
    )
    api_version: str
    api_type: str
    temperature: float = 1
    num_response: int = Field ( # TODO how to generate multiple responses
        default=1, description="The number of LLM response to be generated"
    )
    max_response_token: int = Field(
        default=2000,
        description="The maximum output tokens", # TODO o1 requires a different attribute "max_completion_token"
    )
    oai_params: Optional[Dict[str, Any]] = Field(default={})
    azure_model_params: Optional[Dict[str, Any]] = Field(default={}) 
    # TODO
    # throw exception
    # retry


class AzureOpenAI(BaseLLM):
    config_base_model = AzureOpenAIConfig
    config: AzureOpenAIConfig

    def _get_conn(self, **kwargs):
        llm = AzureChatOpenAI(
            azure_endpoint=kwargs.get("base_url") or self.config.base_url,
            azure_deployment=self.config.model,
            api_key=kwargs.get("api_key") or self.config.api_key,
            api_version=self.config.api_version,
            openai_api_type=self.config.api_type,
            temperature=self.config.temperature,
            max_tokens=self.config.max_response_token,
            n=self.config.num_response,
            include_response_headers=True,
            **self.config.oai_params,
            **self.config.azure_model_params,
        )
        with_structured_output = kwargs.get(
            "with_structured_output", self.config.with_structured_output
        )
        if with_structured_output is not None:
            llm = llm.with_structured_output(with_structured_output, include_raw=True)
        return llm

    async def ainvoke(self, messages, **kwargs) -> LLMOutput:
        t = time.time()
        from_component = kwargs.get("from_component", "")
        generation = []
        metadata = None
        error = None

        try:
            response = await self._get_conn(**kwargs).ainvoke(messages)
            kwargs_get = {"with_structured_output": kwargs.get(
                "with_structured_output", self.config.with_structured_output
            )}
            if kwargs_get["with_structured_output"] is not None:
                generation = response["parsed"]
            else:
                generation = response.content
        except Exception as e:
            # TODO handle exception
            error = LLMError(error_code=0, error_type="LLM Error", error_message=str(e))
        return LLMOutput(
            prompt=messages,
            generation=generation,
            from_component=from_component,
            metadata=metadata,
            error=error,
        )


File: dataqa/pipelines/__init__.py
=======================================


File: dataqa/pipelines/constants.py
=======================================
PIPELINE_START = "START"
PIPELINE_END = "END"
STATE_GRAPH_TYPE = "PipelineState"
COMP_PREFIX = "COMP_"
FILE_PREFIX = "FILE_"
COMPONENT_MARKER = "is_component"
CONDITIONAL_EDGE_MARKER = "is_conditional_edge"
INPUT_SOURCE = "input_source"
COMPONENT_OUTPUT_SUFFIX = "_output"
PIPELINE_INPUT = "input"
PIPELINE_OUTPUT = "output"


File: dataqa/pipelines/pipeline.py
=======================================
from pydantic import Field, create_model
import yaml
from typing import Any, Dict, List, Optional, Type, Union

from langgraph.graph import END, START, StateGraph
from langgraph.graph.graph import CompiledGraph
from langgraph.checkpoint.memory import MemorySaver
from dataqa.errors import PipelineConfigError
from dataqa.pipelines.constants import (
    COMP_PREFIX,
    FILE_PREFIX,
    PIPELINE_END,
    PIPELINE_START,
    INPUT_SOURCE,
    COMPONENT_MARKER,
    CONDITIONAL_EDGE_MARKER,
    COMPONENT_OUTPUT_SUFFIX,
    PIPELINE_INPUT,
    PIPELINE_OUTPUT,
    STATE_GRAPH_TYPE,
)
from dataqa.pipelines.schema import PipelineConfig
from dataqa.utils import cls_from_str, load_file
from dataqa.state import BasePipelineState


# TODO: Add support for loading files from resource manager
def load_or_get_component(
    component_name: str,
    component_definitions: Dict[str, Dict[str, Any]],
    components: Optional[Dict[str, Type]] = None,
):
    """ """
    if component_name in components:
        return components[component_name]

    component_params = component_definitions[component_name].get("params", {})
    component_type = component_definitions[component_name]["type"]

    for key, value in component_params.items():
        if isinstance(value, str):
            if value.startswith(COMP_PREFIX):
                value_component_name = value.removeprefix(COMP_PREFIX)
                if value_component_name == component_name:
                    raise PipelineConfigError(
                        f"Component `{component_name}` references itself in its param `{key}`, please check the config"
                    )
                if value_component_name not in components:
                    load_or_get_component(
                        value_component_name, component_definitions, components
                    )
                component_params[key] = components[value_component_name]
            elif value.startswith(FILE_PREFIX):
                component_params[key] = load_file(value.removeprefix(FILE_PREFIX))

        if isinstance(value, dict):
            for val_key, val in value.items():
                if isinstance(val, str):
                    if val.startswith(COMP_PREFIX):
                        val_component_name = val.removeprefix(COMP_PREFIX)
                        if val_component_name == component_name:
                            raise PipelineConfigError(
                                f"Component `{component_name}` references itself in its param `{key}`, please check the config"
                            )
                        if val_component_name not in components:
                            load_or_get_component(
                                val_component_name, component_definitions, components
                            )
                        value[val_key] = components[val_component_name]
                    elif val.startswith(FILE_PREFIX):
                        value[val_key] = load_file(val.removeprefix(FILE_PREFIX))

    component_instance = cls_from_str(component_type)(**component_params)
    components[component_name] = component_instance
    return component_instance


def update_edge_node_name(node: Union[str, List[str]]) -> Union[str, List[str]]:
    if isinstance(node, str):
        name = node
        if name == PIPELINE_START:
            return START
        if name == PIPELINE_END:
            return END
        return name
    if isinstance(node, list):
        return update_edge_node_name(node)

    return [update_edge_node_name(name) for name in node]


def update_input_mapping(mapping: Dict[str, str]) -> Dict[str, str]:
    new_mapping = {}
    for field, mapped_field in mapping.items():
        names = mapped_field.split(".")
        if names[0] == PIPELINE_START:
            names[0] = f"{names[0]}{COMPONENT_OUTPUT_SUFFIX}"
        else:
            names[0] = PIPELINE_INPUT
        new_mapping[field] = ".".join(names)
    return new_mapping


def build_graph_from_config(
    pipeline_schema: PipelineConfig, pipeline_name: Optional[str] = None
) -> CompiledGraph:
    """
    :param pipeline_schema:
    :return:
    """
    # get pipeline definition
    pipeline_definition = pipeline_schema.get_pipeline_definition(pipeline_name)

    # get component definitions
    component_definitions = pipeline_schema.get_component_definitions()

    # Add some predefined fields to pipeline_state_fields
    components = {}
    pipeline_state_fields = {}

    # First pass to initialize all the components and add their output state to pipeline state
    for node_name in component_definitions.keys():
        component_instance = load_or_get_component(
            node_name, component_definitions, components
        )

        if getattr(component_instance, COMPONENT_MARKER, False) \
            and not getattr(component_instance, CONDITIONAL_EDGE_MARKER, False):
            pipeline_state_fields[f"{node_name}{COMPONENT_OUTPUT_SUFFIX}"] = (
                component_instance.output_base_model,
                Field(None, description=f"Output of {node_name}"),
            )

    pipeline_state_type = create_model(
        STATE_GRAPH_TYPE, __base__=BasePipelineState, **pipeline_state_fields
    )
    graph_workflow = StateGraph(pipeline_state_type)

    nodes = [PIPELINE_END, PIPELINE_START]
    # add nodes to the graph
    for node in pipeline_definition.nodes:
        node_name = node.name
        if node_name not in [PIPELINE_START, PIPELINE_END]:
            # add node
            component_instance = load_or_get_component(
                node_name, component_definitions, components
            )

            if not getattr(component_instance, CONDITIONAL_EDGE_MARKER, False):
                # Component
                # add node
                graph_workflow.add_node(node.name, component_instance)
                nodes.append(node.name)
                # add edges
                for parent_group in node.parent_groups:
                    graph_workflow.add_edge(
                        update_edge_node_name(parent_group.parent),
                        update_edge_node_name(node.name),
                    )
            else:
                # conditional edge, assert that conditional edge has EXACT one parent node
                if not len(node.parent_groups) == 1 \
                    or not len(node.parent_groups[0].parent) == 1:
                    raise PipelineConfigError(f"{node.name} is an conditional edge. It requires exactly one parent node.")
                parent = node.parent_groups[0].parent
                if isinstance(parent, list):
                    parent = parent[0]
                graph_workflow.add_conditional_edges(
                    update_edge_node_name(parent),
                    component_instance.get_function()
                )

        # set input mapping
        if not component_definitions[node.name].get(INPUT_SOURCE, None):
            raise PipelineConfigError(f"'{INPUT_SOURCE}' is required for {node.name} to define a node or an conditional edge")
        mapping = {}
        for field, mapped_field in component_definitions[node.name][INPUT_SOURCE].items():
            names = mapped_field.split(".")
            if names[0] != PIPELINE_START:
                names[0] = f"{names[0]}{COMPONENT_OUTPUT_SUFFIX}"
            else: 
                names[0] = PIPELINE_INPUT
            mapping[field] = ".".join(names)
        component_instance.set_input_mapping(
            update_input_mapping(
                component_definitions[node.name][INPUT_SOURCE]
            )
        )

    elif node.name == PIPELINE_END:
        for parent_group in node.parent_groups:
            graph_workflow.add_edge(
                update_edge_node_name(parent_group.parent),
                update_edge_node_name(node.name),
            )

    compiled_graph = graph_workflow.compile(checkpointer=MemorySaver())
    return compiled_graph, pipeline_state_type


def build_graph_from_yaml(
    pipeline_path: str, pipeline_name: Optional[str] = None
):
    pipeline_config = yaml.safe_load(open(pipeline_path))
    pipeline_schema = PipelineConfig(**pipeline_config)
    return build_graph_from_config(pipeline_schema, pipeline_name)


File: dataqa/pipelines/schema.py
=======================================
from typing import Any, Dict, List, Optional, Type, Union, Tuple

from pydantic import BaseModel, Field, model_validator
from dataqa.errors import PipelineConfigError
from dataqa.pipelines.constants import PIPELINE_START, PIPELINE_END


class PipelineComponent(BaseModel):
    name: str
    type: str
    params: Dict[str, Any]
    input_source: Optional[Dict[str, str]] = None


class ParentGroup(BaseModel):
    parent: Union[str, List[str]]


class NodeEdge(BaseModel):
    name: str
    parent_groups: List[ParentGroup] = Field(
        description="""A list of parent groups.
One parent group represents a group of nodes required together to trigger this nodess.
""",
        default_factory=list
    )


class Pipeline(BaseModel):
    name: str
    nodes: List[NodeEdge]

    @model_validator(mode="after")
    def valid_parent_groups(self):
        """
        Validate that every parent is a node in the pipeline
        """
        # raises ValueError

        for node in self.nodes:
            for parent_group in node.parent_groups:
                if isinstance(parent_group.parent, str):
                    parent = parent_group.parent
                    if not any(
                        parent == n.name
                        for n in self.nodes
                    ) and not parent == PIPELINE_START:
                        raise PipelineConfigError(f"Unknown node {parent} used as the parent node of {node.name}")
                else:
                    for parent in parent_group.parent:
                        if not any(
                            parent == n.name
                            for n in self.nodes
                        ) and not parent == PIPELINE_START:
                            raise PipelineConfigError(f"Unknown node {parent} used as the parent node of {node.name}")
        return self


class PipelineConfig(BaseModel):
    components: List[PipelineComponent]
    pipelines: List[Pipeline]
    version: Optional[str] = None

    @model_validator(mode="after")
    def valid_node_name(self):
        """
        Validate that every node in pipeline is declared in components
        """
        # raises ValueError

        for pipeline in self.pipelines:
            for node in pipeline.nodes:
                if not any(
                    node.name == component.name
                    for component in self.components
                ) and not node.name == PIPELINE_END:
                    raise PipelineConfigError(f"Unknown node {node.name} used in pipeline {pipeline.name}")
        return self

    def get_pipeline_definition(self, pipeline_name: str = None) -> Pipeline:
        """
        :param pipeline_name:
        :return:
        """
        if pipeline_name is None:
            if len(self.pipelines) == 0:
                raise PipelineConfigError(
                    "More than one pipelines specified in the config please specify the pipeline name"
                )
            else:
                return self.pipelines[0]

        pipelines = [
            pipeline for pipeline in self.pipelines if pipeline.name == pipeline_name
        ]

        if len(pipelines) == 1:
            return pipelines[0]

        if not pipelines:
            raise PipelineConfigError(
                f"No pipeline with name ('{pipeline_name}') exists, please check your config"
            )

        if len(pipelines) > 1:
            raise PipelineConfigError(
                f"More than one pipelines with name ({pipeline_name}) present, please correct the config and provide a"
                f" unique pipeline name to every pipeline"
            )

    def get_component_by_name(self, component_name: str) -> PipelineComponent:
        """
        :param component_name:
        :return:
        """
        components = [
            component
            for component in self.components
            if component.name == component_name
        ]

        if len(components) == 1:
            return components[0]

        if not components:
            raise PipelineConfigError(
                f"No component with the name ('{component_name}') found."
            )

        if len(components) > 1:
            raise PipelineConfigError(
                f"More than one components with name ({component_name}) present, please correct the config and provide a"
                f" unique component name to every component"
            )

    def get_component_definitions(self) -> Dict[str, Dict[str, Any]]:
        """
        :return:
        """
        component_definitions = {}
        for component in self.components:
            component_fields = {
                field: getattr(component, field)
                for field in component.model_fields.keys()
            }
            component_definitions[component.name] = component_fields

        return component_definitions


File: dataqa/utils/__init__.py
=======================================


File: dataqa/utils/component_utils.py
=======================================
from typing import List, Optional, Any, Type, Dict
from pydantic import Field, create_model, BaseModel
from dataqa.components.base_component import Variable


def build_base_model_from_parameters(
    base_model_name: str, parameters: List[Variable]
) -> Type[BaseModel]:
    """
    Dynamically build `base_model_name` as a Pydantic BaseModel class.
    The new class contains all the variable in parameters as Fields.
    """
    model_fields = {}
    for field_properties in parameters:
        field_name = field_properties.name
        field_type = eval(field_properties.type) # TODO if we can avoid using `eval`
        field_description = field_properties.description
        default = field_properties.default
        optional = field_properties.optional

        if optional:
            field_type = Optional[field_type]
            model_fields[field_name] = (
                field_type,
                Field(description=field_description, default=default),
            )
        else:
            model_fields[field_name] = (
                field_type,
                Field(..., description=field_description),
            )

    return create_model(base_model_name, **model_fields)


def extract(
    response: str, prefix: str, suffix: str, error_tolerant: bool = True
) -> str:
    """
    Parse the response and return the text between the first `prefix` and the last `suffix`.
    """
    if len(prefix) == 0:
        a = 0
    else:
        a = response.find(prefix)
    b = response.rfind(suffix)
    if a == -1 or b == -1:
        if error_tolerant:
            return ""
        raise ValueError(f"can not find keywords {prefix} or {suffix} in {response}")
    return response[a + len(prefix) : b].strip()


File: dataqa/utils/data_model_util.py
=======================================
from pydantic import BaseModel, Field, create_model
from typing import List, Optional, Type, Any, Dict


def create_base_model(
    model_name: str, parameters: List[Dict], parent_model: Optional[Type[BaseModel]] = None
) -> Type[BaseModel]:
    """
    Create Pydantic base model dynamically
    :param model_name: name of the base model to be created
    :param parameters: list of fields as dictionary
    :param parent_model: class of parent base model
    :return: created base model
    """
    model_fields = dict()
    for field in parameters:
        field_name = field["name"]
        field_type = eval(field["type"])
        field_description = field["description"]
        model_fields[field_name] = (field_type, Field(description=field_description))
    if parent_model is None:
        return create_model(model_name, **model_fields)
    else:
        return create_model(model_name, __base__=parent_model, **model_fields)


File: dataqa/ingest_knowledge.py
=======================================
import yaml
from pydantic import BaseModel, Field, create_model, parse_obj_as
from typing import Any, Dict, List, Optional
import logging
import sys
from datetime import datetime
from dataqa.utils.data_model_util import create_base_model


logger = logging.getLogger(__name__)


class KnowledgeBase:
    """Knowledge base object"""

    def __init__(self, config: Dict):
        """
        :param config: config dictionary that defines all retrievable
        """
        self.config = config
        self.data = self.ingest_knowledge_base()

    def get_kb_by_name(self, kb_name: str) -> Optional[Dict]:
        """
        :param kb_name: string of knowledge base name
        :return: knowledge base with given name
        """
        for kb in self.data:
            if kb["name"] == kb_name:
                return kb
        return None

    def get_kb_by_index(self, kb_index: str) -> Optional[Dict]:
        """
        :param kb_index: string of knowledge base index
        :return: knowledge base with given index
        """
        for kb in self.data:
            if kb["knowledge_base_index"] == kb_index:
                return kb
        return None

    def ingest_knowledge_base(self):
        # TODO: validate retrievable data path
        retrievable_data = yaml.safe_load(
            open(self.config["retrievable_data_path"], "r")
        )
        knowledge_base = []
        for retrievable in self.config["data"]:
            name = retrievable["name"]
            fields = retrievable["fields"]
            knowledge_base_index = retrievable["knowledge_base_index"]

            record_base_model = create_base_model(name, fields)

            data = retrievable_data[name]["data"]
            parsed_data_list = []
            for record in data:
                try:
                    parsed_data = record_base_model.model_validate(record)
                    parsed_data_list.append(parsed_data)
                except:
                    logger.error(
                        f"Failed to parse record for {name} retrievable. Record:\n{record}"
                    )

            knowledge_base.append(
                {
                    "name": name,
                    "base_model": record_base_model,
                    "knowledge_base_index": knowledge_base_index,
                    "records": parsed_data_list,
                }
            )
        return knowledge_base


if __name__ == "__main__":
    retriever_config = yaml.safe_load(
        open("example/ccb_risk/config/config_retriever.yml", "r")
    )
    my_kb = KnowledgeBase(retriever_config["components"][0]["parameters"]["config"])
    print()


File: dataqa/utils.py
=======================================
import importlib
import inspect
import json
import pickle
from copy import deepcopy
from pathlib import Path
from typing import Any, Optional, Text, Type, TypeVar, Union

import yaml

T = TypeVar("T")


def class_from_module_path(
    module_path: Text, lookup_path: Optional[Text] = None
) -> Type:
    """Given the module path and name of a class, tries to retrieve the class.

    The loaded class can be used to instantiate new objects.

    Args:
        module_path: either an absolute path to a Python class,
                     or the name of the class in the local / global scope.
        lookup_path: a path where to load the class from, if it cannot
                     be found in the local / global scope.

    Returns:
        A Python class.

    Raises:
        ImportError, in case the module path cannot be found.
        RasaException, in case the imported result is something other than a class
    """

    klass = None
    if "." in module_path:
        module_name, _, class_name = module_path.rpartition(".")
        m = importlib.import_module(module_name)
        klass = getattr(m, class_name, None)
    elif lookup_path:
        # try to import the class from the lookup path
        m = importlib.import_module(lookup_path)
        klass = getattr(m, module_path, None)

    if klass is None:
        raise ImportError(f"Cannot retrieve class from path {module_path}.")

    if not inspect.isclass(klass):
        raise TypeError(
            f"class_from_module_path() is expected to return a class, "
            f"but for {module_path} we got a {type(klass)}."
        )
    return klass


def cls_from_str(name: str) -> Type[Union[Any, T]]:
    """
    Returns a class object with the given name as a string.
    :param name: The name of the class as a string.
    :return: The class object.
    :raises ImportError: If the class cannot be retrieved from the path.
    """
    try:
        return class_from_module_path(name)
    except (AttributeError, ImportError, TypeError, ValueError):
        raise ImportError(f"Cannot retrieve class from name {name}.")


def load_file(file_path: Union[str, Path]):
    file_path = deepcopy(file_path)
    if isinstance(file_path, Path):
        str_file_path = str(file_path)
    else:
        str_file_path = file_path

    if str_file_path.endswith("json"):
        return json.load(open(str_file_path, "r"))
    if str_file_path.endswith("yml"):
        return yaml.safe_load(open(str_file_path, "r"))
    if str_file_path.endswith("pkl"):
        return pickle.load(open(str_file_path, "rb"))
    return open(str_file_path).read()